problem-solving relies on a sequence of cognitive steps involving phases of task encoding, the structuring of solution steps, and their execution. On the neural level, metabolic neuroimaging studies have associated a frontal-parietal network with various aspects of executive control during numerical and nonnumerical problem-solving. We used EEG-MEG to assess whether frontal cortex contributes specifically to the early structuring of multiple solution steps. Basic multiplication ("3  4" vs. "3  24") was compared with an arithmetic sequence rule ("first add the two digits, then multiply the sum with the smaller digit") on two complexity levels. This allowed dissociating demands of early solution step structuring from early task decoding demands. Structuring demands were high for conditions that required multiple steps, that is, complex multiplication and the two arithmetic sequence conditions, but low for easy multiplication that mostly relied on direct memory retrieval. Increased right frontal activation in time windows between 300 and 450 msec was observed only for conditions that required multiple solution steps. General task decoding demands, operationalized by problem size (one-digit vs. two-digit numbers), did not predict these early frontal effects. In contrast, parietal effects occurred as a function of problem size irrespectively of structuring demands in early phases of task decoding between 100 and 300 msec. We here propose that frontal cortex subserves domain-general processes of problem-solving, such as the structuring of multiple solution steps, whereas parietal cortex supports number-specific early decoding processes that vary as a function of problem size.
Whispering is a unique expression mode that is specific to auditory communication. Individuals switch their vocalization mode to whispering especially when affected by inner emotions in certain social contexts, such as in intimate relationships or intimidating social interactions. Although this context-dependent whispering is adaptive, whispered voices are acoustically far less rich than phonated voices and thus impose higher hearing and neural auditory decoding demands for recognizing their socio-affective value by listeners. The neural dynamics underlying this recognition especially from whispered voices are largely unknown. Here we show that whispered voices in humans are considerably impoverished as quantified by an entropy measure of spectral acoustic information, and this missing information needs large-scale neural compensation in terms of auditory and cognitive processing. Notably, recognizing the socio-affective information from voices was slightly more difficult from whispered voices, probably based on missing tonal information. While phonated voices elicited extended activity in auditory regions for decoding of relevant tonal and time information and the valence of voices, whispered voices elicited activity in a complex auditory-frontal brain network. Our data suggest that a large-scale multidirectional brain network compensates for the impoverished sound quality of socially meaningful environmental signals to support their accurate recognition and valence attribution.
Until now, hypersexuality has not found entry into the common diagnostic classification systems. However it is a frequently discussed phenomenon consisting of excessive sexual appetite that is maladaptive for the individual. Initial studies investigated the neurobiological underpinnings of hypersexuality, but current literature is still insufficient to draw unequivocal conclusions. In the present review, we summarize and discuss findings from various perspectives: neuroimaging and lesion studies, studies on other neurological disorders that are sometimes accompanied by hypersexuality, neuropharmacological evidence, genetic as well as animal studies. Taken together, the evidence seems to imply that alterations in the frontal lobe, amygdala, hippocampus, hypothalamus, septum, and brain regions that process reward play a prominent role in the emergence of hypersexuality. Genetic studies and neuropharmacological treatment approaches point at an involvement of the dopaminergic system.
In this study, we investigated the cognitive processing stages underlying associative recognition using MEG. Over the last four decades, a model of associative recognition has been developed in the ACT-R cognitive architecture. This model was first exclusively based on behavior, but was later evaluated and improved based on fMRI and EEG data. Unfortunately, the limited spatial resolution of EEG and the limited temporal resolution of fMRI have made it difficult to fully understand the spatiotemporal dynamics of associative recognition. We therefore conducted an associative recognition experiment with MEG, which combines excellent temporal resolution with reasonable spatial resolution. To analyze the data, we applied non-parametric cluster analyses and a multivariate classifier. This resulted in a detailed spatio-temporal model of associative recognition. After the visual encoding of the stimuli in occipital regions, three separable memory processes took place: a familiarity process (temporal cortex), a recollection process (temporal cortex and supramarginal gyrus), and a representational process (dorsolateral prefrontal cortex). A late decision process (superior parietal cortex) then acted upon the recollected information represented in the prefrontal cortex, culminating in a late response process (motor cortex). We conclude that existing theories of associative recognition, including the ACT-R model, should be adapted to include these processes.
Fear of pain (FOP) can increase risk for chronic pain and disability but little is known about corresponding neural responses in anticipation of potential pain. In this study, more (10 women, 6 men) and less (7 women, 6 men) pain-fearful groups underwent whole-brain functional magnetic resonance imaging (fMRI) during anticipation of near pain-threshold stimulation. Groups did not differ in the proportion of stimuli judged to be painful but pain-fearful participants reported significantly more state fear prior to stimulus exposure. Within the entire sample, stronger activation was found in several pain perception regions (e.g., bilateral insula, midcingulate cortex (MCC), thalamus, superior frontal gyrus) and visual areas linked to decoding stimulus valences (inferior orbital cortex) during anticipation of "painful" stimuli. Between groups and correlation analyses indicated pain-fearful participants experienced comparatively more activity in regions implicated in evaluating potential threats and processing negative emotions during anticipation (i.e., MCC, mid occipital cortex, superior temporal pole), though group differences were not apparent in most so-called "pain matrix" regions. In sum, trait- and task-based FOP is associated with enhanced responsiveness in regions involved in threat processing and negative affect during anticipation of potentially painful stimulation.
The concept of affordances indicates "action possibilities" as characterized by object properties the environment provides to interacting organisms. Affordances relate to both perception and action and refer to sensory-motor processes emerging from goal-directed object interaction. In contrast to stable properties, affordances may vary with environmental context. A sub-classification into stable and variable affordances was proposed in the framework of the ROSSI project (Borghi et al., 2010; Borghi and Riggio, 2015, 2009). Here, we present a coordinate-based meta-analysis of functional imaging studies on object interaction targeting consistent anatomical correlates of these different types of affordances. Our review revealed the existence of two parallel (but to some extent overlapping) functional pathways. The network for stable affordances consists of predominantly left inferior parietal and frontal cortices in the ventro-dorsal stream, whereas the network for variable affordances is localized preferentially in the dorso-dorsal stream. This is in line with the proposal of differentiated affordances: stable affordances are characterized by the knowledge of invariant object features, whereas variable affordances underlie adaptation to changing object properties.
We are frequently exposed to hand written digits 0-9 in today's modern life. Success in decoding-classification of hand written digits helps us understand the corresponding brain mechanisms and processes and assists seriously in designing more efficient brain-computer interfaces. However, all digits belong to the same semantic category and similarity in appearance of hand written digits makes this decoding-classification a challenging problem. In present study, for the first time, augmented naive Bayes classifier is used for classification of functional Magnetic Resonance Imaging (fMRI) measurements to decode the hand written digits which took advantage of brain connectivity information in decoding-classification. fMRI was recorded from three healthy participants, with an age range of 25-30. Results in different brain lobes (frontal, occipital, parietal, and temporal) show that utilizing connectivity information significantly improves decoding-classification and capability of different brain lobes in decoding-classification of hand written digits were compared to each other. In addition, in each lobe the most contributing areas and brain connectivities were determined and connectivities with short distances between their endpoints were recognized to be more efficient. Moreover, data driven method was applied to investigate the similarity of brain areas in responding to stimuli and this revealed both similarly active areas and active mechanisms during this experiment. Interesting finding was that during the experiment of watching hand written digits, there were some active networks (visual, working memory, motor, and language processing), but the most relevant one to the task was language processing network according to the voxel selection.
The fate of a memory is partly determined at initial encoding. However, the behavioral consequences of memory formation are often tested only once and shortly after learning, which leaves the neuronal predictors for the formation of durable memories largely unknown. Here, we hypothesized that durable memory formation (as opposed to weak or no memory formation) is reflected through increased activation in the medial temporal lobes and prefrontal cortex, and more consistent processing (i.e., stronger pattern similarity) across encoding material. Thirty-four human subjects studied unique picture-location associations while undergoing fMRI and performed a cued recall test immediately after study as well as 48 h later. Associative memories were defined as "weak" if they were retrieved during the immediate test only. Conversely, "durable" memories persisted also after 48 h. The posterior cingulate cortex showed increased pattern similarity during successful memory formation, independent of the eventual durability. For durable memory encoding, we found increased activation in medial and inferior temporal, prefrontal, and parietal regions. This was accompanied by stronger pattern similarity in lateral prefrontal and parietal regions, as well as in anterior and posterior midline structures that were also engaged during later memory retrieval. Thus, we show that pattern similarity, or consistent processing, in the posterior cingulate cortex predicts associative memory formation at encoding. If this is paralleled by additional activation increases in regions typically related to encoding, and by consistent processing in regions involved in later retrieval, formed memories appear durable for at least 48 h.
In daily life the brain is exposed to a large amount of external signals that compete for processing resources. The attentional system can select relevant information based on many possible combinations of goal-directed and stimulus-driven control signals. Here, we investigate the behavioral and physiological effects of competition between distinctive visual events during free-viewing of naturalistic videos. Nineteen healthy subjects underwent functional magnetic resonance imaging (fMRI) while viewing short video-clips of everyday life situations, without any explicit goal-directed task. Each video contained either a single semantically-relevant event on the left or right side (Lat-trials), or multiple distinctive events in both hemifields (Multi-trials). For each video, we computed a salience index to quantify the lateralization bias due to stimulus-driven signals, and a gaze index (based on eye-tracking data) to quantify the efficacy of the stimuli in capturing attention to either side. Behaviorally, our results showed that stimulus-driven salience influenced spatial orienting only in presence of multiple competing events (Multi-trials). fMRI results showed that the processing of competing events engaged the ventral attention network, including the right temporoparietal junction (R TPJ) and the right inferior frontal cortex. Salience was found to modulate activity in the visual cortex, but only in the presence of competing events; while the orienting efficacy of Multi-trials affected activity in both the visual cortex and posterior parietal cortex (PPC). We conclude that in presence of multiple competing events, the ventral attention system detects semantically-relevant events, while regions of the dorsal system make use of saliency signals to select relevant locations and guide spatial orienting.
Speech repetition relies on a series of distributed cortical representations and functional pathways. A speaker must map auditory representations of incoming sounds onto learned speech items, maintain an accurate representation of those items in short-term memory, interface that representation with the motor output system, and fluently articulate the target sequence. A "dorsal stream" consisting of posterior temporal, inferior parietal and premotor regions is thought to mediate auditory-motor representations and transformations, but the nature and activation of these representations for different portions of speech repetition tasks remains unclear. Here we mapped the correlates of phonetic and/or phonological information related to the specific phonemes and syllables that were heard, remembered, and produced using a series of cortical searchlight multi-voxel pattern analyses trained on estimates of BOLD responses from individual trials. Based on responses linked to input events (auditory syllable presentation), predictive vowel-level information was found in the left inferior frontal sulcus, while syllable prediction revealed significant clusters in the left ventral premotor cortex and central sulcus and the left mid superior temporal sulcus. Responses linked to output events (the GO signal cueing overt production) revealed strong clusters of vowel-related information bilaterally in the mid to posterior superior temporal sulcus. For the prediction of onset and coda consonants, input-linked responses yielded distributed clusters in the superior temporal cortices, which were further informative for classifiers trained on output-linked responses. Output-linked responses in the Rolandic cortex made strong predictions for the syllables and consonants produced, but their predictive power was reduced for vowels. The results of this study provide a systematic survey of how cortical response patterns covary with the identity of speech sounds, which will help to constrain and guide theoretical models of speech perception, speech production, and phonological working memory.
Derivational morphology is a cross-linguistically dominant mechanism for word formation, combining existing words with derivational affixes to create new word forms. However, the neurocognitive mechanisms underlying the representation and processing of such forms remain unclear. Recent cross-linguistic neuroimaging research suggests that derived words are stored and accessed as whole forms, without engaging the left-hemisphere perisylvian network associated with combinatorial processing of syntactically and inflectionally complex forms. Using fMRI with a "simple listening" no-task procedure, we reexamine these suggestions in the context of the root-based combinatorially rich Italian lexicon to clarify the role of semantic transparency (between the derived form and its stem) and affix productivity in determining whether derived forms are decompositionally represented and which neural systems are involved. Combined univariate and multivariate analyses reveal a key role for semantic transparency, modulated by affix productivity. Opaque forms show strong cohort competition effects, especially for words with nonproductive suffixes (ventura, "destiny"). The bilateral frontotemporal activity associated with these effects indicates that opaque derived words are processed as whole forms in the bihemispheric language system. Semantically transparent words with productive affixes (libreria, "bookshop") showed no effects of lexical competition, suggesting morphologically structured co-representation of these derived forms and their stems, whereas transparent forms with nonproductive affixes (pineta, pine forest) show intermediate effects. Further multivariate analyses of the transparent derived forms revealed affix productivity effects selectively involving left inferior frontal regions, suggesting that the combinatorial and decompositional processes triggered by such forms can vary significantly across languages.
The role of the orbitofrontal cortex (OFC) in value processing is a focus of research. Conventional imaging analysis, where smoothing and averaging are employed, may not be sufficiently sensitive in studying the OFC, which has heterogeneous anatomical structures and functions. In this study, we employed representational similarity analysis (RSA) to reveal the multi-voxel fMRI patterns in the OFC associated with value processing during the anticipatory and the consummatory phases. We found that multi-voxel activation patterns in the OFC encoded magnitude and partial valence information (win vs. loss) but not outcome (favourable vs. unfavourable) during reward consummation. Furthermore, the lateral OFC rather than the medial OFC encoded loss information. Also, we found that OFC encoded values in a similar way to the ventral striatum (VS) or the anterior insula (AI) during reward anticipation regardless of motivated response and to the medial prefrontal cortex (MPFC) and the VS in reward consummation. In contrast, univariate analysis did not show changes of activation in the OFC. These findings suggest an important role of the OFC in value processing during reward anticipation and consummation.
The human species has developed complex mathematical skills which likely emerge from a combination of multiple foundational abilities. One of them seems to be a preverbal capacity to extract and manipulate the numerosity of sets of objects which is shared with other species and in humans is thought to be integrated with symbolic knowledge to result in a more abstract representation of numerical concepts. For what concerns the functional neuroanatomy of this capacity, neuropsychology and functional imaging have localized key substrates of numerical processing in parietal and frontal cortex. However, traditional fMRI mapping relying on a simple subtraction approach to compare numerical and nonnumerical conditions is limited to tackle with sufficient precision and detail the issue of the underlying code for number, a question which more easily lends itself to investigation by methods with higher spatial resolution, such as neurophysiology. In recent years, progress has been made through the introduction of approaches sensitive to within-category discrimination in combination with fMRI (adaptation and multivariate pattern recognition), and the present review summarizes what these have revealed so far about the neural coding of individual numbers in the human brain, the format of these representations and parallels between human and monkey neurophysiology findings.
Functional near-infrared spectroscopy (fNIRS) is a noninvasive neuroimaging technique used to measure changes in oxygenated hemoglobin (oxy-Hb) and deoxygenated hemoglobin (deoxy-Hb) in the brain. In this study, we present a decomposition approach based on single-channel independent component analysis (scICA) to investigate the contribution of physiological noise to fNIRS signals during rest. Single-channel ICA is an underdetermined decomposition method, which separates a single time series into components containing nonredundant spectral information. Using scICA, fNIRS signals from a total of 17 subjects were decomposed into the constituent physiological components. The percentage contribution of the classes of physiology to the fNIRS signals including low-frequency (LF) fluctuations, respiration, and cardiac oscillations was estimated using spectral domain classification methods. Our results show that LF oscillations accounted for 40% to 55% of total power of both the oxy-Hb and deoxy-Hb signals. Respiration and its harmonics accounted for 10% to 30% of the power, and cardiac pulsations and cardio-respiratory components accounted for 10% to 30%. We describe this scICA method for decomposing fNIRS signals, which unlike other approaches to spatial covariance reduction is applicable to both single- or multiple-channel fNIRS signals and discuss how this approach allows functionally distinct sources of noise with disjoint spectral support to be separated from obscuring systemic physiology.
Guntupalli, Haxby, and colleagues have proposed a new quantitative way to align whole-brain functional imaging data. The new technique, searchlight hyperalignment, allows transformations of a subject's brain activity into a latent common representational space and vice versa.
How is the processing of task information organized in the brain? Many views of brain function emphasize modularity, with different regions specialized for processing different types of information. However, recent accounts also highlight flexibility, pointing especially to the highly consistent pattern of frontoparietal activation across many tasks. Although early insights from functional imaging were based on overall activation levels during different cognitive operations, in the last decade many researchers have used multivoxel pattern analyses to interrogate the representational content of activations, mapping out the brain regions that make particular stimulus, rule, or response distinctions. Here, we drew on 100 searchlight decoding analyses from 57 published papers to characterize the information coded in different brain networks. The outcome was highly structured. Visual, auditory, and motor networks predominantly (but not exclusively) coded visual, auditory, and motor information, respectively. By contrast, the frontoparietal multiple-demand network was characterized by domain generality, coding visual, auditory, motor, and rule information. The contribution of the default mode network and voxels elsewhere was minor. The data suggest a balanced picture of brain organization in which sensory and motor networks are relatively specialized for information in their own domain, whereas a specific frontoparietal network acts as a domain-general "core" with the capacity to code many different aspects of a task.
The functional organization of human medial frontal cortex (MFC) is a subject of intense study. Using fMRI, the MFC has been associated with diverse psychological processes, including motor function, cognitive control, affect, and social cognition. However, there have been few large-scale efforts to comprehensively map specific psychological functions to subregions of medial frontal anatomy. Here we applied a meta-analytic data-driven approach to nearly 10,000 fMRI studies to identify putatively separable regions of MFC and determine which psychological states preferentially recruit their activation. We identified regions at several spatial scales on the basis of meta-analytic coactivation, revealing three broad functional zones along a rostrocaudal axis composed of 2-4 smaller subregions each. Multivariate classification analyses aimed at identifying the psychological functions most strongly predictive of activity in each region revealed a tripartite division within MFC, with each zone displaying a relatively distinct functional signature. The posterior zone was associated preferentially with motor function, the middle zone with cognitive control, pain, and affect, and the anterior with reward, social processing, and episodic memory. Within each zone, the more fine-grained subregions showed distinct, but subtler, variations in psychological function. These results provide hypotheses about the functional organization of medial prefrontal cortex that can be tested explicitly in future studies.
Mental representation of the future is a fundamental component of goal-directed behavior. Computational and animal models highlight prospective spatial coding in the hippocampus, mediated by interactions with the prefrontal cortex, as a putative mechanism for simulating future events. Using whole-brain high-resolution functional magnetic resonance imaging and multi-voxel pattern classification, we tested whether the human hippocampus and interrelated cortical structures support prospective representation of navigational goals. Results demonstrated that hippocampal activity patterns code for future goals to which participants subsequently navigate, as well as for intervening locations along the route, consistent with trajectory-specific simulation. The strength of hippocampal goal representations covaried with goal-related coding in the prefrontal, medial temporal, and medial parietal cortex. Collectively, these data indicate that a hippocampal-cortical network supports prospective simulation of navigational events during goal-directed planning.
Multiple-demand (MD) regions of the human brain show coactivation during many different kinds of task performance. Previous work based on resting-state functional magnetic resonance imaging (fMRI) has shown that MD regions may be divided into two closely coupled subnetworks centered around the lateral frontoparietal (FP) and cingulo-opercular cortex. Here, we used on-task fMRI to test whether this division is apparent during the performance of an executive task. Furthermore, we investigated whether there is a difference in the encoding of task between the two subnetworks. Using connectivity methods, we found that activity across the entire MD cortex is correlated during task performance. Meanwhile, however, there was significantly stronger connectivity within each of the subnetworks than between them. Using multivoxel pattern analysis, we also found that, although we were able to decode task-relevant information from all regions of the MD cortex, classification accuracy scores were significantly higher in the FP subnetwork. These results suggest a nested picture with MD regions as a whole showing coactivation and broad rule representation, but with significant functional distinctions between component subnetworks.
Whereas acute nicotine administration alters brain function which may, in turn, contribute to enhanced attention and performance, chronic cigarette smoking is linked with regional brain atrophy and poorer cognition. However, results from structural magnetic resonance imaging (MRI) studies comparing smokers versus nonsmokers have been inconsistent and measures of gray matter possess limited ability to inform functional relations or behavioral implications. The purpose of this study was to address these interpretational challenges through meta-analytic techniques in the service of clarifying the impact of chronic smoking on gray matter integrity and more fully contextualizing such structural alterations.
Emotional stimuli have been shown to modulate attentional orienting through signals sent by subcortical brain regions that modulate visual perception at early stages of processing. Fewer studies, however, have investigated a similar effect of emotional stimuli on attentional orienting in the auditory domain together with an investigation of brain regions underlying such attentional modulation, which is the general aim of the present study. Therefore, we used an original auditory dot-probe paradigm involving simultaneously presented neutral and angry non-speech vocal utterances lateralized to either the left or the right auditory space, immediately followed by a short and lateralized single sine wave tone presented in the same (valid trial) or in the opposite space as the preceding angry voice (invalid trial). Behavioral results showed an expected facilitation effect for target detection during valid trials while functional data showed greater activation in the middle and posterior superior temporal sulci (STS) and in the medial frontal cortex for valid vs. invalid trials. The use of reaction time facilitation [absolute value of the Z-score of valid-(invalid+neutral)] as a group covariate extended enhanced activity in the amygdalae, auditory thalamus, and visual cortex. Taken together, our results suggest the involvement of a large and distributed network of regions among which the STS, thalamus, and amygdala are crucial for the decoding of angry prosody, as well as for orienting and maintaining attention within an auditory space that was previously primed by a vocal emotional event.
Human voices consist of specific patterns of acoustic features that are considerably enhanced during affective vocalizations. These acoustic features are presumably used by listeners to accurately discriminate between acoustically or emotionally similar vocalizations. Here we used high-field 7T functional magnetic resonance imaging in human listeners together with a so-called experimental 'feature elimination approach' to investigate neural decoding of three important voice features of two affective valence categories (i.e. aggressive and joyful vocalizations). We found a valence-dependent sensitivity to vocal pitch (f0) dynamics and to spectral high-frequency cues already at the level of the auditory thalamus. Furthermore, pitch dynamics and harmonics-to-noise ratio (HNR) showed overlapping, but again valence-dependent sensitivity in tonotopic cortical fields during the neural decoding of aggressive and joyful vocalizations, respectively. For joyful vocalizations we also revealed sensitivity in the inferior frontal cortex (IFC) to the HNR and pitch dynamics. The data thus indicate that several auditory regions were sensitive to multiple, rather than single, discriminative voice features. Furthermore, some regions partly showed a valence-dependent hypersensitivity to certain features, such as pitch dynamic sensitivity in core auditory regions and in the IFC for aggressive vocalizations, and sensitivity to high-frequency cues in auditory belt and parabelt regions for joyful vocalizations.
The basis of motor learning involves decomposing complete actions into a series of predictive individual components that form the whole. The present fMRI study investigated the areas of the human brain important for oculomotor short-term learning, by using a novel sequence learning paradigm that is equivalent in visual and temporal properties for both saccades and pursuit, enabling more direct comparisons between the oculomotor subsystems. In contrast with previous studies that have implemented a series of discrete ramps to observe predictive behaviour as evidence for learning, we presented a continuous sequence of interlinked components that better represents sequences of actions. We implemented both a classic univariate fMRI analysis, followed by a further multivariate pattern analysis (MVPA) within a priori regions of interest, to investigate oculomotor sequence learning in the brain and to determine whether these mechanisms overlap in pursuit and saccades as part of a higher order learning network. This study has uniquely identified an equivalent frontal-parietal network (dorsolateral prefrontal cortex, frontal eye fields and posterior parietal cortex) in both saccades and pursuit sequence learning. In addition, this is the first study to investigate oculomotor sequence learning during fMRI brain imaging, and makes significant contributions to understanding the role of the dorsal networks in motor learning.
A defining trait of human cognition is the capacity to form compounds out of simple thoughts. This ability relies on the logical connectives AND, OR and IF. Simple propositions, e.g., 'There is a fork' and 'There is a knife', can be combined in alternative ways using logical connectives: e.g., 'There is a fork AND there is a knife', 'There is a fork OR there is a knife', 'IF there is a fork, there is a knife'. How does the brain represent compounds based on different logical connectives, and how are compounds evaluated in relation to new facts? In the present study, participants had to maintain and evaluate conjunctive (AND), disjunctive (OR) or conditional (IF) compounds while undergoing functional MRI. Our results suggest that, during maintenance, the left posterior inferior frontal gyrus (pIFG, BA44, or Broca's area) represents the surface form of compounds. During evaluation, the left pIFG switches to processing the full logical meaning of compounds, and two additional areas are recruited: the left anterior inferior frontal gyrus (aIFG, BA47) and the left intraparietal sulcus (IPS, BA40). The aIFG shows a pattern of activation similar to pIFG, and compatible with processing the full logical meaning of compounds, whereas activations in IPS differ with alternative interpretations of conditionals: logical vs conjunctive. These results uncover the functions of a basic cortical network underlying human compositional thought, and provide a shared neural foundation for the cognitive science of language and reasoning.
Offline processing has been shown to strengthen memory traces and enhance learning in the absence of conscious rehearsal or awareness. Here we evaluate whether a brief, two-minute offline processing period can boost associative learning and test a memory reactivation account for these offline processing effects. After encoding paired associates, subjects either completed a distractor task for two minutes or were immediately tested for memory of the pairs in a counterbalanced, within-subjects functional magnetic resonance imaging study. Results showed that brief, awake, offline processing improves memory for associate pairs. Moreover, multi-voxel pattern analysis of the neuroimaging data suggested reactivation of encoded memory representations in dorsolateral prefrontal cortex during offline processing. These results signify the first demonstration of awake, active, offline enhancement of associative memory and suggest that such enhancement is accompanied by the offline reactivation of encoded memory representations.
Intentional action is essential to human behavior, yet its neural basis remains poorly understood. In order to identify neural networks specifically involved in intentional action, freely chosen and externally cued intentions have previously been contrasted. This has led to the identification of a fronto-parietal network, which is involved in freely choosing one's intentions. However, it remains unclear whether this network encodes specific intentions, or whether it merely reflects general preparatory or control processes correlated with intentional action. Here, we used MVPA on fMRI data to identify brain regions encoding non-motor intentions that were either freely chosen or externally cued. We found that a fronto-parietal network, including the lateral prefrontal cortex, premotor, and parietal cortex, contained information about both freely chosen and externally cued intentions. Importantly, MVPA cross-classification indicated that this network represents the content of our intentions similarly, regardless of whether these intentions are freely chosen or externally cued. This finding suggests that the intention network has a general role in processing and representing intentions independent of their origin.
How early blindness reorganizes the brain circuitry that supports auditory motion processing remains controversial. We used fMRI to characterize brain responses to in-depth, laterally moving, and static sounds in early blind and sighted individuals. Whole-brain univariate analyses revealed that the right posterior middle temporal gyrus and superior occipital gyrus selectively responded to both in-depth and laterally moving sounds only in the blind. These regions overlapped with regions selective for visual motion (hMT+/V5 and V3A) that were independently localized in the sighted. In the early blind, the right planum temporale showed enhanced functional connectivity with right occipito-temporal regions during auditory motion processing and a concomitant reduced functional connectivity with parietal and frontal regions. Whole-brain searchlight multivariate analyses demonstrated higher auditory motion decoding in the right posterior middle temporal gyrus in the blind compared to the sighted, while decoding accuracy was enhanced in the auditory cortex bilaterally in the sighted compared to the blind. Analyses targeting individually defined visual area hMT+/V5 however indicated that auditory motion information could be reliably decoded within this area even in the sighted group. Taken together, the present findings demonstrate that early visual deprivation triggers a large-scale imbalance between auditory and "visual" brain regions that typically support the processing of motion information.
The brain basis for auditory working memory, the process of actively maintaining sounds in memory over short periods of time, is controversial. Using functional magnetic resonance imaging in human participants, we demonstrate that the maintenance of single tones in memory is associated with activation in auditory cortex. In addition, sustained activation was observed in hippocampus and inferior frontal gyrus. Multivoxel pattern analysis showed that patterns of activity in auditory cortex and left inferior frontal gyrus distinguished the tone that was maintained in memory. Functional connectivity during maintenance was demonstrated between auditory cortex and both the hippocampus and inferior frontal cortex. The data support a system for auditory working memory based on the maintenance of sound-specific representations in auditory cortex by projections from higher-order areas, including the hippocampus and frontal cortex.
The qualities of perception depend not only on the sensory inputs but also on the brain state before stimulus presentation. Although the collective evidence from neuroimaging studies for a relation between prestimulus state and perception is strong, the interpretation in the context of sensory computations or decision processes has remained difficult. In the auditory system, for example, previous studies have reported a wide range of effects in terms of the perceptually relevant frequency bands and state parameters (phase/power). To dissociate influences of state on earlier sensory representations and higher-level decision processes, we collected behavioral and EEG data in human participants performing two auditory discrimination tasks relying on distinct acoustic features. Using single-trial decoding, we quantified the relation between prestimulus activity, relevant sensory evidence, and choice in different task-relevant EEG components. Within auditory networks, we found that phase had no direct influence on choice, whereas power in task-specific frequency bands affected the encoding of sensory evidence. Within later-activated frontoparietal regions, theta and alpha phase had a direct influence on choice, without involving sensory evidence. These results delineate two consistent mechanisms by which prestimulus activity shapes perception. However, the timescales of the relevant neural activity depend on the specific brain regions engaged by the respective task.
Human amygdalae are involved in various behavioral functions such as affective and stress processing. For these behavioral functions, as well as for psychophysiological arousal including cortisol release, sex differences are reported. Here, we assessed cortisol levels and resting-state functional connectivity (rsFC) of left and right amygdalae in 81 healthy participants (42 women) to investigate potential modulation of amygdala rsFC by sex and cortisol concentration. Our analyses revealed that rsFC of the left amygdala significantly differed between women and men: Women showed stronger rsFC than men between the left amygdala and left middle temporal gyrus, inferior frontal gyrus, postcentral gyrus and hippocampus, regions involved in face processing, inner-speech, fear and pain processing. No stronger connections were detected for men and no sex difference emerged for right amygdala rsFC. Also, an interaction of sex and cortisol appeared: In women, cortisol was negatively associated with rsFC of the amygdalae with striatal regions, mid-orbital frontal gyrus, anterior cingulate gyrus, middle and superior frontal gyri, supplementary motor area and the parietal-occipital sulcus. Contrarily in men, positive associations of cortisol with rsFC of the left amygdala and these structures were observed. Functional decoding analyses revealed an association of the amygdalae and these regions with emotion, reward and memory processing, as well as action execution. Our results suggest that functional connectivity of the amygdalae as well as the regulatory effect of cortisol on brain networks differs between women and men. These sex-differences and the mediating and sex-dependent effect of cortisol on brain communication systems should be taken into account in affective and stress-related neuroimaging research. Thus, more studies including both sexes are required.
Rest breaks are commonly administered as a countermeasure to reduce on-the-job fatigue, both physical and mental. However, this practice makes the assumption that recovery from fatigue, as measured by the reversal of performance declines, is the sole effect of taking a break on behavior. Here, through administering rest breaks of differing lengths in between blocks of a mentally demanding symbol decoding task, we show that this assumption may not be strictly true. First, we replicate previous work by showing that taking a longer break leads to two correlated effects: greater immediate rebound in performance, and greater subsequent time-on-task decline. Using fMRI, we reveal that time-on-task in this paradigm is associated with increasing recruitment of fronto-parietal areas associated with top-down control, and decreasing deactivation in the default-mode network. Finally, by analyzing individual differences, we reveal a potential neural basis for our behavioral observation: greater recovery following long breaks is associated with greater activity in the putamen, an area associated with the automatic generation of motor responses, followed by greater activity in left middle frontal gyrus by the end of those task periods. Taken together, this suggests a shift in the implicit engagement of automatic and controlled attentional processing following longer breaks. This shift may be undesirable or detrimental in real-world situations where maintaining a stable level of attention over time is necessary.
Current models of the functional architecture of human cortex emphasize areas that capture coarse-scale features of cortical topography but provide no account for population responses that encode information in fine-scale patterns of activity. Here, we present a linear model of shared representational spaces in human cortex that captures fine-scale distinctions among population responses with response-tuning basis functions that are common across brains and models cortical patterns of neural responses with individual-specific topographic basis functions. We derive a common model space for the whole cortex using a new algorithm, searchlight hyperalignment, and complex, dynamic stimuli that provide a broad sampling of visual, auditory, and social percepts. The model aligns representations across brains in occipital, temporal, parietal, and prefrontal cortices, as shown by between-subject multivariate pattern classification and intersubject correlation of representational geometry, indicating that structural principles for shared neural representations apply across widely divergent domains of information. The model provides a rigorous account for individual variability of well-known coarse-scale topographies, such as retinotopy and category selectivity, and goes further to account for fine-scale patterns that are multiplexed with coarse-scale topographies and carry finer distinctions.
The ability to maintain representations in the absence of external sensory stimulation, such as in working memory, is critical for guiding human behavior. Human functional brain imaging studies suggest that visual working memory can recruit a network of brain regions from visual to parietal to prefrontal cortex. In this review, we focus on the maintenance of representations during visual working memory and discuss factors determining the topography of those representations. In particular, we review recent studies employing multi-voxel pattern analysis (MVPA) that demonstrate decoding of the maintained content in visual cortex, providing support for a "sensory recruitment" model of visual working memory. However, there is some evidence that maintained content can also be decoded in areas outside of visual cortex, including parietal and frontal cortex. We suggest that the ability to maintain representations during working memory is a general property of cortex, not restricted to specific areas, and argue that it is important to consider the nature of the information that must be maintained. Such information-content is critically determined by the task and the recruitment of specific regions during visual working memory will be both task- and stimulus-dependent. Thus, the common finding of maintained information in visual, but not parietal or prefrontal, cortex may be more of a reflection of the need to maintain specific types of visual information and not of a privileged role of visual cortex in maintenance.
Task preparation has traditionally been thought to rely upon persistent representations of instructions that permit their execution after delays. Accumulating evidence suggests, however, that accurate retention of task knowledge can be insufficient for successful performance. Here, we hypothesized that instructed facts would be organized into a task set; a temporary coding scheme that proactively tunes sensorimotor pathways according to instructions to enable highly efficient "reflex-like" performance. We devised a paradigm requiring either implementation or memorization of novel stimulus-response mapping instructions, and used multivoxel pattern analysis of neuroimaging data to compare neural coding of instructions during the pretarget phase. Although participants could retain instructions under both demands, we observed striking differences in their representation. To-be-memorized instructions could only be decoded from mid-occipital and posterior parietal cortices, consistent with previous work on visual short-term memory storage. In contrast, to-be-implemented instructions could also be decoded from frontoparietal "multiple-demand" regions, and dedicated visual areas, implicated in processing instructed stimuli. Neural specificity in the latter moreover correlated with performance speed only when instructions were prepared, likely reflecting the preconfiguration of instructed decision circuits. Together, these data illuminate how the brain proactively optimizes performance, and help dissociate neural mechanisms supporting task control and short-term memory storage.
Objective metrics of technical performance (e.g., dexterity, time, and path length) are insufficient to fully characterize operator skill level, which may be encoded deep within neural function. Unlike reports that capture plasticity across days or weeks, this articles studies long-term plasticity in functional connectivity that occurs over years of professional task practice. Optical neuroimaging data are acquired from professional surgeons of varying experience on a complex bimanual coordination task with the aim of investigating learning-related disparity in frontal lobe functional connectivity that arises as a consequence of motor skill level. The results suggest that prefrontal and premotor seed connectivity is more critical during naive versus expert performance. Given learning-related differences in connectivity, a least-squares support vector machine with a radial basis function kernel is employed to evaluate skill level using connectivity data. The results demonstrate discrimination of operator skill level with accuracy 0.82 and Multiclass Matthew's Correlation Coefficient 0.70. Furthermore, these indices are improved when local (i.e., within-region) rather than inter-regional (i.e., between-region) frontal connectivity is considered (p = 0.002). The results suggest that it is possible to classify operator skill level with good accuracy from functional connectivity data, upon which objective assessment and neurofeedback may be used to improve operator performance during technical skill training.
The representation of object identity is fundamental to human vision. Using fMRI and multivoxel pattern analysis, here we report the representation of highly abstract object identity information in human parietal cortex. Specifically, in superior intraparietal sulcus (IPS), a region previously shown to track visual short-term memory capacity, we found object identity representations for famous faces varying freely in viewpoint, hairstyle, facial expression, and age; and for well known cars embedded in different scenes, and shown from different viewpoints and sizes. Critically, these parietal identity representations were behaviorally relevant as they closely tracked the perceived face-identity similarity obtained in a behavioral task. Meanwhile, the task-activated regions in prefrontal and parietal cortices (excluding superior IPS) did not exhibit such abstract object identity representations. Unlike previous studies, we also failed to observe identity representations in posterior ventral and lateral visual object-processing regions, likely due to the greater amount of identity abstraction demanded by our stimulus manipulation here. Our MRI slice coverage precluded us from examining identity representation in anterior temporal lobe, a likely region for the computing of identity information in the ventral region. Overall, we show that human parietal cortex, part of the dorsal visual processing pathway, is capable of holding abstract and complex visual representations that are behaviorally relevant. These results argue against a "content-poor" view of the role of parietal cortex in attention. Instead, the human parietal cortex seems to be "content rich" and capable of directly participating in goal-driven visual information representation in the brain.
In the perceptual domain, it has been shown that the human brain is strongly shaped through experience, leading to expertise in highly-skilled professionals. What has remained unclear is whether specialization also shapes brain networks underlying mental imagery. In our fMRI study, we aimed to uncover modality-specific mental imagery specialization of film experts. Using multi-voxel pattern analysis we decoded from brain activity of professional cinematographers and sound designers whether they were imagining sounds or images of particular film clips. In each expert group distinct multi-voxel patterns, specific for the modality of their expertise, were found during classification of imagery modality. These patterns were mainly localized in the occipito-temporal and parietal cortex for cinematographers and in the auditory cortex for sound designers. We also found generalized patterns across perception and imagery that were distinct for the two expert groups: they involved frontal cortex for the cinematographers and temporal cortex for the sound designers. Notably, the mental representations of film clips and sounds of cinematographers contained information that went beyond modality-specificity. We were able to successfully decode the implicit presence of film genre from brain activity during mental imagery in cinematographers. The results extend existing neuroimaging literature on expertise into the domain of mental imagery and show that experience in visual versus auditory imagery can alter the representation of information in modality-specific association cortices.
Humans can flexibly select locations, features, or objects in a visual scene for prioritized processing. Although it is relatively straightforward to manipulate location- and feature-based attention, it is difficult to isolate object-based selection. Because objects are always composed of features, studies of object-based selection can often be interpreted as the selection of a combination of locations and features. Here we examined the neural representation of attentional priority in a paradigm that isolated object-based selection. Participants viewed two superimposed gratings that continuously changed their color, orientation, and spatial frequency, such that the gratings traversed the same exact feature values within a trial. Participants were cued at the beginning of each trial to attend to one or the other grating to detect a brief luminance increment, while their brain activity was measured with fMRI. Using multi-voxel pattern analysis, we were able to decode the attended grating in a set of frontoparietal areas, including anterior intraparietal sulcus (IPS), frontal eye field (FEF), and inferior frontal junction (IFJ). Thus, a perceptually varying object can be represented by patterned neural activity in these frontoparietal areas. We suggest that these areas can encode attentional priority for abstract, high-level objects independent of their locations and features.
Perceiving other people's actions triggers activity in premotor and parietal areas, brain areas also involved in executing and sensing our own actions. Paralleling this phenomenon, observing emotional states (including pain) in others is associated with activity in the same brain areas as activated when experiencing similar emotions directly. This emotion perception associated activity has been shown to be affected by the perceived fairness of the actor, and in-group membership more generally. Here, we examine whether action observation associated brain activity is also affected by the perceived social fairness of the actors. Perceived fairness was manipulated using an alternating iterated Prisoner's Dilemma game between the participant and two confederates, one of whom played fairly and the other unfairly. During fMRI scanning the participants watched movies of the confederates performing object-directed hand actions, and then performed hand actions themselves. Mass-univariate analysis showed that observing the actions triggered robust activation in regions associated with action execution, but failed to identify a strong modulation of this activation based on perceived fairness. Multivariate pattern analysis, however, identified clusters potentially carrying information about the perceived fairness of the actor in the middle temporal gyrus, left postcentral gyrus, right inferior parietal lobule, right middle cingulate cortex, right angular gyrus, and right superioroccipital gyrus. Despite being identified by a whole-brain searchlight analysis (and so without anatomical restriction), these clusters fall into areas frequently associated with action observation. We conclude that brain activity during action observation may be modulated by perceived fairness, but such modulation is subtle; robust activity is associated with observing the actions of both fair and unfair individuals.
The prefrontal cortex is activated during working memory, as evidenced by fMRI results in human studies and neurophysiological recordings in animal models. Persistent activity during the delay period of working memory tasks, after the offset of stimuli that subjects are required to remember, has traditionally been thought of as the neural correlate of working memory. In the last few years several findings have cast doubt on the role of this activity. By some accounts, activity in other brain areas, such as the primary visual and posterior parietal cortex, is a better predictor of information maintained in visual working memory and working memory performance; dynamic patterns of activity may convey information without requiring persistent activity at all; and prefrontal neurons may be ill-suited to represent non-spatial information about the features and identity of remembered stimuli. Alternative interpretations about the role of the prefrontal cortex have thus been suggested, such as that it provides a top-down control of information represented in other brain areas, rather than maintaining a working memory trace itself. Here we review evidence for and against the role of prefrontal persistent activity, with a focus on visual neurophysiology. We show that persistent activity predicts behavioral parameters precisely in working memory tasks. We illustrate that prefrontal cortex represents features of stimuli other than their spatial location, and that this information is largely absent from early cortical areas during working memory. We examine memory models not dependent on persistent activity, and conclude that each of those models could mediate only a limited range of memory-dependent behaviors. We review activity decoded from brain areas other than the prefrontal cortex during working memory and demonstrate that these areas alone cannot mediate working memory maintenance, particularly in the presence of distractors. We finally discuss the discrepancy between BOLD activation and spiking activity findings, and point out that fMRI methods do not currently have the spatial resolution necessary to decode information within the prefrontal cortex, which is likely organized at the micrometer scale. Therefore, we make the case that prefrontal persistent activity is both necessary and sufficient for the maintenance of information in working memory.
One major goal in decision neuroscience is to investigate the neuronal mechanisms being responsible for the computation of product preferences. The aim of the present fMRI study was to investigate whether similar patterns of brain activity, reflecting category dependent and category independent preference signals, can be observed in case of different food product categories (i.e. chocolate bars and salty snacks). To that end we used a multivariate searchlight approach in which a linear support vector machine (l-SVM) was trained to distinguish preferred from non-preferred chocolate bars and subsequently tested its predictive power in case of chocolate bars (within category prediction) and salty snacks (across category prediction). Preferences were measured by a binary forced choice decision paradigm before the fMRI task. In the scanner, subjects saw only one product per trial which they had to rate after presentation. Consistent with previous multi voxel pattern analysis (MVPA) studies, we found category dependent preference signals in the ventral parts of medial prefrontal cortex (mPFC), but also in dorsal anterior cingulate cortex (dACC) and dorsolateral prefrontal cortex (dlPFC). Category independent preference signals were observed in the dorsal parts of mPFC, dACC, and dlPFC. While the first two results have also been reported in a closely related study, the activation in dlPFC is new in this context. We propose that the dlPFC activity does not reflect the products' value computation per se, but rather a modulatory signal which is computed in anticipation of the forthcoming product rating after stimulus presentation. Furthermore we postulate that this kind of dlPFC activation emerges only if the anticipated choices fall into the domain of primary rewards, such as foods. Thus, in contrast to previous studies which investigated preference decoding for stimuli from utterly different categories, the present study revealed some food domain specific aspects of preference processing in the human brain.
Pattern recognition analysis (PRA) applied to functional magnetic resonance imaging (fMRI) has been used to decode cognitive processes and identify possible biomarkers for mental illness. In the present study, we investigated whether the positive affect (PA) or negative affect (NA) personality traits could be decoded from patterns of brain activation in response to a human threat using a healthy sample.
Machine learning or MVPA (Multi Voxel Pattern Analysis) studies have shown that the neural representation of quantities of objects can be decoded from fMRI patterns, in cases where the quantities were visually displayed. Here we apply these techniques to investigate whether neural representations of quantities depicted in one modality (say, visual) can be decoded from brain activation patterns evoked by quantities depicted in the other modality (say, auditory). The main finding demonstrated, for the first time, that quantities of dots were decodable by a classifier that was trained on the neural patterns evoked by quantities of auditory tones, and vice-versa. The representations that were common across modalities were mainly right-lateralized in frontal and parietal regions. A second finding was that the neural patterns in parietal cortex that represent quantities were common across participants. These findings demonstrate a common neuronal foundation for the representation of quantities across sensory modalities and participants and provide insight into the role of parietal cortex in the representation of quantity information.
Extant neuroimaging data implicate frontoparietal and medial-temporal lobe regions in episodic retrieval, and the specific pattern of activity within and across these regions is diagnostic of an individual's subjective mnemonic experience. For example, in laboratory-based paradigms, memories for recently encoded faces can be accurately decoded from single-trial fMRI patterns [Uncapher, M. R., Boyd-Meredith, J. T., Chow, T. E., Rissman, J., & Wagner, A. D. Goal-directed modulation of neural memory patterns: Implications for fMRI-based memory detection. Journal of Neuroscience, 35, 8531-8545, 2015; Rissman, J., Greely, H. T., & Wagner, A. D. Detecting individual memories through the neural decoding of memory states and past experience. Proceedings of the National Academy of Sciences, U.S.A., 107, 9849-9854, 2010]. Here, we investigated the neural patterns underlying memory for real-world autobiographical events, probed at 1- to 3-week retention intervals as well as whether distinct patterns are associated with different subjective memory states. For 3 weeks, participants (n = 16) wore digital cameras that captured photographs of their daily activities. One week later, they were scanned while making memory judgments about sequences of photos depicting events from their own lives or events captured by the cameras of others. Whole-brain multivoxel pattern analysis achieved near-perfect accuracy at distinguishing correctly recognized events from correctly rejected novel events, and decoding performance did not significantly vary with retention interval. Multivoxel pattern classifiers also differentiated recollection from familiarity and reliably decoded the subjective strength of recollection, of familiarity, or of novelty. Classification-based brain maps revealed dissociable neural signatures of these mnemonic states, with activity patterns in hippocampus, medial PFC, and ventral parietal cortex being particularly diagnostic of recollection. Finally, a classifier trained on previously acquired laboratory-based memory data achieved reliable decoding of autobiographical memory states. We discuss the implications for neuroscientific accounts of episodic retrieval and comment on the potential forensic use of fMRI for probing experiential knowledge.
Many decisions require a context-dependent mapping from sensory evidence to action. The capacity for flexible information processing of this sort is thought to depend on a cognitive control system in frontoparietal cortex, but the costs and limitations of control entail that its engagement should be minimized. Here, we show that humans reduce demands on control by exploiting statistical structure in their environment. Using a context-dependent perceptual discrimination task and model-based analyses of behavioral and neuroimaging data, we found that predictions about task context facilitated decision making and that a quantitative measure of context prediction error accounted for graded engagement of the frontoparietal control network. Within this network, multivariate analyses further showed that context prediction error enhanced the representation of task context. These results indicate that decision making is adaptively tuned by experience to minimize costs while maintaining flexibility.
In an fMRI study, participants were trained to play a complex video game. They were scanned early and then again after substantial practice. While better players showed greater activation in one region (right dorsal striatum) their relative skill was better diagnosed by considering the sequential structure of whole brain activation. Using a cognitive model that played this game, we extracted a characterization of the mental states that are involved in playing a game and the statistical structure of the transitions among these states. There was a strong correspondence between this measure of sequential structure and the skill of different players. Using multi-voxel pattern analysis, it was possible to recognize, with relatively high accuracy, the cognitive states participants were in during particular scans. We used the sequential structure of these activation-recognized states to predict the skill of individual players. These findings indicate that important features about information-processing strategies can be identified from a model-based analysis of the sequential structure of brain activation.
The quest for a putative human homolog of the reaching-grasping network identified in monkeys has been the focus of many neuropsychological and neuroimaging studies in recent years. These studies have shown that the network underlying reaching-only and reach-to-grasp movements includes the superior parieto-occipital cortex (SPOC), the anterior part of the human intraparietal sulcus (hAIP), the ventral and the dorsal portion of the premotor cortex, and the primary motor cortex (M1). Recent evidence for a wider frontoparietal network coding for different aspects of reaching-only and reach-to-grasp actions calls for a more fine-grained assessment of the reaching-grasping network in humans by exploiting pattern decoding methods (multivoxel pattern analysis--MVPA).
Rostrolateral prefrontal cortex (RLPFC) is widely appreciated to support higher cognitive functions, including analogical reasoning and episodic memory retrieval. However, these tasks have typically been studied in isolation, and thus it is unclear whether they involve common or distinct RLPFC mechanisms. Here, we introduce a novel functional magnetic resonance imaging (fMRI) task paradigm to compare brain activity during reasoning and memory tasks while holding bottom-up perceptual stimulation and response demands constant. Univariate analyses on fMRI data from twenty participants identified a large swath of left lateral prefrontal cortex, including RLPFC, that showed common engagement on reasoning trials with valid analogies and memory trials with accurately retrieved source details. Despite broadly overlapping recruitment, multi-voxel activity patterns within left RLPFC reliably differentiated these two trial types, highlighting the presence of at least partially distinct information processing modes. Functional connectivity analyses demonstrated that while left RLPFC showed consistent coupling with the fronto-parietal control network across tasks, its coupling with other cortical areas varied in a task-dependent manner. During the memory task, this region strengthened its connectivity with the default mode and memory retrieval networks, whereas during the reasoning task it coupled more strongly with a nearby left prefrontal region (BA 45) associated with semantic processing, as well as with a superior parietal region associated with visuospatial processing. Taken together, these data suggest a domain-general role for left RLPFC in monitoring and/or integrating task-relevant knowledge representations and showcase how its function cannot solely be attributed to episodic memory or analogical reasoning computations.
The ability to learn is assumed to support successful recovery and rehabilitation therapy after stroke. Hence, learning impairments may reduce the recovery potential. Here, the hypothesis is tested that stroke survivors have deficits in feedback-driven implicit learning. Stroke survivors (n=30) and healthy age-matched control subjects (n=21) learned a probabilistic classification task with brain activation measured using functional magnetic resonance imaging in a subset of these individuals (17 stroke and 10 controls). Stroke subjects learned slower than controls to classify cues. After being rewarded with a smiley face, they were less likely to give the same response when the cue was repeated. Stroke subjects showed reduced brain activation in putamen, pallidum, thalamus, frontal and prefrontal cortices and cerebellum when compared with controls. Lesion analysis identified those stroke survivors as learning-impaired who had lesions in frontal areas, putamen, thalamus, caudate and insula. Lesion laterality had no effect on learning efficacy or brain activation. These findings suggest that stroke survivors have deficits in reinforcement learning that may be related to dysfunctional processing of feedback-based decision-making, reward signals and working memory.
Humans use external cues and prior knowledge about the environment to monitor their positions during spatial navigation. View expectation is essential for correlating scene views with a cognitive map. To determine how the brain performs view expectation during spatial navigation, we applied a multiple parallel decoding technique to functional magnetic resonance imaging (fMRI) when human participants performed scene choice tasks in learned maze navigation environments. We decoded participants' view expectation from fMRI signals in parietal and medial prefrontal cortices, whereas activity patterns in occipital cortex represented various types of external cues. The decoder's output reflected participants' expectations even when they were wrong, corresponding to subjective beliefs opposed to objective reality. Thus, view expectation is subjectively represented in human brain, and the fronto-parietal network is involved in integrating external cues and prior knowledge during spatial navigation.
How do people understand the minds of others? Existing psychological theories have suggested a number of dimensions that perceivers could use to make sense of others' internal mental states. However, it remains unclear which of these dimensions, if any, the brain spontaneously uses when we think about others. The present study used multivoxel pattern analysis (MVPA) of neuroimaging data to identify the primary organizing principles of social cognition. We derived four unique dimensions of mental state representation from existing psychological theories and used functional magnetic resonance imaging to test whether these dimensions organize the neural encoding of others' mental states. MVPA revealed that three such dimensions could predict neural patterns within the medial prefrontal and parietal cortices, temporoparietal junction, and anterior temporal lobes during social thought: rationality, social impact, and valence. These results suggest that these dimensions serve as organizing principles for our understanding of other people.
In this study, healthy volunteers were scanned using functional magnetic resonance imaging (fMRI) to investigate the neural systems involved in processing the threatening content conveyed via visually presented "threat words." The neural responses elicited by these words were compared to those elicited by matched neutral control words. The results demonstrate that linguistic threat, when presented in written form, can selectively engage areas of lateral temporal and inferior frontal cortex, distinct from the core language areas implicated in aphasia. Additionally, linguistic threat modulates neural activity in visceral/emotional systems (amygdala, parahippocampal gyrus and periaqueductal gray), and at earlier stages of the visual-linguistic processing stream involved in visual word form representations (ventral occipitotemporal cortex). We propose a model whereby limbic activation modulates activity at multiple nodes along the visual-linguistic-semantic processing stream, including a perisylvian "semantic access network" involved in decoding word meaning, suggesting a dynamic interplay between feedforward and feedback processes.
This study investigates to what extent social and competence traits are represented in a similar or different neural trait code. To localize these trait codes, we used functional magnetic resonance imaging repetition suppression, which is a rapid reduction of neuronal responses upon repeated presentation of the same implied trait. Participants had to infer an agent's trait from brief trait-implying behavioral descriptions. In each trial, the critical target sentence was preceded by a prime sentence that implied the same trait or a different competence-related trait which was also opposite in valence. The results revealed robust repetition suppression from prime to target in the ventral medial prefrontal cortex (mPFC) given a similar (social) as well as a dissimilar (competence) prime. The suppression given a similar prime confirms earlier research demonstrating that a trait code is represented in the ventral mPFC. The suppression given a dissimilar prime is interpreted as indicating that participants categorize a combination of competence and social information into novel subcategories, reflecting nice (but incompetent) or nerdy (but socially awkward) traits. A multi-voxel pattern analysis broadly confirmed these results, and pinpointed the inferior parietal cortex, cerebellum, temporo-parietal junction and mPFC as areas that differentiate between social and competence traits.
Multivariate pattern analysis can reveal new information from neuroimaging data to illuminate human cognition and its disturbances. Here, we develop a methodological approach, based on multivariate statistical/machine learning and time series analysis, to discern cognitive processing stages from functional magnetic resonance imaging (fMRI) blood oxygenation level dependent (BOLD) time series. We apply this method to data recorded from a group of healthy adults whilst performing a virtual reality version of the delayed win-shift radial arm maze (RAM) task. This task has been frequently used to study working memory and decision making in rodents. Using linear classifiers and multivariate test statistics in conjunction with time series bootstraps, we show that different cognitive stages of the task, as defined by the experimenter, namely, the encoding/retrieval, choice, reward and delay stages, can be statistically discriminated from the BOLD time series in brain areas relevant for decision making and working memory. Discrimination of these task stages was significantly reduced during poor behavioral performance in dorsolateral prefrontal cortex (DLPFC), but not in the primary visual cortex (V1). Experimenter-defined dissection of time series into class labels based on task structure was confirmed by an unsupervised, bottom-up approach based on Hidden Markov Models. Furthermore, we show that different groupings of recorded time points into cognitive event classes can be used to test hypotheses about the specific cognitive role of a given brain region during task execution. We found that whilst the DLPFC strongly differentiated between task stages associated with different memory loads, but not between different visual-spatial aspects, the reverse was true for V1. Our methodology illustrates how different aspects of cognitive information processing during one and the same task can be separated and attributed to specific brain regions based on information contained in multivariate patterns of voxel activity.
The human brain encodes experience in an integrative fashion by binding together the various features of an event (i.e., stimuli and responses) into memory "event files." A subsequent reoccurrence of an event feature can then cue the retrieval of the memory file to "prime" cognition and action. Intriguingly, recent behavioral studies indicate that, in addition to linking concrete stimulus and response features, event coding may also incorporate more abstract, "internal" event features such as attentional control states. In the present study, we used fMRI in healthy human volunteers to determine the neural mechanisms supporting this type of holistic event binding. Specifically, we combined fMRI with a task protocol that dissociated the expression of event feature-binding effects pertaining to concrete stimulus and response features, stimulus categories, and attentional control demands. Using multivariate neural pattern classification, we show that the hippocampus and putamen integrate event attributes across all of these levels in conjunction with other regions representing concrete-feature-selective (primarily visual cortex), category-selective (posterior frontal cortex), and control demand-selective (insula, caudate, anterior cingulate, and parietal cortex) event information. Together, these results suggest that the hippocampus and putamen are involved in binding together holistic event memories that link physical stimulus and response characteristics with internal representations of stimulus categories and attentional control states. These bindings then presumably afford shortcuts to adaptive information processing and response selection in the face of recurring events.
The use of top-down cognitive control mechanisms to regulate emotional responses as circumstances change is critical for mental and physical health. Several theoretical models of emotion regulation have been postulated; it remains unclear, however, in which brain regions emotion regulation goals (e.g., the downregulation of fear) are represented. Here, we examined the neural mechanisms of regulating emotion using fMRI and identified brain regions representing reappraisal goals. Using a multimethodological analysis approach, combining standard activation-based and pattern-information analyses, we identified a distributed network of lateral frontal, temporal, and parietal regions implicated in reappraisal and within it, a core system that represents reappraisal goals in an abstract, stimulus-independent fashion. Within this core system, the neural pattern-separability in a subset of regions including the left inferior frontal gyrus, middle temporal gyrus, and inferior parietal lobe was related to the success in emotion regulation. Those brain regions might link the prefrontal control regions with the subcortical affective regions. Given the strong association of this subsystem with inner speech functions and semantic memory, we conclude that those cognitive mechanisms may be used for orchestrating emotion regulation. Hum Brain Mapp 37:600-620, 2016.  2015 Wiley Periodicals, Inc.
How motor maps are organized while imagining actions is an intensely debated issue. It is particularly unclear whether motor imagery relies on action-specific representations in premotor and posterior parietal cortices. This study tackled this issue by attempting to decode the content of motor imagery from spatial patterns of Blood Oxygen Level Dependent (BOLD) signals recorded in the frontoparietal motor imagery network. During fMRI-scanning, 20 right-handed volunteers worked on three experimental conditions and one baseline condition. In the experimental conditions, they had to imagine three different types of right-hand actions: an aiming movement, an extension-flexion movement, and a squeezing movement. The identity of imagined actions was decoded from the spatial patterns of BOLD signals they evoked in premotor and posterior parietal cortices using multivoxel pattern analysis. Results showed that the content of motor imagery (i.e., the action type) could be decoded significantly above chance level from the spatial patterns of BOLD signals in both frontal (PMC, M1) and parietal areas (SPL, IPL, IPS). An exploratory searchlight analysis revealed significant clusters motor- and motor-associated cortices, as well as in visual cortices. Hence, the data provide evidence that patterns of activity within premotor and posterior parietal cortex vary systematically with the specific type of hand action being imagined.
According to the dual-route model, a printed string of letters can be processed by either a grapheme-to-phoneme conversion (GPC) route or a lexical-semantic route. Although meta-analyses of the imaging literature support the existence of distinct but interacting reading procedures, individual neuroimaging studies that explored neural correlates of reading yielded inconclusive results. We used a list-manipulation paradigm to provide a fresh empirical look at this issue and to isolate specific areas that underlie the two reading procedures. In a lexical condition, we embedded disyllabic Italian words (target stimuli) in lists of either loanwords or trisyllabic Italian words with unpredictable stress position. In a GPC condition, similar target stimuli were included within lists of pseudowords. The procedure was designed to induce participants to emphasize either the lexical-semantic or the GPC reading procedure, while controlling for possible linguistic confounds and keeping the reading task requirements stable across the two conditions. Thirty-three adults participated in the behavioral study, and 20 further adult participants were included in the fMRI study. At the behavioral level, we found sizeable effects of the framing manipulations that included slower voice onset times for stimuli in the pseudoword frames. At the functional anatomical level, the occipital and temporal regions, and the intraparietal sulcus were specifically activated when subjects were reading target words in a lexical frame. The inferior parietal and anterior fusiform cortex were specifically activated in the GPC condition. These patterns of activation represented a valid classifying model of fMRI images associated with target reading in both frames in the multi-voxel pattern analyses. Further activations were shared by the two procedures in the occipital and inferior parietal areas, in the premotor cortex, in the frontal regions and the left supplementary motor area. These regions are most likely involved in either early input or late output processes.
The human brain consists of a network of regions that are engaged when one observes the movements of others. Observing unexpected movements, as defined by the context, often elicits greater activity, particularly in the right posterior superior temporal sulcus (pSTS). This implies that observers use contextual information to form expectations about an agent's goal and subsequent movements. The current study sought to identify regions that support the formation of these context-dependent expectations, with the pSTS being one candidate, given the consistent contextual modulation of its activity. We presented participants with fictitious individuals who had emotion-dependent food preferences, and instructed participants to indicate which food they expected each individual to choose based on the individual's current emotional state. Each individual's preference and emotional state therefore created a context that informed the observer's expectation of the individual's choice. Multi-voxel pattern analysis (MVPA) was used to assess if these different contexts could be discriminated in the pSTS and elsewhere in the brain. No evidence for context discrimination was found in the pSTS. Context discrimination was found instead a network of other brain regions including the anterior medial prefrontal cortex (amPFC), bilateral parietal cortex, left middle temporal gyrus (L MTG) and left anterior temporal lobe (L ATL), which have been previously associated with context processing, and semantic and memory retrieval. All together, these regions possibly support the formation of context-dependent expectations of an agent's goal.
Neuroimaging studies of recognition memory have identified distinct patterns of cortical activity associated with two sets of cognitive processes: Recollective processes supporting retrieval of information specifying a probe item's original source are associated with the posterior hippocampus, ventral posterior parietal cortex, and medial pFC. Familiarity processes supporting the correct identification of previously studied probes (in the absence of a recollective response) are associated with activity in anterior medial temporal lobe (MTL) structures including the perirhinal cortex and anterior hippocampus, in addition to lateral prefrontal and dorsal posterior parietal cortex. Here, we address an open question in the cognitive neuroscientific literature: To what extent are these same neurocognitive processes engaged during an internally directed memory search task like free recall? We recorded fMRI activity while participants performed a series of free recall and source recognition trials, and we used a combination of univariate and multivariate analysis techniques to compare neural activation profiles across the two tasks. Univariate analyses showed that posterior MTL regions were commonly associated with recollective processes during source recognition and with free recall responses. Prefrontal and posterior parietal regions were commonly associated with familiarity processes and free recall responses, whereas anterior MTL regions were only associated with familiarity processes during recognition. In contrast with the univariate results, free recall activity patterns characterized using multivariate pattern analysis did not reliably match the neural patterns associated with recollective processes. However, these free recall patterns did reliably match patterns associated with familiarity processes, supporting theories of memory in which common cognitive mechanisms support both item recognition and free recall.
Regions in human lateral and ventral occipitotemporal cortices (OTC) respond selectively to pictures of the human body and its parts. What are the organizational principles underlying body part responses in these regions? Here we used representational similarity analysis (RSA) of fMRI data to test multiple possible organizational principles: shape similarity, physical proximity, cortical homunculus proximity, and semantic similarity. Participants viewed pictures of whole persons, chairs, and eight body parts (hands, arms, legs, feet, chests, waists, upper faces, and lower faces). The similarity of multivoxel activity patterns for all body part pairs was established in whole person-selective OTC regions. The resulting neural similarity matrices were then compared with similarity matrices capturing the hypothesized organizational principles. Results showed that the semantic similarity model best captured the neural similarity of body parts in lateral and ventral OTC, which followed an organization in three clusters: (1) body parts used as action effectors (hands, feet, arms, and legs), (2) noneffector body parts (chests and waists), and (3) face parts (upper and lower faces). Whole-brain RSA revealed, in addition to OTC, regions in parietal and frontal cortex in which neural similarity was related to semantic similarity. In contrast, neural similarity in occipital cortex was best predicted by shape similarity models. We suggest that the semantic organization of body parts in high-level visual cortex relates to the different functions associated with the three body part clusters, reflecting the unique processing and connectivity demands associated with the different types of information (e.g., action, social) different body parts (e.g., limbs, faces) convey. Significance statement: While the organization of body part representations in motor and somatosensory cortices has been well characterized, the principles underlying body part representations in visual cortex have not yet been explored. In the present fMRI study we used multivoxel pattern analysis and representational similarity analysis to characterize the organization of body maps in human occipitotemporal cortex (OTC). Results indicate that visual and shape dimensions do not fully account for the organization of body part representations in OTC. Instead, the representational structure of body maps in OTC appears strongly related to functional-semantic properties of body parts. We suggest that this organization reflects the unique processing and connectivity demands associated with the different types of information different body parts convey.
Allocating attentional resources to currently relevant information in a dynamically changing environment is critical to goal-directed behavior. Previous studies in nonhuman primates (NHPs) have demonstrated modulation of neural representations of stimuli, in particular visual categorizations, by behavioral significance in the lateral prefrontal cortex. In the human brain, a network of frontal and parietal regions, the "multiple demand" (MD) system, is involved in cognitive and attentional control. To test for the effect of behavioral significance on categorical discrimination in the MD system in humans, we adapted a previously used task in the NHP and used multivoxel pattern analysis for fMRI data. In a cued-detection categorization task, participants detected whether an image from one of two target visual categories was present in a display. Our results revealed that categorical discrimination is modulated by behavioral relevance, as measured by the distributed pattern of response across the MD network. Distinctions between categories with different behavioral status (e.g., a target and a nontarget) were significantly discriminated. Category distinctions that were not behaviorally relevant (e.g., between two targets) were not discriminated. Other aspects of the task that were orthogonal to the behavioral decision did not modulate categorical discrimination. In a high visual region, the lateral occipital complex, modulation by behavioral relevance was evident in its posterior subregion but not in the anterior subregion. The results are consistent with the view of the MD system as involved in top-down attentional and cognitive control by selective coding of task-relevant discriminations. Significance statement: Control of cognitive demands fundamentally involves flexible allocation of attentional resources depending on a current behavioral context. Essential to such a mechanism is the ability to select currently relevant information and at the same time filter out information that is irrelevant. In an fMRI study, we measured distributed patterns of activity for objects from different visual categories while manipulating the behavioral relevance of the categorical distinctions. In a network of frontal and parietal cortical regions, the multiple-demand (MD) network, patterns reflected category distinctions that were relevant to behavior. Patterns could not be used to make task-irrelevant category distinctions. These findings demonstrate the ability of the MD network to implement complex goal-directed behavior by focused attention.
It is now established that the perception of tools engages a left-lateralized network of frontoparietal and occipitotemporal cortical regions. Nevertheless, the precise computational role played by these areas is not yet well understood. To address this question, we used functional MRI to investigate the distribution of responses to pictures of tools and hands relative to other object categories in the so-called "tool" areas. Although hands and tools are visually not alike and belong to different object categories, these are both functionally linked when considering the common role of hands and tools in object manipulation. This distinction can provide insight into the differential functional role of areas within the "tool" network. Results demonstrated that images of hands and tools activate a common network of brain areas in the left intraparietal sulcus (IPS), left lateral occipitotemporal cortex (LOTC) and ventral occipitotemporal cortex (VOTC). Importantly, multivoxel pattern analysis revealed that the distribution of hand and tool response patterns in these regions differs. These observations provide support for the idea that the left IPS, left LOTC and VOTC might have distinct computational roles with regard to tool use. Specifically, these results suggest that while left IPS supports tool action-related computations and VOTC primarily encodes category specific aspects of objects, left LOTC bridges ventro occipitotemporal perception-related and parietal action-related representations by encoding both types of object information.
Brain development is adversely affected by preterm birth. Magnetic resonance image analysis has revealed a complex fusion of structural alterations across all tissue compartments that are apparent by term-equivalent age, persistent into adolescence and adulthood, and associated with wide-ranging neurodevelopment disorders. Although functional MRI has revealed the relatively advanced organisational state of the neonatal brain, the full extent and nature of functional disruptions following preterm birth remain unclear. In this study, we apply machine-learning methods to compare whole-brain functional connectivity in preterm infants at term-equivalent age and healthy term-born neonates in order to test the hypothesis that preterm birth results in specific alterations to functional connectivity by term-equivalent age. Functional connectivity networks were estimated in 105 preterm infants and 26 term controls using group-independent component analysis and a graphical lasso model. A random forest-based feature selection method was used to identify discriminative edges within each network and a nonlinear support vector machine was used to classify subjects based on functional connectivity alone. We achieved 80% cross-validated classification accuracy informed by a small set of discriminative edges. These edges connected a number of functional nodes in subcortical and cortical grey matter, and most were stronger in term neonates compared to those born preterm. Half of the discriminative edges connected one or more nodes within the basal ganglia. These results demonstrate that functional connectivity in the preterm brain is significantly altered by term-equivalent age, confirming previous reports of altered connectivity between subcortical structures and higher-level association cortex following preterm birth.
The hippocampal memory system is thought to alternate between two opposing processing states: encoding and retrieval. When present experience overlaps with past experience, this creates a potential tradeoff between encoding the present and retrieving the past. This tradeoff may be resolved by memory integration-that is, by forming a mnemonic representation that links present experience with overlapping past experience. Here, we used fMRI decoding analyses to predict when - and establish how - past and present experiences become integrated in memory. In an initial experiment, we alternately instructed subjects to adopt encoding, retrieval or integration states during overlapping learning. We then trained across-subject pattern classifiers to 'read out' the instructed processing states from fMRI activity patterns. We show that an integration state was clearly dissociable from encoding or retrieval states. Moreover, trial-by-trial fluctuations in decoded evidence for an integration state during learning reliably predicted behavioral expressions of successful memory integration. Strikingly, the decoding algorithm also successfully predicted specific instances of spontaneous memory integration in an entirely independent sample of subjects for whom processing state instructions were not administered. Finally, we show that medial prefrontal cortex and hippocampus differentially contribute to encoding, retrieval, and integration states: whereas hippocampus signals the tradeoff between encoding vs. retrieval states, medial prefrontal cortex actively represents past experience in relation to new learning.
Neocortical structures typically only support slow acquisition of declarative memory; however, learning through fast mapping may facilitate rapid learning-induced cortical plasticity and hippocampal-independent integration of novel associations into existing semantic networks. During fast mapping the meaning of new words and concepts is inferred, and durable novel associations are incidentally formed, a process thought to support early childhood's exuberant learning. The anterior temporal lobe, a cortical semantic memory hub, may critically support such learning. We investigated encoding of semantic associations through fast mapping using fMRI and multivoxel pattern analysis. Subsequent memory performance following fast mapping was more efficiently predicted using anterior temporal lobe than hippocampal voxels, while standard explicit encoding was best predicted by hippocampal activity. Searchlight algorithms revealed additional activity patterns that predicted successful fast mapping semantic learning located in lateral occipitotemporal and parietotemporal neocortex and ventrolateral prefrontal cortex. By contrast, successful explicit encoding could be classified by activity in medial and dorsolateral prefrontal and parahippocampal cortices. We propose that fast mapping promotes incidental rapid integration of new associations into existing neocortical semantic networks by activating related, nonoverlapping conceptual knowledge. In healthy adults, this is better captured by unique anterior and lateral temporal lobe activity patterns, while hippocampal involvement is less predictive of this kind of learning.
Effective mental sub-health early warning mechanism is of great significance in the protection of individual mental health. The traditional mental health assessment method is mainly based on questionnaire surveys, which may have some uncertainties. In this study, based on the relationship between the default mode network (DMN) and the mental health status, we proposed a human mental sub-health early warning method by utilizing two-fold support vector machine (SVM) model, where seafarers' fMRI data analysis was utilized as an example. The method firstly constructed a structural-functional DMN template by combining the anatomical automatic labeling template with the functional DMN extracted by independent component analysis. Then, it put forward a two-fold SVM-based classifier, with one-class SVM utilized for the training of the initial classifier and two-class SVM utilized to refine the classification performance, to identify seafarers' mental health status by utilizing the correlation coefficients (CCs) among the areas of structural-functional DMN as the features. The experimental results showed that the proposed model could discriminate the seafarers with DMN function alteration from the healthy control (HC) effectively, and further the results demonstrated that when compared with the HC group, the brain functional disorders of the mental sub-healthy seafarers mainly manifested as follows: the functional connectivity of DMN had obvious alteration; the CCs among the different DMN regions were significant lower; the regional homogeneity decreased in parts of the prefrontal cortex and increased in multi-regions of the parietal, temporal and occipital cortices; the fractional amplitude of low-frequency fluctuation decreased in parts of the prefrontal cortex and increased in parts of the parietal cortex. All of the results showed that fMRI-based analysis of brain functional activities could be effectively used to distinguish the mental health and sub-health status.
Humans are highly adept at multisensory processing of object shape in both vision and touch. Previous studies have mostly focused on where visually perceived object-shape information can be decoded, with haptic shape processing receiving less attention. Here, we investigate visuo-haptic shape processing in the human brain using multivoxel correlation analyses. Importantly, we use tangible, parametrically defined novel objects as stimuli. Two groups of participants first performed either a visual or haptic similarity-judgment task. The resulting perceptual object-shape spaces were highly similar and matched the physical parameter space. In a subsequent fMRI experiment, objects were first compared within the learned modality and then in the other modality in a one-back task. When correlating neural similarity spaces with perceptual spaces, visually perceived shape was decoded well in the occipital lobe along with the ventral pathway, whereas haptically perceived shape information was mainly found in the parietal lobe, including frontal cortex. Interestingly, ventrolateral occipito-temporal cortex decoded shape in both modalities, highlighting this as an area capable of detailed visuo-haptic shape processing. Finally, we found haptic shape representations in early visual cortex (in the absence of visual input), when participants switched from visual to haptic exploration, suggesting top-down involvement of visual imagery on haptic shape processing.
Embodiment theory predicts that mental imagery of object words recruits neural circuits involved in object perception. The degree of visual imagery present in routine thought and how it is encoded in the brain is largely unknown. We test whether fMRI activity patterns elicited by participants reading objects' names include embodied visual-object representations, and whether we can decode the representations using novel computational image-based semantic models. We first apply the image models in conjunction with text-based semantic models to test predictions of visual-specificity of semantic representations in different brain regions. Representational similarity analysis confirms that fMRI structure within ventral-temporal and lateral-occipital regions correlates most strongly with the image models and conversely text models correlate better with posterior-parietal/lateral-temporal/inferior-frontal regions. We use an unsupervised decoding algorithm that exploits commonalities in representational similarity structure found within both image model and brain data sets to classify embodied visual representations with high accuracy (8/10) and then extend it to exploit model combinations to robustly decode different brain regions in parallel. By capturing latent visual-semantic structure our models provide a route into analyzing neural representations derived from past perceptual experience rather than stimulus-driven brain activity. Our results also verify the benefit of combining multimodal data to model human-like semantic representations.
Electrophysiological and neuroimaging evidence suggest the existence of common mechanisms for monitoring erroneous events, independent of the source of errors. Previous works have described modulations of theta activity in the medial frontal cortex elicited by either self-generated errors or erroneous feedback. In turn, similar patterns have recently been reported to appear after the observation of external errors. We report cross-regional interactions after observation of errors at both average and single-trial levels. We recorded scalp electroencephalography (EEG) signals from 15 subjects while monitoring the movement of a cursor on a computer screen. Connectivity patterns, estimated using multivariate auto-regressive models, show increased error-related modulations of the information transfer in the theta and alpha bands between frontocentral and frontolateral areas. Conversely, a decrease of connectivity in the beta band is also observed. These network patterns are similar to those elicited by self-generated errors. However, since no motor response is required, they appear to be related to intrinsic mechanisms of error processing, instead of being linked to co-activation of motor areas. Noticeably, we demonstrate that cross-regional interaction patterns can be estimated on a trial-by-trial basis. These trial-specific patterns, consistent with the multi-trial analysis, convey discriminant information on whether a trial was elicited by observation of an erroneous action. Overall, our study supports the role of frequency-specific modulations in the medial frontal cortex in coordinating cross-regional activity during cognitive monitoring at a single-trial basis.
The voice is a rich source of information, which the human brain has evolved to decode and interpret. Empirical observations have shown that the human auditory system is especially sensitive to the human voice, and that activity within the voice-sensitive regions of the primary and secondary auditory cortex is modulated by the emotional quality of the vocal signal, and may therefore subserve, with frontal regions, the cognitive ability to correctly identify the speaker's affective state. So far, the network involved in the processing of vocal affect has been mainly characterised at the cortical level. However, anatomical and functional evidence suggests that acoustic information relevant to the affective quality of the auditory signal might be processed prior to the auditory cortex. Here we review the animal and human literature on the main subcortical structures along the auditory pathway, and propose a model whereby the distinction between different types of vocal affect in auditory communication begins at very early stages of auditory processing, and relies on the analysis of individual acoustic features of the sound signal. We further suggest that this early feature-based decoding occurs at a subcortical level along the ascending auditory pathway, and provides a preliminary coarse (but fast) characterisation of the affective quality of the auditory signal before the more refined (but slower) cortical processing is completed.
How humans extract the identity of speech sounds from highly variable acoustic signals remains unclear. Here, we use searchlight representational similarity analysis (RSA) to localize and characterize neural representations of syllables at different levels of the hierarchically organized temporo-frontal pathways for speech perception. We asked participants to listen to spoken syllables that differed considerably in their surface acoustic form by changing speaker and degrading surface acoustics using noise-vocoding and sine wave synthesis while we recorded neural responses with functional magnetic resonance imaging. We found evidence for a graded hierarchy of abstraction across the brain. At the peak of the hierarchy, neural representations in somatomotor cortex encoded syllable identity but not surface acoustic form, at the base of the hierarchy, primary auditory cortex showed the reverse. In contrast, bilateral temporal cortex exhibited an intermediate response, encoding both syllable identity and the surface acoustic form of speech. Regions of somatomotor cortex associated with encoding syllable identity in perception were also engaged when producing the same syllables in a separate session. These findings are consistent with a hierarchical account of how variable acoustic signals are transformed into abstract representations of the identity of speech sounds.
How the human brain represents distinct motor features into a unique finalized action still remains undefined. Previous models proposed the distinct features of a motor act to be hierarchically organized in separated, but functionally interconnected, cortical areas. Here, we hypothesized that distinct patterns across a wide expanse of cortex may actually subserve a topographically organized coding of different categories of actions that represents, at a higher cognitive level and independently from the distinct motor features, the action and its final aim as a whole. Using functional magnetic resonance imaging and pattern classification approaches on the neural responses of 14 right-handed individuals passively watching short movies of hand-performed tool-mediated, transitive, and meaningful intransitive actions, we were able to discriminate with a high accuracy and characterize the category-specific response patterns. Actions are distinctively coded in distributed and overlapping neural responses within an action-selective network, comprising frontal, parietal, lateral occipital and ventrotemporal regions. This functional organization, that we named action topography, subserves a higher-level and more abstract representation of finalized actions and has the capacity to provide unique representations for multiple categories of actions.
Encoding and retrieval processes enhance long-term memory performance. The efficiency of encoding processes has recently been linked to representational consistency: the reactivation of a representation that gets more specific each time an item is further studied. Here we examined the complementary hypothesis of whether the efficiency of retrieval processes also is linked to representational consistency. Alternatively, recurrent retrieval might foster representational variability--the altering or adding of underlying memory representations. Human participants studied 60 Swahili-Swedish word pairs before being scanned with fMRI the same day and 1 week later. On Day 1, participants were tested three times on each word pair, and on Day 7 each pair was tested once. A BOLD signal change in right superior parietal cortex was associated with subsequent memory on Day 1 and with successful long-term retention on Day 7. A representational similarity analysis in this parietal region revealed that beneficial recurrent retrieval was associated with representational variability, such that the pattern similarity on Day 1 was lower for retrieved words subsequently remembered compared with those subsequently forgotten. This was mirrored by a monotonically decreased BOLD signal change in dorsolateral prefrontal cortex on Day 1 as a function of repeated successful retrieval for words subsequently remembered, but not for words subsequently forgotten. This reduction in prefrontal response could reflect reduced demands on cognitive control. Collectively, the results offer novel insights into why memory retention benefits from repeated retrieval, and they suggest fundamental differences between repeated study and repeated testing.
Functional connectivity (FC) patterns in functional MRI exhibit dynamic behavior on the scale of seconds, with rich spatiotemporal structure and limited sets of whole-brain, quasi-stable FC configurations (FC states) recurring across time and subjects. Based on previous evidence linking various aspects of cognition to group-level, minute-to-minute FC changes in localized connections, we hypothesized that whole-brain FC states may reflect the global, orchestrated dynamics of cognitive processing on the scale of seconds. To test this hypothesis, subjects were continuously scanned as they engaged in and transitioned between mental states dictated by tasks. FC states computed within windows as short as 22.5 s permitted robust tracking of cognition in single subjects with near perfect accuracy. Accuracy dropped markedly for subjects with the lowest task performance. Spatially restricting FC information decreased accuracy at short time scales, emphasizing the distributed nature of whole-brain FC dynamics, beyond univariate magnitude changes, as valuable markers of cognition.
The role of attention in creative cognition remains controversial. Neuroimaging studies have reported activation of brain regions linked to both cognitive control and spontaneous imaginative processes, raising questions about how these regions interact to support creative thought. Using functional magnetic resonance imaging (fMRI), we explored this question by examining dynamic interactions between brain regions during a divergent thinking task. Multivariate pattern analysis revealed a distributed network associated with divergent thinking, including several core hubs of the default (posterior cingulate) and executive (dorsolateral prefrontal cortex) networks. The resting-state network affiliation of these regions was confirmed using data from an independent sample of participants. Graph theory analysis assessed global efficiency of the divergent thinking network, and network efficiency was found to increase as a function of individual differences in divergent thinking ability. Moreover, temporal connectivity analysis revealed increased coupling between default and salience network regions (bilateral insula) at the beginning of the task, followed by increased coupling between default and executive network regions at later stages. Such dynamic coupling suggests that divergent thinking involves cooperation between brain networks linked to cognitive control and spontaneous thought, which may reflect focused internal attention and the top-down control of spontaneous cognition during creative idea production.
Cognitive and neuronal effects of nicotine show high interindividual variability. Recent findings indicate that genetic variations that affect the cholinergic and dopaminergic neurotransmitter system impact performance in cognitive tasks and effects of nicotine. The current pharmacogenetic functional magnetic resonance imaging (fMRI) study aimed to investigate epistasis effects of CHRNA4/DRD2 variations on behavioural and neural correlates of visuospatial attention after nicotine challenge using a data driven partial least squares discriminant analysis (PLS-DA) approach. Fifty young healthy non-smokers were genotyped for CHRNA4 (rs1044396) and DRD2 (rs6277). They received either 7 mg transdermal nicotine or a matched placebo in a double blind within subject design prior to performing a cued target detection task with valid and invalid trials. On behavioural level, the strongest benefits of nicotine in invalid trials were observed in participants carrying both, the DRD2 T- and CHRNA4 C+ variant. Neurally, we were able to demonstrate that different DRD2/CHRNA4 groups can be decoded from the pattern of brain activity in invalid trials under nicotine. Neural substrates of interindividual variability were found in a network of attention-related brain regions comprising the pulvinar, the striatum, the middle and superior frontal gyri, the insula, the left precuneus, and the right middle temporal gyrus. Our findings suggest that polymorphisms in the CHRNA4 and DRD2 genes are a relevant source of individual variability in pharmacological studies with nicotine.
Perceptual sensitivity to tactile roughness varies across individuals for the same degree of roughness. A number of neurophysiological studies have investigated the neural substrates of tactile roughness perception, but the neural processing underlying the strong individual differences in perceptual roughness sensitivity remains unknown. In this study, we explored the human brain activation patterns associated with the behavioral discriminability of surface texture roughness using functional magnetic resonance imaging (fMRI). First, a whole-brain searchlight multi-voxel pattern analysis (MVPA) was used to find brain regions from which we could decode roughness information. The searchlight MVPA revealed four brain regions showing significant decoding results: the supplementary motor area (SMA), contralateral postcentral gyrus (S1), and superior portion of the bilateral temporal pole (STP). Next, we evaluated the behavioral roughness discrimination sensitivity of each individual using the just-noticeable difference (JND) and correlated this with the decoding accuracy in each of the four regions. We found that only the SMA showed a significant correlation between neuronal decoding accuracy and JND across individuals; Participants with a smaller JND (i.e., better discrimination ability) exhibited higher decoding accuracy from their voxel response patterns in the SMA. Our findings suggest that multivariate voxel response patterns presented in the SMA represent individual perceptual sensitivity to tactile roughness and people with greater perceptual sensitivity to tactile roughness are likely to have more distinct neural representations of different roughness levels in their SMA.
Effective generalization in a multiple-category situation involves both assessing potential membership in individual categories and resolving conflict between categories while implementing a decision bound. We separated generalization from decision bound implementation using an information integration task in which category exemplars varied over two incommensurable feature dimensions. Human subjects first learned to categorize stimuli within limited training regions, and then, during fMRI scanning, they also categorized transfer stimuli from new regions of perceptual space. Transfer stimuli differed both in distance from the training region prototype and distance from the decision bound, allowing us to independently assess neural systems sensitive to each. Across all stimulus regions, categorization was associated with activity in the extrastriate visual cortex, basal ganglia, and the bilateral intraparietal sulcus. Categorizing stimuli near the decision bound was associated with recruitment of the frontoinsular cortex and medial frontal cortex, regions often associated with conflict and which commonly coactivate within the salience network. Generalization was measured in terms of greater distance from the decision bound and greater distance from the category prototype (average training region stimulus). Distance from the decision bound was associated with activity in the superior parietal lobe, lingual gyri, and anterior hippocampus, whereas distance from the prototype was associated with left intraparietal sulcus activity. The results are interpreted as supporting the existence of different uncertainty resolution mechanisms for uncertainty about category membership (representational uncertainty) and uncertainty about decision bound (decisional uncertainty).
We present a literature review of functional brain imaging studies of haptic and visual texture perception, and highlight our ongoing functional magnetic resonance imaging (fMRI) experiment of the crossmodal links between vision and touch. In the fMRI experiment, the subjects viewed or touched a piece of wool or denim cloth. A multivoxel pattern analysis of whole-brain fMRI activity revealed the crossmodal nature of natural texture perception. Visual texture representations were found in the somatosensory and association cortices as well as the visual cortex, while haptic texture representations were found in the visual cortex and association cortices as well as the somatosensory cortex. Furthermore, shared visuo-haptic representations were found in the parietal association, somatosensory, and visual cortices. These results that suggested the crossmodal transfer of texture information across functionally segregated sensory and associative brain regions are discussed in relation to previous findings on texture perception and aesthetic texture or shitsukan.
How do our brains achieve the cognitive control that is required for flexible behavior? Several models of cognitive control propose a role for frontoparietal cortex in the structure and representation of task sets or rules. For behavior to be flexible, however, the system must also rapidly reorganize as mental focus changes. Here we used multivoxel pattern analysis of fMRI data to demonstrate adaptive reorganization of frontoparietal activity patterns following a change in the complexity of the task rules. When task rules were relatively simple, frontoparietal cortex did not hold detectable information about these rules. In contrast, when the rules were more complex, frontoparietal cortex showed clear and decodable rule discrimination. Our data demonstrate that frontoparietal activity adjusts to task complexity, with better discrimination of rules that are behaviorally more confusable. The change in coding was specific to the rule element of the task and was not mirrored in more specialized cortex (early visual cortex) where coding was independent of difficulty. In line with an adaptive view of frontoparietal function, the data suggest a system that rapidly reconfigures in accordance with the difficulty of a behavioral task. This system may provide a neural basis for the flexible control of human behavior.
Amazingly, human observers can track four independently moving targets. The present study investigated the neural correlates of multiple-object tracking (MOT). Based on previous work we used a modified MOT-task to which subjects exhibited different behaviors. One half of the subjects showed slower RTs and higher error rates with increasing correspondence between tracked items and a probe consisting of 4 highlighted items presented after the tracking. The other half of the subjects had better performance when the probe fully matched the tracked items. Here we sought to investigate the neural representation of the two divergent behavior types. Using multivariate pattern analysis we observed two partly overlapping functional networks associated with the different behaviors. Subjects that responded fast and accurate to full-congruity trials predominantly showed a functional pattern for the full-congruity condition that was very different from patterns associated with any of the partly congruent conditions. This "deviant" pattern was observed in frontal, parietal and extrastriate visual brain areas. In the group of subjects with decreasing performance for increasing target-probe congruity these same regions exhibited a very different functional relationship, in which increasing congruities were associated with linearly changing neural activity patterns. Early low-tier visual areas exclusively exhibited the linear classification pattern while area LO and the primary motor cortex exclusively showed the deviant pattern across all subjects. The coexistence of both networks in groups with different behaviors provides the neural basis for a flexible behavior that can be flexibly adjusted as a function of the strategy employed in the task.
The neural basis of speech comprehension has been investigated intensively during the past few decades. Incoming auditory signals are analysed for speech-like patterns and meaningful information can be extracted by mapping these sounds onto stored semantic representations. Investigation into the neural basis of speech comprehension has largely focused on the temporal lobe, in particular the superior and posterior regions. The ventral anterior temporal lobe (vATL), which includes the inferior temporal gyrus (ITG) and temporal fusiform gyrus (TFG) is consistently omitted in fMRI studies. In contrast, PET studies have shown the involvement of these ventral temporal regions. One crucial factor is the signal loss experienced using conventional echo planar imaging (EPI) for fMRI, at tissue interfaces such as the vATL. One method to overcome this signal loss is to employ a dual-echo EPI technique. The aim of this study was to use intelligible and unintelligible (spectrally rotated) sentences to determine if the vATL could be detected during a passive speech comprehension task using a dual-echo acquisition. A whole brain analysis for an intelligibility contrast showed bilateral superior temporal lobe activations and a cluster of activation within the left vATL. Converging evidence implicates the same ventral temporal regions during semantic processing tasks, which include language processing. The specific role of the ventral temporal region during intelligible speech processing cannot be determined from this data alone, but the converging evidence from PET, MEG, TMS and neuropsychology strongly suggest that it contains the stored semantic representations, which are activated by the speech decoding process.
In the last decade, a number of neuroimaging studies have investigated the neurophysiological effects associated with contemplative practices. Meditation-related changes in resting state functional connectivity (rsFC) have been previously reported, particularly in the default mode network, frontoparietal attentional circuits, saliency-related regions, and primary sensory cortices. We collected functional magnetic resonance imaging data from a sample of 12 experienced Zen meditators and 12 meditation-naive matched controls during a basic attention-to-breathing protocol, together with behavioral performance outside the scanner on a set of computerized neuropsychological tests. We adopted a network system of 209 nodes, classified into nine functional modules, and a multi-stage approach to identify rsFC differences in meditators and controls. Between-group comparisons of modulewise FC, summarized by the first principal component of the relevant set of edges, revealed important connections of frontoparietal circuits with early visual and executive control areas. We also identified several group differences in positive and negative edgewise FC, often involving the visual, or frontoparietal regions. Multivariate pattern analysis of modulewise FC, using support vector machine (SVM), classified meditators, and controls with 79% accuracy and selected 10 modulewise connections that were jointly prominent in distinguishing meditators and controls; a similar SVM procedure based on the subjects' scores on the neuropsychological battery yielded a slightly weaker accuracy (75%). Finally, we observed a good correlation between the across-subject variation in strength of modulewise connections among frontoparietal, executive, and visual circuits, on the one hand, and in the performance on a rapid visual information processing test of sustained attention, on the other. Taken together, these findings highlight the usefulness of employing network analysis techniques in investigating the neural correlates of contemplative practices.
The analysis of brain imaging data often requires simplifying assumptions because exhaustive analyses are computationally intractable. Standard univariate and multivariate analyses of brain activity ignore interactions between regions and analyses of interactions (functional connectivity) reduce the computational challenge by using seed regions of interest or brain parcellations.
The neural correlates of consciousness are typically sought by comparing the overall brain responses to perceived and unperceived stimuli. However, this comparison may be contaminated by non-specific attention, alerting, performance, and reporting confounds. Here, we pursue a novel approach, tracking the neuronal coding of consciously and unconsciously perceived contents while keeping behavior identical (blindsight). EEG and MEG were recorded while participants reported the spatial location and visibility of a briefly presented target. Multivariate pattern analysis demonstrated that considerable information about spatial location traverses the cortex on blindsight trials, but that starting 270 ms post-onset, information unique to consciously perceived stimuli, emerges in superior parietal and superior frontal regions. Conscious access appears characterized by the entry of the perceived stimulus into a series of additional brain processes, each restricted in time, while the failure of conscious access results in the breaking of this chain and a subsequent slow decay of the lingering unconscious activity.
Refreshing is the component cognitive process of directing reflective attention to one of several active mental representations. Previous studies using fMRI suggested that refresh tasks involve a component process of initiating refreshing as well as the top-down modulation of representational regions central to refreshing. However, those studies were limited by fMRI's low temporal resolution. In this study, we used EEG to examine the time course of refreshing on the scale of milliseconds rather than seconds. ERP analyses showed that a typical refresh task does have a distinct electrophysiological response as compared to a control condition and includes at least two main temporal components: an earlier (400 msec) positive peak reminiscent of a P3 response and a later (800-1400 msec) sustained positivity over several sites reminiscent of the late directing attention positivity. Overall, the evoked potentials for refreshing representations from three different visual categories (faces, scenes, words) were similar, but multivariate pattern analysis showed that some category information was nonetheless present in the EEG signal. When related to previous fMRI studies, these results are consistent with a two-phase model, with the first phase dominated by frontal control signals involved in initiating refreshing and the second by the top-down modulation of posterior perceptual cortical areas that constitutes refreshing a representation. This study also lays the foundation for future studies of the neural correlates of reflective attention at a finer temporal resolution than is possible using fMRI.
Near-infrared spectroscopy (NIRS) brain-computer interface (BCI) studies have primarily made use of measurements taken from a single cortical area. In particular, the anterior prefrontal cortex has been the key area used for detecting higher-level cognitive task performance. However, mental task execution typically requires coordination between several, spatially-distributed brain regions. We investigated the value of expanding the area of interrogation to include NIRS measurements from both the prefrontal and parietal cortices to decode mental states. Hemodynamic activity was monitored at 46 locations over the prefrontal and parietal cortices using a continuous-wave near-infrared spectrometer while 11 able-bodied adults rested or performed either the verbal fluency task (VFT) or Stroop task. Offline classification was performed for the three possible binary problems using 25 iterations of bagging with a linear discriminant base classifier. Classifiers were trained on a 10 dimensional feature set. When all 46 measurement locations were considered for classification, average accuracies of 80.47.0%, 82.47.6%, and 82.85.9% in differentiating VFT vs rest, Stroop vs rest and VFT vs Stroop, respectively, were obtained. Relative to using measurements from the anterior PFC alone, an overall average improvement of 11.3% was achieved. Utilizing NIRS measurements from the prefrontal and parietal cortices can be of value in classifying mental states involving working memory and attention. NIRS-BCI accuracies may be improved by incorporating measurements from several, distinct cortical regions, rather than a single area alone. Further development of an NIRS-BCI supporting combinations of VFT, Stroop task and rest states is also warranted.
We aimed to uncover differences in brain circuits of adolescents with parental positive or negative histories of substance use disorders (SUD), when performing a task that elicits emotional conflict, testing whether the brain circuits could serve as endophenotype markers to distinguish these adolescents. We acquired functional magnetic resonance imaging data from 11 adolescents with a positive familial history of SUD (FH+ group) and seven adolescents with a negative familial history of SUD (FH- group) when performing an emotional stroop task. We extracted brain features from the conflict-related contrast images in group level analyses and granger causality indices (GCIs) that measure the causal interactions among regions. Support vector machine (SVM) was applied to classify the FH+ and FH- adolescents. Adolescents with FH+ showed greater activity and weaker connectivity related to emotional conflict, decision making and reward system including anterior cingulate cortex (ACC), prefrontal cortex (PFC), and ventral tegmental area (VTA). High classification accuracies were achieved with leave-one-out cross validation (89.75% for the maximum conflict, 96.71% when combining maximum conflict and general conflict contrast, 97.28% when combining activity of the two contrasts and GCIs). Individual contributions of the brain features to the classification were further investigated, indicating that activation in PFC, ACC, VTA and effective connectivity from PFC to ACC play the most important roles. We concluded that fundamental differences of neural substrates underlying cognitive behaviors of adolescents with parental positive or negative histories of SUD provide new insight into potential neurobiological mechanisms contributing to the elevated risk of FH+ individuals for developing SUD.
Performing multiple tasks concurrently places a load on limited attentional resources and results in disrupted task performance. Although human neuroimaging studies have investigated the neural correlates of attentional load, how attentional load affects task processing is poorly understood. Here, task-related neural activity was investigated using fMRI with conventional univariate analysis and multivariate pattern analysis (MVPA) while participants performed blocks of prosaccades and antisaccades, either with or without a rapid serial visual presentation (RSVP) task. Performing prosaccades and antisaccades with RSVP increased error rates and RTs, decreased mean activation in frontoparietal brain areas associated with oculomotor control, and eliminated differences in activation between prosaccades and antisaccades. However, task identity could be decoded from spatial patterns of activation both in the absence and presence of an attentional load. Furthermore, in the FEFs and intraparietal sulcus, these spatial representations were found to be similar using cross-trial-type MVPA, which suggests stability under attentional load. These results demonstrate that attentional load may disrupt the strength of task-related neural activity, rather than the identity of task representations.
Categorical models of emotions posit neurally and physiologically distinct human basic emotions. We tested this assumption by using multivariate pattern analysis (MVPA) to classify brain activity patterns of 6 basic emotions (disgust, fear, happiness, sadness, anger, and surprise) in 3 experiments. Emotions were induced with short movies or mental imagery during functional magnetic resonance imaging. MVPA accurately classified emotions induced by both methods, and the classification generalized from one induction condition to another and across individuals. Brain regions contributing most to the classification accuracy included medial and inferior lateral prefrontal cortices, frontal pole, precentral and postcentral gyri, precuneus, and posterior cingulate cortex. Thus, specific neural signatures across these regions hold representations of different emotional states in multimodal fashion, independently of how the emotions are induced. Similarity of subjective experiences between emotions was associated with similarity of neural patterns for the same emotions, suggesting a direct link between activity in these brain regions and the subjective emotional experience.
Interest in the lateralization of the human brain is evident through a multidisciplinary number of scientific studies. Understanding volumetric brain asymmetries allows the distinction between normal development stages and behavior, as well as brain diseases. We aimed to evaluate volumetric asymmetries in order to select the best gyri able to classify right- versus left cerebral hemispheres. A cross-sectional study performed in 47 right-handed young-adults healthy volunteers. SPM-based software performed brain segmentation, automatic labeling and volumetric analyses for 54 regions involving the cerebral lobes, basal ganglia and cerebellum from each cerebral hemisphere. Multivariate discriminant analysis (DA) allowed the assembling of a predictive model. DA revealed one discriminant function that significantly differentiated left vs. right cerebral hemispheres: Wilks'  = 0.008, (2) (9) = 238.837, P < 0.001. The model explained 99.20% of the variation in the grouping variable and depicted an overall predictive accuracy of 98.8%. With the influence of gender; the selected gyri able to discriminate between hemispheres were middle orbital frontal gyrus (g.), angular g., supramarginal g., middle cingulum g., inferior orbital frontal g., calcarine g., inferior parietal lobule and the pars triangularis inferior frontal g. Specific brain gyri are able to accurately classify left vs. right cerebral hemispheres by using a multivariate approach; the selected regions correspond to key brain areas involved in attention, internal thought, vision and language; our findings favored the concept that lateralization has been evolutionary favored by mental processes increasing cognitive efficiency and brain capacity.
Human lateral prefrontal cortex (LPFC) is thought to play a critical role in enabling cognitive flexibility, particularly when performing novel tasks. However, it remains to be established whether LPFC representation of task-relevant information in such situations actually contributes to successful performance. We utilized pattern classification analyses of functional MRI activity to identify novelty-sensitive brain regions as participants rapidly switched between performance of 64 complex tasks, 60 of which were novel. In three of these novelty-sensitive regions-located within distinct areas of left anterior LPFC-trial-evoked activity patterns discriminated correct from error trials. Further, these regions also contained information regarding the task-relevant decision rule, but only for successfully performed trials. This suggests that left anterior LPFC may be particularly important for representing task information that contributes to the cognitive flexibility needed to perform successfully in novel task situations.
Emotions can be aroused by various kinds of stimulus modalities. Recent neuroimaging studies indicate that several brain regions represent emotions at an abstract level, i.e., independently from the sensory cues from which they are perceived (e.g., face, body, or voice stimuli). If emotions are indeed represented at such an abstract level, then these abstract representations should also be activated by the memory of an emotional event. We tested this hypothesis by asking human participants to learn associations between emotional stimuli (videos of faces or bodies) and non-emotional stimuli (fractals). After successful learning, fMRI signals were recorded during the presentations of emotional stimuli and emotion-associated fractals. We tested whether emotions could be decoded from fMRI signals evoked by the fractal stimuli using a classifier trained on the responses to the emotional stimuli (and vice versa). This was implemented as a whole-brain searchlight, multivoxel activation pattern analysis, which revealed successful emotion decoding in four brain regions: posterior cingulate cortex (PCC), precuneus, MPFC, and angular gyrus. The same analysis run only on responses to emotional stimuli revealed clusters in PCC, precuneus, and MPFC. Multidimensional scaling analysis of the activation patterns revealed clear clustering of responses by emotion across stimulus types. Our results suggest that PCC, precuneus, and MPFC contain representations of emotions that can be evoked by stimuli that carry emotional information themselves or by stimuli that evoke memories of emotional stimuli, while angular gyrus is more likely to take part in emotional memory retrieval.
To date, research into the biomarker-aided early recognition of psychosis has focused on predicting the transition likelihood of clinically defined individuals with different at-risk mental states (ARMS) based on structural (and functional) brain changes. However, it is currently unknown whether neuroimaging patterns could be identified to facilitate the individualized prediction of symptomatic and functional recovery. Therefore, we investigated whether cortical surface alterations analyzed by means of multivariate pattern recognition methods could enable the single-subject identification of functional outcomes in twenty-seven ARMS individuals. Subjects were dichotomized into 'good' vs. 'poor' outcome groups on average 4years after the baseline MRI scan using a Global Assessment of Functioning (GAF) threshold of 70. Cortical surface-based pattern classification predicted good (N=14) vs. poor outcome status (N=13) at follow-up with an accuracy of 82% as determined by nested leave-one-cross-validation. Neuroanatomical prediction involved cortical area reductions in superior temporal, inferior frontal and inferior parietal areas and was not confounded by functional impairment at baseline, or antipsychotic medication and transition status over the follow-up period. The prediction model's decision scores were correlated with positive and general symptom scores in the ARMS group at follow-up, whereas negative symptoms were not linked to predicted poorer functional outcome. These findings suggest that poorer functional outcomes are associated with non-resolving attenuated psychosis and could be predicted at the single-subject level using multivariate neuroanatomical risk stratification methods. However, the generalizability and specificity of the suggested prediction model should be thoroughly investigated in future large-scale and cross-diagnostic MRI studies.
Recent research revealed that the presentation of crime related details during the Concealed Information Test (CIT) reliably activates a network of bilateral inferior frontal, right medial frontal and right temporal-parietal brain regions. However, the ecological validity of these findings as well as the influence of the encoding context are still unclear. To tackle these questions, three different groups of subjects participated in the current study. Two groups of guilty subjects encoded critical details either only by planning (guilty intention group) or by really enacting (guilty action group) a complex, realistic mock crime. In addition, a group of informed innocent subjects encoded half of the relevant details in a neutral context. Univariate analyses showed robust activation differences between known relevant compared to neutral details in the previously identified ventral frontal-parietal network with no differences between experimental groups. Moreover, validity estimates for average changes in neural activity were similar between groups when focusing on the known details and did not differ substantially from the validity of electrodermal recordings. Additional multivariate analyses provided evidence for differential patterns of activity in the ventral fronto-parietal network between the guilty action and the informed innocent group and yielded higher validity coefficients for the detection of crime related knowledge when relying on whole brain data. Together, these findings demonstrate that an fMRI-based CIT enables the accurate detection of concealed crime related memories, largely independent of encoding context. On the one hand, this indicates that even persons who planned a (mock) crime could be validly identified as having specific crime related knowledge. On the other hand, innocents with such knowledge have a high risk of failing the test, at least when considering univariate changes of neural activation.
The prefrontal cortex houses representations critical for ongoing and future behavior expressed in the form of patterns of neural activity. Dopamine has long been suggested to play a key role in the integrity of such representations, with D2-receptor activation rendering them flexible but weak. However, it is currently unknown whether and how D2-receptor activation affects prefrontal representations in humans. In the current study, we use dopamine receptor-specific pharmacology and multivoxel pattern-based functional magnetic resonance imaging to test the hypothesis that blocking D2-receptor activation enhances prefrontal representations. Human subjects performed a simple reward prediction task after double-blind and placebo controlled administration of the D2-receptor antagonist amisulpride. Using a whole-brain searchlight decoding approach we show that D2-receptor blockade enhances decoding of reward signals in the medial orbitofrontal cortex. Examination of activity patterns suggests that amisulpride increases the separation of activity patterns related to reward versus no reward. Moreover, consistent with the cortical distribution of D2 receptors, post hoc analyses showed enhanced decoding of motor signals in motor cortex, but not of visual signals in visual cortex. These results suggest that D2-receptor blockade enhances content-specific representations in frontal cortex, presumably by a dopamine-mediated increase in pattern separation. These findings are in line with a dual-state model of prefrontal dopamine, and provide new insights into the potential mechanism of action of dopaminergic drugs.
While there is accumulating evidence for the existence of distinct neural systems supporting goal-directed and habitual action selection in the mammalian brain, much less is known about the nature of the information being processed in these different brain regions. Associative learning theory predicts that brain systems involved in habitual control, such as the dorsolateral striatum, should contain stimulus and response information only, but not outcome information, while regions involved in goal-directed action, such as ventromedial and dorsolateral prefrontal cortex and dorsomedial striatum, should be involved in processing information about outcomes as well as stimuli and responses. To test this prediction, human participants underwent fMRI while engaging in a binary choice task designed to enable the separate identification of these different representations with a multivariate classification analysis approach. Consistent with our predictions, the dorsolateral striatum contained information about responses but not outcomes at the time of an initial stimulus, while the regions implicated in goal-directed action selection contained information about both responses and outcomes. These findings suggest that differential contributions of these regions to habitual and goal-directed behavioral control may depend in part on basic differences in the type of information that these regions have access to at the time of decision making.
A relatively underexplored question in fMRI is whether there are intrinsic differences in terms of signal composition patterns that can effectively characterize and differentiate task-based or resting state fMRI (tfMRI or rsfMRI) signals. In this paper, we propose a novel two-stage sparse representation framework to examine the fundamental difference between tfMRI and rsfMRI signals. Specifically, in the first stage, the whole-brain tfMRI or rsfMRI signals of each subject were composed into a big data matrix, which was then factorized into a subject-specific dictionary matrix and a weight coefficient matrix for sparse representation. In the second stage, all of the dictionary matrices from both tfMRI/rsfMRI data across multiple subjects were composed into another big data-matrix, which was further sparsely represented by a cross-subjects common dictionary and a weight matrix. This framework has been applied on the recently publicly released Human Connectome Project (HCP) fMRI data and experimental results revealed that there are distinctive and descriptive atoms in the cross-subjects common dictionary that can effectively characterize and differentiate tfMRI and rsfMRI signals, achieving 100% classification accuracy. Moreover, our methods and results can be meaningfully interpreted, e.g., the well-known default mode network (DMN) activities can be recovered from the very noisy and heterogeneous aggregated big-data of tfMRI and rsfMRI signals across all subjects in HCP Q1 release.
A fundamental challenge in studying the frontal lobe is to parcellate this cortex into "natural" functional modules despite the absence of topographic maps, which are so helpful in primary sensory areas. Here we show that unsupervised clustering algorithms, applied to 96-channel array recordings from prearcuate gyrus, reveal spatially segregated subnetworks that remain stable across behavioral contexts. Looking for natural groupings of neurons based on response similarities, we discovered that the recorded area includes at least two spatially segregated subnetworks that differentially represent behavioral choice and reaction time. Importantly, these subnetworks are detectable during different behavioral states and, surprisingly, are defined better by "common noise" than task-evoked responses. Our parcellation process works well on "spontaneous" neural activity, and thus bears strong resemblance to the identification of "resting-state" networks in fMRI data sets. Our results demonstrate a powerful new tool for identifying cortical subnetworks by objective classification of simultaneously recorded electrophysiological activity.
The sexual differentiation of the brain is primarily driven by gonadal hormones during fetal development. Leading theories on the etiology of gender dysphoria (GD) involve deviations herein. To examine whether there are signs of a sex-atypical brain development in GD, we quantified regional neural gray matter (GM) volumes in 55 female-to-male and 38 male-to-female adolescents, 44 boys and 52 girls without GD and applied both univariate and multivariate analyses. In girls, more GM volume was observed in the left superior medial frontal cortex, while boys had more volume in the bilateral superior posterior hemispheres of the cerebellum and the hypothalamus. Regarding the GD groups, at whole-brain level they differed only from individuals sharing their gender identity but not from their natal sex. Accordingly, using multivariate pattern recognition analyses, the GD groups could more accurately be automatically discriminated from individuals sharing their gender identity than those sharing their natal sex based on spatially distributed GM patterns. However, region of interest analyses indicated less GM volume in the right cerebellum and more volume in the medial frontal cortex in female-to-males in comparison to girls without GD, while male-to-females had less volume in the bilateral cerebellum and hypothalamus than natal boys. Deviations from the natal sex within sexually dimorphic structures were also observed in the untreated subsamples. Our findings thus indicate that GM distribution and regional volumes in GD adolescents are largely in accordance with their respective natal sex. However, there are subtle deviations from the natal sex in sexually dimorphic structures, which can represent signs of a partial sex-atypical differentiation of the brain.
What are the neural mechanisms of face recognition? It is believed that the network of face-selective areas, which spans the occipital, temporal, and frontal cortices, is important in face recognition. A number of previous studies indeed reported that face identity could be discriminated based on patterns of multivoxel activity in the fusiform face area and the anterior temporal lobe. However, given the difficulty in localizing the face-selective area in the anterior temporal lobe, its role in face recognition is still unknown. Furthermore, previous studies limited their analysis to occipito-temporal regions without testing identity decoding in more anterior face-selective regions, such as the amygdala and prefrontal cortex. In the current high-resolution functional Magnetic Resonance Imaging study, we systematically examined the decoding of the identity of famous faces in the temporo-frontal network of face-selective and adjacent non-face-selective regions. A special focus has been put on the face-area in the anterior temporal lobe, which was reliably localized using an optimized scanning protocol. We found that face-identity could be discriminated above chance level only in the fusiform face area. Our results corroborate the role of the fusiform face area in face recognition. Future studies are needed to further explore the role of the more recently discovered anterior face-selective areas in face recognition.
Incremental instruction on the workings of a set of mechanical systems induced a progression of changes in the neural representations of the systems. The neural representations of four mechanical systems were assessed before, during, and after three phases of incremental instruction (which first provided information about the system components, then provided partial causal information, and finally provided full functional information). In 14 participants, the neural representations of four systems (a bathroom scale, a fire extinguisher, an automobile braking system, and a trumpet) were assessed using three recently developed techniques: (1) machine learning and classification of multi-voxel patterns; (2) localization of consistently responding voxels; and (3) representational similarity analysis (RSA). The neural representations of the systems progressed through four stages, or states, involving spatially and temporally distinct multi-voxel patterns: (1) initially, the representation was primarily visual (occipital cortex); (2) it subsequently included a large parietal component; (3) it eventually became cortically diverse (frontal, parietal, temporal, and medial frontal regions); and (4) at the end, it demonstrated a strong frontal cortex weighting (frontal and motor regions). At each stage of knowledge, it was possible for a classifier to identify which one of four mechanical systems a participant was thinking about, based on their brain activation patterns. The progression of representational states was suggestive of progressive stages of learning: (1) encoding information from the display; (2) mental animation, possibly involving imagining the components moving; (3) generating causal hypotheses associated with mental animation; and finally (4) determining how a person (probably oneself) would interact with the system. This interpretation yields an initial, cortically-grounded, theory of learning of physical systems that potentially can be related to cognitive learning theories by suggesting links between cortical representations, stages of learning, and the understanding of simple systems.
Lapses of attention can have negative consequences, including accidents and lost productivity. Here we used closed-loop neurofeedback to improve sustained attention abilities and reduce the frequency of lapses. During a sustained attention task, the focus of attention was monitored in real time with multivariate pattern analysis of whole-brain neuroimaging data. When indicators of an attentional lapse were detected in the brain, we gave human participants feedback by making the task more difficult. Behavioral performance improved after one training session, relative to control participants who received feedback from other participants' brains. This improvement was largest when feedback carried information from a frontoparietal attention network. A neural consequence of training was that the basal ganglia and ventral temporal cortex came to represent attentional states more distinctively. These findings suggest that attentional failures do not reflect an upper limit on cognitive potential and that attention can be trained with appropriate feedback about neural signals.
Handedness is associated with differences in activation levels in various motor tasks performed with the dominant or non-dominant hand. Here we tested whether handedness is reflected in the functional architecture of the motor system even in the absence of an overt motor task. Using resting-state functional magnetic resonance imaging we investigated 18 right- and 18 left-handers. Whole-brain functional connectivity maps of the primary motor cortex (M1), supplementary motor area (SMA), dorsolateral premotor cortex (PMd), pre-SMA, inferior frontal junction and motor putamen were compared between right- and left-handers. We further used a multivariate linear support vector machine (SVM) classifier to reveal the specificity of brain regions for classifying handedness based on individual resting-state maps. Using left M1 as seed region, functional connectivity analysis revealed stronger interhemispheric functional connectivity between left M1 and right PMd in right-handers as compared to left-handers. This connectivity cluster contributed to the individual classification of right- and left-handers with 86.2% accuracy. Consistently, also seeding from right PMd yielded a similar handedness-dependent effect in left M1, albeit with lower classification accuracy (78.1%). Control analyses of the other resting-state networks including the speech and the visual network revealed no significant differences in functional connectivity related to handedness. In conclusion, our data revealed an intrinsically higher functional connectivity in right-handers. These results may help to explain that hand preference is more lateralized in right-handers than in left-handers. Furthermore, enhanced functional connectivity between left M1 and right PMd may serve as an individual marker of handedness.
The multivariate analysis of brain signals has recently sparked a great amount of interest, yet accessible and versatile tools to carry out decoding analyses are scarce. Here we introduce The Decoding Toolbox (TDT) which represents a user-friendly, powerful and flexible package for multivariate analysis of functional brain imaging data. TDT is written in Matlab and equipped with an interface to the widely used brain data analysis package SPM. The toolbox allows running fast whole-brain analyses, region-of-interest analyses and searchlight analyses, using machine learning classifiers, pattern correlation analysis, or representational similarity analysis. It offers automatic creation and visualization of diverse cross-validation schemes, feature scaling, nested parameter selection, a variety of feature selection methods, multiclass capabilities, and pattern reconstruction from classifier weights. While basic users can implement a generic analysis in one line of code, advanced users can extend the toolbox to their needs or exploit the structure to combine it with external high-performance classification toolboxes. The toolbox comes with an example data set which can be used to try out the various analysis methods. Taken together, TDT offers a promising option for researchers who want to employ multivariate analyses of brain activity patterns.
Reward motivation often enhances task performance, but the neural mechanisms underlying such cognitive enhancement remain unclear. Here, we used a multivariate pattern analysis (MVPA) approach to test the hypothesis that motivation-related enhancement of cognitive control results from improved encoding and representation of task set information. Participants underwent two fMRI sessions of cued task switching, the first under baseline conditions, and the second with randomly intermixed reward incentive and no-incentive trials. Information about the upcoming task could be successfully decoded from cue-related activation patterns in a set of frontoparietal regions typically associated with task control. More critically, MVPA classifiers trained on the baseline session had significantly higher decoding accuracy on incentive than non-incentive trials, with decoding improvement mediating reward-related enhancement of behavioral performance. These results strongly support the hypothesis that reward motivation enhances cognitive control, by improving the discriminability of task-relevant information coded and maintained in frontoparietal brain regions.
Previous functional MRI (fMRI) studies have demonstrated group differences in brain activity between deceptive and honest responses. The functional connectivity network related to lie-telling remains largely uncharacterized.
The appropriate use of everyday objects requires the integration of action and function knowledge. Previous research suggests that action knowledge is represented in frontoparietal areas while function knowledge is represented in temporal lobe regions. Here we used multivoxel pattern analysis to investigate the representation of object-directed action and function knowledge while participants executed pantomimes of familiar tool actions. A novel approach for decoding object knowledge was used in which classifiers were trained on one pair of objects and then tested on a distinct pair; this permitted a measurement of classification accuracy over and above object-specific information. Region of interest (ROI) analyses showed that object-directed actions could be decoded in tool-preferring regions of both parietal and temporal cortex, while no independently defined tool-preferring ROI showed successful decoding of object function. However, a whole-brain searchlight analysis revealed that while frontoparietal motor and peri-motor regions are engaged in the representation of object-directed actions, medial temporal lobe areas in the left hemisphere are involved in the representation of function knowledge. These results indicate that both action and function knowledge are represented in a topographically coherent manner that is amenable to study with multivariate approaches, and that the left medial temporal cortex represents knowledge of object function.
A fundamental goal of the human auditory system is to map complex acoustic signals onto stable internal representations of the basic sound patterns of speech. Phonemes and the distinctive features that they comprise constitute the basic building blocks from which higher-level linguistic representations, such as words and sentences, are formed. Although the neural structures underlying phonemic representations have been well studied, there is considerable debate regarding frontal-motor cortical contributions to speech as well as the extent of lateralization of phonological representations within auditory cortex. Here we used functional magnetic resonance imaging (fMRI) and multivoxel pattern analysis to investigate the distributed patterns of activation that are associated with the categorical and perceptual similarity structure of 16 consonant exemplars in the English language used in Miller and Nicely's (1955) classic study of acoustic confusability. Participants performed an incidental task while listening to phonemes in the MRI scanner. Neural activity in bilateral anterior superior temporal gyrus and supratemporal plane was correlated with the first two components derived from a multidimensional scaling analysis of a behaviorally derived confusability matrix. We further showed that neural representations corresponding to the categorical features of voicing, manner of articulation, and place of articulation were widely distributed throughout bilateral primary, secondary, and association areas of the superior temporal cortex, but not motor cortex. Although classification of phonological features was generally bilateral, we found that multivariate pattern information was moderately stronger in the left compared with the right hemisphere for place but not for voicing or manner of articulation.
Selective attention is fundamental for human activity, but the details of its neural implementation remain elusive. One influential theory, the adaptive coding hypothesis (Duncan, 2001, An adaptive coding model of neural function in prefrontal cortex, Nature Reviews Neuroscience 2:820-829), proposes that single neurons in certain frontal and parietal regions dynamically adjust their responses to selectively encode relevant information. This selective representation may in turn support selective processing in more specialized brain regions such as the visual cortices. Here, we use multi-voxel decoding of functional magnetic resonance images to demonstrate selective representation of attended--and not distractor--objects in frontal, parietal, and visual cortices. In addition, we highlight a critical role for task demands in determining which brain regions exhibit selective coding. Strikingly, representation of attended objects in frontoparietal cortex was highest under conditions of high perceptual demand, when stimuli were hard to perceive and coding in early visual cortex was weak. Coding in early visual cortex varied as a function of attention and perceptual demand, while coding in higher visual areas was sensitive to the allocation of attention but robust to changes in perceptual difficulty. Consistent with high-profile reports, peripherally presented objects could also be decoded from activity at the occipital pole, a region which corresponds to the fovea. Our results emphasize the flexibility of frontoparietal and visual systems. They support the hypothesis that attention enhances the multi-voxel representation of information in the brain, and suggest that the engagement of this attentional mechanism depends critically on current task demands.
Post-traumatic stress disorder (PTSD) is considered a multidimensional disorder, with distinct symptom clusters including re-experiencing, avoidance/numbing, hyperarousal, and most recently depersonalization/derealization. However, the extent of differing intrinsic network connectivity underlying these symptoms has not been fully investigated. We therefore investigated the degree of association between resting connectivity of the salience (SN), default mode (DMN), and central executive (CEN) networks and PTSD symptom severity.
In rodent studies, elevated cholinergic neurotransmission in right prefrontal cortex (PFC) is essential for maintaining attentional performance, especially in challenging conditions. Apparently paralleling the rises in acetylcholine seen in rodent studies, fMRI studies in humans reveal right PFC activation at or near Brodmann's areas 9 (BA 9) increases in response to elevated attentional demand. In the present study, we leveraged human genetic variability in the cholinergic system to test the hypothesis that the cholinergic system contributes to the BA 9 response to attentional demand. Specifically, we scanned (BOLD fMRI) participants with a polymorphism of the choline transporter gene that is thought to limit choline transport capacity (Ile89Val variant of the choline transporter gene SLC5A7, rs1013940) and matched controls while they completed a task previously used to demonstrate demand-related increases in right PFC cholinergic transmission in rats and right PFC activation in humans. As hypothesized, we found that although controls showed the typical pattern of robust BA 9 responses to increased attentional demand, Ile89Val participants did not. Further, pattern analysis of activation within this region significantly predicted participant genotype. Additional exploratory pattern classification analyses suggested that Ile89Val participants differentially recruited orbitofrontal cortex and parahippocampal gyrus to maintain attentional performance to the level of controls. These results contribute to a growing body of translational research clarifying the role of cholinergic signaling in human attention and functional neural measures, and begin to outline the risk and resiliency factors associated with potentially suboptimal cholinergic function with implications for disorders characterized by cholinergic dysregulation.
The right temporoparietal junction (rTPJ) is engaged by tasks that manipulate biological motion processing, Theory of Mind attributions, and attention reorienting. The proximity of activations elicited by these tasks raises the question of whether these tasks share common cognitive component processes that are subserved by common neural substrates. Here, we used high-resolution whole-brain functional magnetic resonance imaging in a within-subjects design to determine whether these tasks activate common regions of the rTPJ. Each participant was presented with the 3 tasks in the same imaging session. In a whole-brain analysis, we found that only the right and left TPJs were activated by all 3 tasks. Multivoxel pattern analysis revealed that the regions of overlap could still discriminate the 3 tasks. Notably, we found significant cross-task classification in the right TPJ, which suggests a shared neural process between the 3 tasks. Taken together, these results support prior studies that have indicated functional heterogeneity within the rTPJ but also suggest a convergence of function within a region of overlap. These results also call for further investigation into the nature of the function subserved in this overlap region.
Interindividual differences in the effects of reward on performance are prevalent and poorly understood, with some individuals being more dependent than others on the rewarding outcomes of their actions. The origin of this variability in reward dependence is unknown. Here, we tested the relationship between reward dependence and brain structure in healthy humans. Subjects trained on a visuomotor skill-acquisition task and received performance feedback in the presence or absence of reward. Reward dependence was defined as the statistical trial-by-trial relation between reward and subsequent performance. We report a significant relationship between reward dependence and the lateral prefrontal cortex, where regional gray-matter volume predicted reward dependence but not feedback alone. Multivoxel pattern analysis confirmed the anatomical specificity of this relationship. These results identified a likely anatomical marker for the prospective influence of reward on performance, which may be of relevance in neurorehabilitative settings.
By exploiting information that is contained in the spatial arrangement of neural activations, multivariate pattern analysis (MVPA) can detect distributed brain activations which are not accessible by standard univariate analysis. Recent methodological advances in MVPA regularization techniques have made it feasible to produce sparse discriminative whole-brain maps with highly specific patterns. Furthermore, the most recent refinement, the Graph Net, explicitly takes the 3D-structure of fMRI data into account. Here, these advanced classification methods were applied to a large fMRI sample (N=70) in order to gain novel insights into the functional localization of outcome integration processes. While the beneficial effect of differential outcomes is well-studied in trial-and-error learning, outcome integration in the context of instruction-based learning has remained largely unexplored. In order to examine neural processes associated with outcome integration in the context of instruction-based learning, two groups of subjects underwent functional imaging while being presented with either differential or ambiguous outcomes following the execution of varying stimulus-response instructions. While no significant univariate group differences were found in the resulting fMRI dataset, L1-regularized (sparse) classifiers performed significantly above chance and also clearly outperformed the standard L2-regularized (dense) Support Vector Machine on this whole-brain between-subject classification task. Moreover, additional L2-regularization via the Elastic Net and spatial regularization by the Graph Net improved interpretability of discriminative weight maps but were accompanied by reduced classification accuracies. Most importantly, classification based on sparse regularization facilitated the identification of highly specific regions differentially engaged under ambiguous and differential outcome conditions, comprising several prefrontal regions previously associated with probabilistic learning, rule integration and reward processing. Additionally, a detailed post-hoc analysis of these regions revealed that distinct activation dynamics underlay the processing of ambiguous relative to differential outcomes. Together, these results show that L1-regularization can improve classification performance while simultaneously providing highly specific and interpretable discriminative activation patterns.
How does the brain mediate visual artistic creativity? Here we studied behavioral and neural changes in drawing and painting students compared to students who did not study art. We investigated three aspects of cognition vital to many visual artists: creative cognition, perception, and perception-to-action. We found that the art students became more creative via the reorganization of prefrontal white matter but did not find any significant changes in perceptual ability or related neural activity in the art students relative to the control group. Moreover, the art students improved in their ability to sketch human figures from observation, and multivariate patterns of cortical and cerebellar activity evoked by this drawing task became increasingly separable between art and non-art students. Our findings suggest that the emergence of visual artistic skills is supported by plasticity in neural pathways that enable creative cognition and mediate perceptuomotor integration.
Echo planar imaging (EPI) is the MRI technique that is most widely used for blood oxygen level-dependent (BOLD) functional MRI (fMRI). Recent advances in EPI speed have been made possible with simultaneous multi-slice (SMS) methods which combine acceleration factors M from multiband (MB) radiofrequency pulses and S from simultaneous image refocusing (SIR) to acquire a total of N=SM images in one echo train, providing up to N times speed-up in total acquisition time over conventional EPI. We evaluated accelerations as high as N=48 using different combinations of S and M which allow for whole brain imaging in as little as 100ms at 3T with a 32 channel head coil. The various combinations of acceleration parameters were evaluated by tSNR as well as BOLD contrast-to-noise ratio (CNR) and information content from checkerboard and movie clips in fMRI experiments. We found that at low acceleration factors (N6), setting S=1 and varying M alone yielded the best results in all evaluation metrics, while at acceleration N=8 the results were mixed using both S=1 and S=2 sequences. At higher acceleration factors (N>8), using S=2 yielded maximal BOLD CNR and information content as measured by classification of movie clip frames. Importantly, we found significantly greater BOLD information content using relatively fast TRs in the range of 300ms-600ms compared to a TR of 2s, suggesting that faster TRs capture more information per unit time in task based fMRI.
Although the emotions of other people can often be perceived from overt reactions (e.g., facial or vocal expressions), they can also be inferred from situational information in the absence of observable expressions. How does the human brain make use of these diverse forms of evidence to generate a common representation of a target's emotional state? In the present research, we identify neural patterns that correspond to emotions inferred from contextual information and find that these patterns generalize across different cues from which an emotion can be attributed. Specifically, we use functional neuroimaging to measure neural responses to dynamic facial expressions with positive and negative valence and to short animations in which the valence of a character's emotion could be identified only from the situation. Using multivoxel pattern analysis, we test for regions that contain information about the target's emotional state, identifying representations specific to a single stimulus type and representations that generalize across stimulus types. In regions of medial prefrontal cortex (MPFC), a classifier trained to discriminate emotional valence for one stimulus (e.g., animated situations) could successfully discriminate valence for the remaining stimulus (e.g., facial expressions), indicating a representation of valence that abstracts away from perceptual features and generalizes across different forms of evidence. Moreover, in a subregion of MPFC, this neural representation generalized to trials involving subjectively experienced emotional events, suggesting partial overlap in neural responses to attributed and experienced emotions. These data provide a step toward understanding how the brain transforms stimulus-bound inputs into abstract representations of emotion.
Language processing engages large-scale functional networks in both hemispheres. Although it is widely accepted that left perisylvian regions have a key role in supporting complex grammatical computations, patient data suggest that some aspects of grammatical processing could be supported bilaterally. We investigated the distribution and the nature of grammatical computations across language processing networks by comparing two types of combinatorial grammatical sequences--inflectionally complex words and minimal phrases--and contrasting them with grammatically simple words. Novel multivariate analyses revealed that they engage a coalition of separable subsystems: inflected forms triggered left-lateralized activation, dissociable into dorsal processes supporting morphophonological parsing and ventral, lexically driven morphosyntactic processes. In contrast, simple phrases activated a consistently bilateral pattern of temporal regions, overlapping with inflectional activations in L middle temporal gyrus. These data confirm the role of the left-lateralized frontotemporal network in supporting complex grammatical computations. Critically, they also point to the capacity of bilateral temporal regions to support simple, linear grammatical computations. This is consistent with a dual neurobiological framework where phylogenetically older bihemispheric systems form part of the network that supports language function in the modern human, and where significant capacities for language comprehension remain intact even following severe left hemisphere damage.
To achieve a certain sensory outcome, multiple actions can be executed. For example, unlocking a door might require clockwise or counterclockwise key turns depending on regional norms. Using fMRI in healthy human subjects, we examined the neural networks that dissociate intended sensory outcome from underlying motor actions. Subjects controlled a figure on a computer screen by performing pen traces on an MR-compatible digital tablet. Our design allowed us to dissociate intended sensory outcome (moving the figure in a certain direction) from the underlying motor action (horizontal/vertical pen traces). Using multivoxel pattern analysis and a whole-brain searchlight strategy, we found that activity patterns in left (contralateral) motor and parietal cortex and also right (ipsilateral) motor cortex significantly discriminated direction of pen traces regardless of intended direction of figure movement. Conversely, activity patterns in right superior parietal lobule and premotor cortex, and also left frontopolar cortex, significantly discriminated intended direction of figure movement regardless of underlying direction of hand movement. Together, these results highlight the role of ipsilateral motor cortex in coding movement directions and point to a network of brain regions involved in high order representation of intended sensory outcome that is dissociated from specific motor plans.
The right inferior frontal cortex (rIFC) and the right anterior insula (rAI) have been implicated consistently in inhibitory control, but their differential roles are poorly understood. Here we use multiple quantitative techniques to dissociate the functional organization and roles of the rAI and rIFC. We first conducted a meta-analysis of 70 published inhibitory control studies to generate a commonly activated right fronto-opercular cortex volume of interest (VOI). We then segmented this VOI using two types of features: (1) intrinsic brain activity; and (2) stop-signal task-evoked hemodynamic response profiles. In both cases, segmentation algorithms identified two stable and distinct clusters encompassing the rAI and rIFC. The rAI and rIFC clusters exhibited several distinct functional characteristics. First, the rAI showed stronger intrinsic and task-evoked functional connectivity with the anterior cingulate cortex, whereas the rIFC had stronger intrinsic and task-evoked functional connectivity with dorsomedial prefrontal and lateral fronto-parietal cortices. Second, the rAI showed greater activation than the rIFC during Unsuccessful, but not Successful, Stop trials, and multivoxel response profiles in the rAI, but not the rIFC, accurately differentiated between Successful and Unsuccessful Stop trials. Third, activation in the rIFC, but not rAI, predicted individual differences in inhibitory control abilities. Crucially, these findings were replicated in two independent cohorts of human participants. Together, our findings provide novel quantitative evidence for the dissociable roles of the rAI and rIFC in inhibitory control. We suggest that the rAI is particularly important for detecting behaviorally salient events, whereas the rIFC is more involved in implementing inhibitory control.
Recently, the multivariate analysis methods have been widely used for predicting the human cognitive states from fMRI data. Here, we explore the possibility of predicting the human cognitive states using a pattern of brain activities associated with thinking about concrete objects. The fMRI signals in conjunction with pattern recognition methods were used for the analysis of cognitive functions associated with viewing of 60 object pictures named by the words in 12 categories. The important step in Multi Voxel Pattern Analysis (MVPA) is feature extraction and feature selection parts. In this study, the new feature selection method (accuracy method) was developed for multi-class fMRI dataset to select the informative voxels corresponding to the objects category from the whole brain voxels. Here the result of three multivariate classifiers namely, Naive Bayes, K-nearest neighbor and support vector machine, were compared for predicting the category of presented objects from activation BOLD patterns in human whole brain. We investigated whether the multivariate classifiers are capable to find the associated regions of the brain with the visual presentation of categories of various objects. Overall Naive Bayes classifier perfumed best and it was the best method for extracting features from the whole brain data. In addition, the results of this study indicate that thinking about different semantic categories of objects have an effect on different spatial patterns of neural activation, and so it is possible to identify the category of the objects based on the patterns of neural activation recorded during representation of object line drawing from participants with high accuracy. Finally we demonstrated that the selected brain regions that were informative for object categorization were similar across subjects and this distribution of selected voxels on the cortex may neutrally represent the various object's category properties.
To create subjective experience, our brain must translate physical stimulus input by incorporating prior knowledge and expectations. For example, we perceive color and not wavelength information, and this in part depends on our past experience with colored objects ( Hansen et al. 2006; Mitterer and de Ruiter 2008). Here, we investigated the influence of object knowledge on the neural substrates underlying subjective color vision. In a functional magnetic resonance imaging experiment, human subjects viewed a color that lay midway between red and green (ambiguous with respect to its distance from red and green) presented on either typical red (e.g., tomato), typical green (e.g., clover), or semantically meaningless (nonsense) objects. Using decoding techniques, we could predict whether subjects viewed the ambiguous color on typical red or typical green objects based on the neural response of veridical red and green. This shift of neural response for the ambiguous color did not occur for nonsense objects. The modulation of neural responses was observed in visual areas (V3, V4, VO1, lateral occipital complex) involved in color and object processing, as well as frontal areas. This demonstrates that object memory influences wavelength information relatively early in the human visual system to produce subjective color vision.
Social context plays an important role in human communication. Depending on the nature of the source, the same communication signal might be processed in fundamentally different ways. However, the selective modulation (or "gating") of the flow of neural information during communication is not fully understood. Here, we use multivoxel pattern analysis (MVPA) and multivoxel connectivity analysis (MVCA), a novel technique that allows to analyse context-dependent changes of the strength interregional coupling between ensembles of voxels, to examine how the human brain differentially gates content-specific sensory information during ongoing perception of communication signals. In a simulated electronic communication experiment, participants received two alternative text messages during fMRI ("happy" or "sad") which they believed had been sent either by their real-life friend outside the scanner or by a computer. A region in the dorsal medial prefrontal cortex (dmPFC) selectively increased its functional coupling with sensory-content encoding regions in the visual cortex when a text message was perceived as being sent by the participant's friend, and decreased its functional coupling with these regions when a text message was perceived as being sent by the computer. Furthermore, the strength of neural encoding of content-specific information of text messages in the dmPFC was modulated by the social tie between the participant and her friend: the more of her spare time a participant reported to spend with her friend the stronger was the neural encoding. This suggests that the human brain selectively gates sensory information into the relevant network for processing the mental states of others, depending on the source of the communication signal.
In this fMRI study we contrasted emotional responses to literary reading in late bilinguals' first or second language. German participants with adequate English proficiency in their second language (L2) English read short text passages from Harry Potter books characterized by a "negative" or "positive" versus "neutral" emotional valence manipulation. Previous studies have suggested that given sufficient L2 proficiency, neural substrates involved in L1 versus L2 do not differ (Fabbro, 2001). On the other hand, the question of attenuated emotionality of L2 language processing is still an open debate (see Conrad, Recio, & Jacobs, 2011). Our results revealed a set of neural structures involved in the processing of emotion-laden literature, including emotion-related amygdala and a set of lateral prefrontal, anterior temporal, and temporo-parietal regions associated with discourse comprehension, high-level semantic integration, and Theory-of-Mind processing. Yet, consistent with post-scan emotion ratings of text passages, factorial fMRI analyses revealed stronger hemodynamic responses to "happy" than to "neutral" in bilateral amygdala and the left precentral cortex that were restricted to L1 reading. Furthermore, multivariate pattern analyses (MVPA) demonstrated better classifiability of differential patterns of brain activity elicited by passages of different emotional content in L1 than in L2 for the whole brain level. Overall, our results suggest that reading emotion-laden texts in our native language provides a stronger and more differentiated emotional experience than reading in a second language.
For more than 50 years, psychologists and neuroscientists have recognized the importance of a working memory to coordinate processing when multiple goals are active and to guide behavior with information that is not present in the immediate environment. In recent years, psychological theory and cognitive neuroscience data have converged on the idea that information is encoded into working memory by allocating attention to internal representations, whether semantic long-term memory (e.g., letters, digits, words), sensory, or motoric. Thus, information-based multivariate analyses of human functional MRI data typically find evidence for the temporary representation of stimuli in regions that also process this information in nonworking memory contexts. The prefrontal cortex (PFC), on the other hand, exerts control over behavior by biasing the salience of mnemonic representations and adjudicating among competing, context-dependent rules. The "control of the controller" emerges from a complex interplay between PFC and striatal circuits and ascending dopaminergic neuromodulatory signals.
Chronic cannabis use can cause cognitive, perceptual and personality alterations, which are believed to be associated with regional brain changes and possible changes in connectivity between functional regions. This study aims to identify the changes from resting state functional magnetic resonance imaging scans. A two-level multi-voxel pattern analysis was proposed to classify male cannabis users from normal controls. The first level analysis works on a voxel basis and identifies clusters for the input of a second level analysis, which works on the functional connectivity between these regions. We found distinct clusters for male cannabis users in the middle frontal gyrus, precentral gyrus, superior frontal gyrus, posterior cingulate cortex, cerebellum and some other regions. Based on the functional connectivity of these clusters, a high overall accuracy rate of 84-88% in classification accuracy was achieved. High correlations were also found between the overall classification accuracy and Barrett Barrett Impulsiveness Scale factor scores of attention and motor. Our result suggests regional differences in the brains of male cannabis users that span from the cerebellum to the prefrontal cortex, which are associated with differences in functional connectivity.
Visual working memory (WM) enables us to store and manipulate visual information for a short duration. Traditionally, prefrontal and parietal regions have been associated with visual WM processing; however recent fMRI studies have shown that visual WM information can be decoded from the visual cortex as well. In this study, we used transcranial direct current stimulation (tDCS) to investigate the role of the visual cortex in retaining visual WM information. All subjects participated in two sessions of sham and active tDCS followed by a standard visual WM task. Two conditions were tested: in short encoding trials, the memory array (6 colored circles) was presented for 200ms whereas in long encoding trials it was presented for 500ms. We hypothesized that if stimulation over visual cortex modulates WM retention, then performance should be enhanced in both encoding conditions. However, if stimulation over visual cortex modulates mainly WM consolidation, then performance should improve only in the short encoding condition. The results supported the latter possibility as stimulation improved performance in the short encoding condition but not in the long encoding condition. Consequently, the robust advantage of the long encoding condition over the short encoding condition after sham stimulation was eliminated after active stimulation. These results suggest that the visual cortex is significant for WM consolidation, while it plays a smaller part in holding visual WM representations.
Humans recognize faces and objects with high speed and accuracy regardless of their orientation. Recent studies have proposed that orientation invariance in face recognition involves an intermediate representation where neural responses are similar for mirror-symmetric views. Here, we used fMRI, multivariate pattern analysis, and computational modeling to investigate the neural encoding of faces and vehicles at different rotational angles. Corroborating previous studies, we demonstrate a representation of face orientation in the fusiform face-selective area (FFA). We go beyond these studies by showing that this representation is category-selective and tolerant to retinal translation. Critically, by controlling for low-level confounds, we found the representation of orientation in FFA to be compatible with a linear angle code. Aspects of mirror-symmetric coding cannot be ruled out when FFA mean activity levels are considered as a dimension of coding. Finally, we used a parametric family of computational models, involving a biased sampling of view-tuned neuronal clusters, to compare different face angle encoding models. The best fitting model exhibited a predominance of neuronal clusters tuned to frontal views of faces. In sum, our findings suggest a category-selective and monotonic code of face orientation in the human FFA, in line with primate electrophysiology studies that observed mirror-symmetric tuning of neural responses at higher stages of the visual system, beyond the putative homolog of human FFA.
Functional neuroimaging studies consistently report that the visual perception of faces and bodies strongly activates regions within ventral occipitotemporal cortex (VOTC) and, in particular, within the mid-lateral fusiform gyrus. One unresolved issue is the degree to which faces and bodies activate discrete or overlapping cortical regions within this region. Here, we examined VOTC activity to faces and bodies at high spatial resolution, using univariate and multivariate analysis approaches sensitive to differences in both the strength and spatial pattern of activation. Faces and bodies evoked substantially overlapping activations in the fusiform gyrus when each was compared to the control category of houses. No discrete regions of activation for faces and bodies in the fusiform gyrus survived a direct statistical comparison using standard univariate statistics. However, multi-voxel pattern analysis differentiated faces and bodies in regions where univariate analysis found no significant difference in the strength of activation. Using a whole-brain multivariate searchlight approach, we also found that extensive regions in VOTC beyond those defined as fusiform face and body areas using standard criteria where the spatial pattern of activation discriminated faces and bodies. These findings provide insights into the spatial distribution of face- and body-specific activations in VOTC and the identification of functionally specialized regions.
Recent studies suggest common neural substrates involved in verbal and visual working memory (WM), interpreted as reflecting shared attention-based, short-term retention mechanisms. We used a machine-learning approach to determine more directly the extent to which common neural patterns characterize retention in verbal WM and visual WM. Verbal WM was assessed via a standard delayed probe recognition task for letter sequences of variable length. Visual WM was assessed via a visual array WM task involving the maintenance of variable amounts of visual information in the focus of attention. We trained a classifier to distinguish neural activation patterns associated with high- and low-visual WM load and tested the ability of this classifier to predict verbal WM load (high-low) from their associated neural activation patterns, and vice versa. We observed significant between-task prediction of load effects during WM maintenance, in posterior parietal and superior frontal regions of the dorsal attention network; in contrast, between-task prediction in sensory processing cortices was restricted to the encoding stage. Furthermore, between-task prediction of load effects was strongest in those participants presenting the highest capacity for the visual WM task. This study provides novel evidence for common, attention-based neural patterns supporting verbal and visual WM.
There is not only evidence for behavioral differences in voice perception between female and male listeners, but also recent suggestions for differences in neural correlates between genders. The fMRI functional voice localizer (comprising a univariate analysis contrasting stimulation with vocal vs. non-vocal sounds) is known to give robust estimates of the temporal voice areas (TVAs). However, there is growing interest in employing multivariate analysis approaches to fMRI data (e.g., multivariate pattern analysis; MVPA). The aim of the current study was to localize voice-related areas in both female and male listeners and to investigate whether brain maps may differ depending on the gender of the listener. After a univariate analysis, a random effects analysis was performed on female (n = 149) and male (n = 123) listeners and contrasts between them were computed. In addition, MVPA with a whole-brain searchlight approach was implemented and classification maps were entered into a second-level permutation based random effects models using statistical non-parametric mapping (SnPM; Nichols and Holmes, 2002). Gender differences were found only in the MVPA. Identified regions were located in the middle part of the middle temporal gyrus (bilateral) and the middle superior temporal gyrus (right hemisphere). Our results suggest differences in classifier performance between genders in response to the voice localizer with higher classification accuracy from local BOLD signal patterns in several temporal-lobe regions in female listeners.
Skilled performance is characterized by precise and flexible control of movement sequences in space and time. Recent theories suggest that integrated spatio-temporal trajectories are generated by intrinsic dynamics of motor and premotor networks. This contrasts with behavioural advantages that emerge when a trained spatial or temporal feature of sequences is transferred to a new spatio-temporal combination arguing for independent neural representations of these sequence features. We used a new fMRI pattern classification approach to identify brain regions with independent vs integrated representations. A distinct regional dissociation within motor areas was revealed: whereas only the contralateral primary motor cortex exhibited unique patterns for each spatio-temporal sequence combination, bilateral premotor areas represented spatial and temporal features independently of each other. These findings advocate a unique function of higher motor areas for flexible recombination and efficient encoding of complex motor behaviours.
Perceptual confidence refers to the degree to which we believe in the accuracy of our percepts. Signal detection theory suggests that perceptual confidence is computed from an internal "decision variable," which reflects the amount of available information in favor of one or another perceptual interpretation of the sensory input. The neural processes underlying these computations have, however, remained elusive. Here, we used fMRI and multivariate decoding techniques to identify regions of the human brain that encode this decision variable and confidence during a visual motion discrimination task. We used observers' binary perceptual choices and confidence ratings to reconstruct the internal decision variable that governed the subjects' behavior. A number of areas in prefrontal and posterior parietal association cortex encoded this decision variable, and activity in the ventral striatum reflected the degree of perceptual confidence. Using a multivariate connectivity analysis, we demonstrate that patterns of brain activity in the right ventrolateral prefrontal cortex reflecting the decision variable were linked to brain signals in the ventral striatum reflecting confidence. Our results suggest that the representation of perceptual confidence in the ventral striatum is derived from a transformation of the continuous decision variable encoded in the cerebral cortex.
Cognitive control allows stimulus-response processing to be aligned with internal goals and is thus central to intelligent, purposeful behavior. Control is thought to depend in part on the active representation of task information in prefrontal cortex (PFC), which provides a source of contextual bias on perception, decision making, and action. In the present study, we investigated the organization, influences, and consequences of context representation as human subjects performed a cued sorting task that required them to flexibly judge the relationship between pairs of multivalent stimuli. Using a connectivity-based parcellation of PFC and multivariate decoding analyses, we determined that context is specifically and transiently represented in a region spanning the inferior frontal sulcus during context-dependent decision making. We also found strong evidence that decision context is represented within the intraparietal sulcus, an area previously shown to be functionally networked with the inferior frontal sulcus at rest and during task performance. Rule-guided allocation of attention to different stimulus dimensions produced discriminable patterns of activation in visual cortex, providing a signature of top-down bias over perception. Furthermore, demands on cognitive control arising from the task structure modulated context representation, which was found to be strongest after a shift in task rules. When context representation in frontoparietal areas increased in strength, as measured by the discriminability of high-dimensional activation patterns, the bias on attended stimulus features was enhanced. These results provide novel evidence that illuminates the mechanisms by which humans flexibly guide behavior in complex environments.
The human brain is remarkably adept at integrating complex information to form unified psychological representations of agents, objects, and events in the environment. Two domains in which this ability is particularly salient are the processing of social and valence information and are supported by common cortical areas in the medial pFC (MPFC). Because social information is often embedded within valenced emotional contexts, it is possible that activation patterns within the MPFC may represent both of these types of cognitive processes when presented simultaneously. The current study tested this possibility by employing a large-scale automated meta-analysis tool, together with multivoxel pattern analysis to investigate the representational similarity of social and valence information in the MPFC during fMRI. Using a representational similarity analysis, we found a high degree of representational similarity both within social dimensions and within valence dimensions, but not across them (e.g., positive social information was highly dissimilar to negative nonsocial information), in a ventral portion of the MPFC. These results were significantly correlated with a behaviorally measured similarity structure of the same stimuli, suggesting that a psychologically meaningful representation of social and valence information is reflected by multivoxel activation patterns in the ventral MPFC.
Rational decision-making models assume that people resolve an economic problem based on its properties and the underlying utility. Here we challenge this view by examining whether pre-stimulus endogenous neuronal fluctuations can bias economic decisions. We recorded subjects' pre-stimulus neural activation patterns with fMRI before presentation and choice between pairs of certain outcomes and risky gambles. Our results indicate that activities in the left nucleus accumbens and medial frontal gyrus can bias subsequent risky decision making, showing that neuronal activities in regions associated with uncertainty and reward processing are involved in biasing subsequent choice selection. This finding challenges theories which propose that choices merely reveal stable underlying distributions of hedonic utility. Endogenous brain states of this sort might originate from a systematic cause or a stochastic type of neural noise, which can be construed as contextual factors that shape people's decision making.
When choosing actions, humans have to balance carefully between different task demands. On the one hand, they should perform tasks repeatedly to avoid frequent and effortful switching between different tasks. On the other hand, subjects have to retain their flexibility to adapt to changes in external task demands such as switching away from an increasingly difficult task. Here, we developed a difficulty-based choice task to investigate how subjects voluntarily select task-sets in predictably changing environments. Subjects were free to choose 1 of the 3 task-sets on a trial-by-trial basis, while the task difficulty changed dynamically over time. Subjects self-sequenced their behavior in this environment while we measured brain responses with functional magnetic resonance imaging (fMRI). Using multivariate decoding, we found that task choices were encoded in the medial prefrontal cortex (dorso-medial prefrontal cortex, dmPFC, and dorsal anterior cingulate cortex, dACC). The same regions were found to encode task difficulty, a major factor influencing choices. Importantly, the present paradigm allowed us to disentangle the neural code for task choices and task difficulty, ensuring that activation patterns in dmPFC/dACC independently encode these 2 factors. This finding provides new evidence for the importance of the dmPFC/dACC for task-selection and motivational functions in highly dynamic environments.
Sustained, long-term cognitive workload is associated with variations and decrements in performance. Such fluctuations in vigilance can be a risk factor especially during dangerous attention demanding activities. Functional MRI studies have shown that attentional performance is correlated with BOLD-signals, especially in parietal and prefrontal cortical regions. An interesting question is whether these BOLD-signals could be measured in real-world scenarios, say to warn in a dangerous workplace whenever a subjects' vigilance is low. Because fMRI lacks the mobility needed for such applications, we tested whether the monitoring of vigilance might be possible using Near-Infrared Spectroscopy (NIRS). NIRS is a highly mobile technique that measures hemodynamics in the surface of the brain. We demonstrate that non-invasive NIRS signals correlate with vigilance. These signals carry enough information to decode subjects' reaction times at a single trial level.
Visual stimuli can be kept from awareness using various methods. The extent of processing that a given stimulus receives in the absence of awareness is typically used to make claims about the role of consciousness more generally. The neural processing elicited by a stimulus, however, may also depend on the method used to keep it from awareness, and not only on whether the stimulus reaches awareness. Here we report that the method used to render an image invisible has a dramatic effect on how category information about the unseen stimulus is encoded across the human brain. We collected fMRI data while subjects viewed images of faces and tools, that were rendered invisible using either continuous flash suppression (CFS) or chromatic flicker fusion (CFF). In a third condition, we presented the same images under normal fully visible viewing conditions. We found that category information about visible images could be extracted from patterns of fMRI responses throughout areas of neocortex known to be involved in face or tool processing. However, category information about stimuli kept from awareness using CFS could be recovered exclusively within occipital cortex, whereas information about stimuli kept from awareness using CFF was also decodable within temporal and frontal regions. We conclude that unconsciously presented objects are processed differently depending on how they are rendered subjectively invisible. Caution should therefore be used in making generalizations on the basis of any one method about the neural basis of consciousness or the extent of information processing without consciousness.
Machine learning-based approaches are now able to examine functional magnetic resonance imaging data in a multivariate manner and extract features predictive of group membership. We applied support vector machine (SVM)-based classification to resting state functional connectivity (rsFC) data from nicotine-dependent smokers and healthy controls to identify brain-based features predictive of nicotine dependence. By employing a network-centered approach, we observed that within-network functional connectivity measures offered maximal information for predicting smoking status, as opposed to between-network connectivity, or the representativeness of each individual node with respect to its parent network. Further, our analysis suggests that connectivity measures within the executive control and frontoparietal networks are particularly informative in predicting smoking status. Our findings suggest that machine learning-based approaches to classifying rsFC data offer a valuable alternative technique to understanding large-scale differences in addiction-related neurobiology.
To investigate the neural underpinnings of word decoding, and how it changes as a function of repeated exposure, we trained Dutch participants repeatedly over the course of a month of training to articulate a set of novel disyllabic input strings written in Greek script to avoid the use of familiar orthographic representations. The syllables in the input were phonotactically legal combinations but non-existent in the Dutch language, allowing us to assess their role in novel word decoding. Not only trained disyllabic pseudowords were tested but also pseudowords with recombined patterns of syllables to uncover the emergence of syllabic representations. We showed that with extensive training, articulation became faster and more accurate for the trained pseudowords. On the neural level, the initial stage of decoding was reflected by increased activity in visual attention areas of occipito-temporal and occipito-parietal cortices, and in motor coordination areas of the precentral gyrus and the inferior frontal gyrus. After one month of training, memory representations for holistic information (whole word unit) were established in areas encompassing the angular gyrus, the precuneus and the middle temporal gyrus. Syllabic representations also emerged through repeated training of disyllabic pseudowords, such that reading recombined syllables of the trained pseudowords showed similar brain activation to trained pseudowords and were articulated faster than novel combinations of letter strings used in the trained pseudowords.
Previous functional magnetic resonance imaging (fMRI) research on action observation has emphasized the role of putative mirror neuron areas such as Broca's area, ventral premotor cortex, and the inferior parietal lobule. However, recent evidence suggests action observation involves many distributed cortical regions, including dorsal premotor and superior parietal cortex. How these different regions relate to traditional mirror neuron areas, and whether traditional mirror neuron areas play a special role in action representation, is unclear. Here we use multi-voxel pattern analysis (MVPA) to show that action representations, including observation, imagery, and execution of reaching movements: (1) are distributed across both dorsal (superior) and ventral (inferior) premotor and parietal areas; (2) can be decoded from areas that are jointly activated by observation, execution, and imagery of reaching movements, even in cases of equal-amplitude blood oxygen level-dependent (BOLD) responses; and (3) can be equally accurately classified from either posterior parietal or frontal (premotor and inferior frontal) regions. These results challenge the presumed dominance of traditional mirror neuron areas such as Broca's area in action observation and action representation more generally. Unlike traditional univariate fMRI analyses, MVPA was able to discriminate between imagined and observed movements from previously indistinguishable BOLD activations in commonly activated regions, suggesting finer-grained distributed patterns of activation.
In Ridley Scott's film "Blade Runner", empathy-detection devices are employed to measure affiliative emotions. Despite recent neurocomputational advances, it is unknown whether brain signatures of affiliative emotions, such as tenderness/affection, can be decoded and voluntarily modulated. Here, we employed multivariate voxel pattern analysis and real-time fMRI to address this question. We found that participants were able to use visual feedback based on decoded fMRI patterns as a neurofeedback signal to increase brain activation characteristic of tenderness/affection relative to pride, an equally complex control emotion. Such improvement was not observed in a control group performing the same fMRI task without neurofeedback. Furthermore, the neurofeedback-driven enhancement of tenderness/affection-related distributed patterns was associated with local fMRI responses in the septohypothalamic area and frontopolar cortex, regions previously implicated in affiliative emotion. This demonstrates that humans can voluntarily enhance brain signatures of tenderness/affection, unlocking new possibilities for promoting prosocial emotions and countering antisocial behavior.
Multi-variate pattern analysis (MVPA) applied to BOLD-fMRI has proven successful at decoding complicated fMRI signal patterns associated with a variety of cognitive processes. One cognitive process, not yet investigated, is the mental representation of "Yes/No" thoughts that precede the actual overt response to a binary "Yes/No" question. In this study, we focus on examining: (1) whether spatial patterns of the hemodynamic response carry sufficient information to allow reliable decoding of "Yes/No" thoughts; and (2) whether decoding of "Yes/No" thoughts is independent of the intention to respond honestly or dishonestly. To achieve this goal, we conducted two separate experiments. Experiment 1, collected on a 3T scanner, examined the whole brain to identify regions that carry sufficient information to permit significantly above-chance prediction of "Yes/No" thoughts at the group level. In Experiment 2, collected on a 7T scanner, we focused on the regions identified in Experiment 1 to examine the capability of achieving high decoding accuracy at the single subject level. A set of regions--namely right superior temporal gyrus, left supra-marginal gyrus, and left middle frontal gyrus--exhibited high decoding power. Decoding accuracy for these regions increased with trial averaging. When 18 trials were averaged, the median accuracies were 82.5%, 77.5%, and 79.5%, respectively. When trials were separated according to deceptive intentions (set via experimental cues), and classifiers were trained on honest trials, but tested on trials where subjects were asked to deceive, the median accuracies of these regions still reached 66%, 75%, and 78.5%. These results provide evidence that concealed "Yes/No" thoughts are encoded in the BOLD signal, retaining some level of independence from the subject's intentions to answer honestly or dishonestly. These findings also suggest the theoretical possibility for more efficient brain-computer interfaces where subjects only need to think their answers to communicate.
We have developed a brain-computer interface (BCI) system based on real-time functional magnetic resonance imaging (fMRI) with virtual reality feedback. The advantage of fMRI is the relatively high spatial resolution and the coverage of the whole brain; thus we expect that it may be used to explore novel BCI strategies, based on new types of mental activities. However, fMRI suffers from a low temporal resolution and an inherent delay, since it is based on a hemodynamic response rather than electrical signals. Thus, our objective in this paper was to explore whether subjects could perform a BCI task in a virtual environment using our system, and how their performance was affected by the delay.
During wakefulness, a constant and continuous stream of complex stimuli and self-driven thoughts permeate the human mind. Here, eleven participants were asked to count down numbers and remember negative or positive autobiographical episodes of their personal lives, for 32 seconds at a time, during which they could freely engage in the execution of those tasks. We then examined the possibility of determining from a single whole-brain functional magnetic resonance imaging scan which one of the two mental tasks each participant was performing at a given point in time. Linear support-vector machines were used to build within-participant classifiers and across-participants classifiers. The within-participant classifiers could correctly discriminate scans with an average accuracy as high as 82%, when using data from all individual voxels in the brain. These results demonstrate that it is possible to accurately classify self-driven mental tasks from whole-brain activity patterns recorded in a time interval as short as 2 seconds.
Multivoxel pattern analysis (MVPA) is a sensitive and increasingly popular method for examining differences between neural activation patterns that cannot be detected using classical mass-univariate analysis. Recently, Todd et al. ("Confounds in multivariate pattern analysis: Theory and rule representation case study", 2013, NeuroImage 77: 157-165) highlighted a potential problem for these methods: high sensitivity to confounds at the level of individual participants due to the use of directionless summary statistics. Unlike traditional mass-univariate analyses where confounding activation differences in opposite directions tend to approximately average out at group level, group level MVPA results may be driven by any activation differences that can be discriminated in individual participants. In Todd et al.'s empirical data, factoring out differences in reaction time (RT) reduced a classifier's ability to distinguish patterns of activation pertaining to two task rules. This raises two significant questions for the field: to what extent have previous multivoxel discriminations in the literature been driven by RT differences, and by what methods should future studies take RT and other confounds into account? We build on the work of Todd et al. and compare two different approaches to remove the effect of RT in MVPA. We show that in our empirical data, in contrast to that of Todd et al., the effect of RT on rule decoding is negligible, and results were not affected by the specific details of RT modelling. We discuss the meaning of and sensitivity for confounds in traditional and multivoxel approaches to fMRI analysis. We observe that the increased sensitivity of MVPA comes at a price of reduced specificity, meaning that these methods in particular call for careful consideration of what differs between our conditions of interest. We conclude that the additional complexity of the experimental design, analysis and interpretation needed for MVPA is still not a reason to favour a less sensitive approach.
The goal of this research is to discover the stages of mathematical problem solving, the factors that influence the duration of these stages, and how these stages are related to the learning of a new mathematical competence. Using a combination of multivariate pattern analysis (MVPA) and hidden Markov models (HMM), we found that participants went through 5 major phases in solving a class of problems: A Define Phase where they identified the problem to be solved, an Encode Phase where they encoded the needed information, a Compute Phase where they performed the necessary arithmetic calculations, a Transform Phase where they performed any mathematical transformations, and a Respond Phase where they entered an answer. The Define Phase is characterized by activity in visual attention and default network regions, the Encode Phase by activity in visual regions, the Compute Phase by activity in regions active in mathematical tasks, the Transform Phase by activity in mathematical and response regions, and the Respond phase by activity in motor regions. The duration of the Compute and Transform Phases were the only ones that varied with condition. Two features distinguished the mastery trials on which participants came to understand a new problem type. First, the duration of late phases of the problem solution increased. Second, there was increased activation in the rostrolateral prefrontal cortex (RLPFC) and angular gyrus (AG), regions associated with metacognition. This indicates the importance of reflection to successful learning.
Many powerful human emotional thoughts are generated in the absence of a precipitating event in the environment. Here, we tested whether we can decode the valence of internally driven, self-generated thoughts during task-free rest based on neural similarities with task-related affective mental states. We acquired functional magnetic resonance imaging (fMRI) data while participants generated positive and negative thoughts as part of an attribution task (Session A) and while they reported the occurrence of comparable mental states during task-free rest periods (Session B). With the use of multivariate pattern analyses (MVPA), we identified response patterns in the medial orbitofrontal cortex (mOFC) that encode the affective content of thoughts that are generated in response to an external experimental cue. Importantly, these task driven response patterns reliably predicted the occurrence of affective thoughts generated during unconstrained rest periods recorded one week apart. This demonstrates that at least certain elements of task-cued and task-free affective experiences rely on a common neural code. Furthermore, our findings reveal the role that the mOFC plays in determining the affective tone of unconstrained thoughts. More generally, our results suggest that MVPA is an important methodological tool for attempts to understand unguided subject driven mental states such as mind-wandering and daydreaming based on neural similarities with task-based experiences.
Behavioral studies have demonstrated that descending pain modulation can be spatially specific, as is evident in placebo analgesia, which can be limited to the location at which pain relief is expected. This suggests that higher-order cortical structures of the descending pain modulatory system carry spatial information about the site of stimulation. Here, we used functional magnetic resonance imaging and multivariate pattern analysis in 15 healthy human volunteers to test whether spatial information of painful stimuli is represented in areas of the descending pain modulatory system. We show that the site of nociceptive stimulation (arm or leg) can be successfully decoded from local patterns of brain activity during the anticipation and receipt of painful stimulation in the rostral anterior cingulate cortex, the dorsolateral prefrontal cortices, and the contralateral parietal operculum. These results demonstrate that information regarding the site of nociceptive stimulation is represented in these brain regions. Attempts to predict arm and leg stimulation from the periaqueductal gray, control regions (e.g., white matter) or the control time interval in the intertrial phase did not allow for classifications above chance level. This finding represents an important conceptual advance in the understanding of endogenous pain control mechanisms by bridging the gap between previous behavioral and neuroimaging studies, suggesting a spatial specificity of endogenous pain control.
Multivariate pattern classification analysis (MVPA) has been applied to functional magnetic resonance imaging (fMRI) data to decode brain states from spatially distributed activation patterns. Decoding upper limb movements from non-invasively recorded human brain activation is crucial for implementing a brain-machine interface that directly harnesses an individual's thoughts to control external devices or computers. The aim of this study was to decode the individual finger movements from fMRI single-trial data. Thirteen healthy human subjects participated in a visually cued delayed finger movement task, and only one slight button press was performed in each trial. Using MVPA, the decoding accuracy (DA) was computed separately for the different motor-related regions of interest. For the construction of feature vectors, the feature vectors from two successive volumes in the image series for a trial were concatenated. With these spatial-temporal feature vectors, we obtained a 63.1% average DA (84.7% for the best subject) for the contralateral primary somatosensory cortex and a 46.0% average DA (71.0% for the best subject) for the contralateral primary motor cortex; both of these values were significantly above the chance level (20%). In addition, we implemented searchlight MVPA to search for informative regions in an unbiased manner across the whole brain. Furthermore, by applying searchlight MVPA to each volume of a trial, we visually demonstrated the information for decoding, both spatially and temporally. The results suggest that the non-invasive fMRI technique may provide informative features for decoding individual finger movements and the potential of developing an fMRI-based brain-machine interface for finger movement.
Using textures mapped onto virtual nonsense objects, it has recently been shown that early visual cortex plays an important role in processing material properties. Here, we examined brain activation to photographs of materials, consisting of wood, stone, metal and fabric surfaces. These photographs were close-ups in the sense that the materials filled the image. In the first experiment, observers categorized the material in each image (i.e., wood, stone, metal, or fabric), while in an fMRI-scanner. We predicted the assigned material category using the obtained voxel patterns using a linear classifier. Region-of-interest and whole-brain analyses demonstrated material coding in the early visual regions, with lower accuracies for more anterior regions. There was little evidence for material coding in other brain regions. In the second experiment, we used an adaptation paradigm to reveal additional brain areas involved in the perception of material categories. Participants viewed images of wood, stone, metal, and fabric, presented in blocks with images of either different material categories (no adaptation) or images of different samples from the same material category (material adaptation). To measure baseline activation, blocks with the same material sample were presented (baseline adaptation). Material adaptation effects were found mainly in the parahippocampal gyrus, in agreement with fMRI-studies of texture perception. Our findings suggest that the parahippocampal gyrus, early visual cortex, and possibly the supramarginal gyrus are involved in the perception of material categories, but in different ways. The different outcomes from the two studies are likely due to inherent differences between the two paradigms. A third experiment suggested, based on anatomical overlap between activations, that spatial frequency information is important for within-category material discrimination.
Face pareidolia is the illusory perception of non-existent faces. The present study, for the first time, contrasted behavioral and neural responses of face pareidolia with those of letter pareidolia to explore face-specific behavioral and neural responses during illusory face processing. Participants were shown pure-noise images but were led to believe that 50% of them contained either faces or letters; they reported seeing faces or letters illusorily 34% and 38% of the time, respectively. The right fusiform face area (rFFA) showed a specific response when participants "saw" faces as opposed to letters in the pure-noise images. Behavioral responses during face pareidolia produced a classification image (CI) that resembled a face, whereas those during letter pareidolia produced a CI that was letter-like. Further, the extent to which such behavioral CIs resembled faces was directly related to the level of face-specific activations in the rFFA. This finding suggests that the rFFA plays a specific role not only in processing of real faces but also in illusory face perception, perhaps serving to facilitate the interaction between bottom-up information from the primary visual cortex and top-down signals from the prefrontal cortex (PFC). Whole brain analyses revealed a network specialized in face pareidolia, including both the frontal and occipitotemporal regions. Our findings suggest that human face processing has a strong top-down component whereby sensory input with even the slightest suggestion of a face can result in the interpretation of a face.
In this study, we use functional magnetic resonance imaging (fMRI) in combination with multivoxel pattern analysis to address the question of how mental activities that correspond to sentence polarity (affirmative or negative sentences) are encoded in the brain. This approach allows us to investigate the role of left/right dorsolateral prefrontal cortex (DLPFC) in predicting the neural activity of fMRI associated with sentence polarities. Subjects in the experiment were asked to judge the matching of the presented picture with the meaning of affirmative and negative sentences. Our results highlight the role of RDLPFC in encoding of the related mental activity to sentence polarities such that the right hemisphere (RDLPFC) can predict sentence polarity with high accuracy as compared to the left hemisphere (LDLPFC), and that the negative sentences are decoded with high performance as compared to affirmative sentences from the RDLPFC across subjects. In addition, this experiment's results show that negative sentences involve more syntactic structure than affirmative sentences.
Perception reflects an integration of "bottom-up" (sensory-driven) and "top-down" (internally generated) signals. Although models of visual processing often emphasize the central role of feed-forward hierarchical processing, less is known about the impact of top-down signals on complex visual representations. Here, we investigated whether and how the observer's goals modulate object processing across the cortex. We examined responses elicited by a diverse set of objects under six distinct tasks, focusing on either physical (e.g., color) or conceptual properties (e.g., man-made). Critically, the same stimuli were presented in all tasks, allowing us to investigate how task impacts the neural representations of identical visual input. We found that task has an extensive and differential impact on object processing across the cortex. First, we found task-dependent representations in the ventral temporal and prefrontal cortex. In particular, although object identity could be decoded from the multivoxel response within task, there was a significant reduction in decoding across tasks. In contrast, the early visual cortex evidenced equivalent decoding within and across tasks, indicating task-independent representations. Second, task information was pervasive and present from the earliest stages of object processing. However, although the responses of the ventral temporal, prefrontal, and parietal cortex enabled decoding of both the type of task (physical/conceptual) and the specific task (e.g., color), the early visual cortex was not sensitive to type of task and could only be used to decode individual physical tasks. Thus, object processing is highly influenced by the behavioral goal of the observer, highlighting how top-down signals constrain and inform the formation of visual representations.
Language is a high-level cognitive function, so exploring the neural correlates of unconscious language processing is essential for understanding the limits of unconscious processing in general. The results of several functional magnetic resonance imaging studies have suggested that unconscious lexical and semantic processing is confined to the posterior temporal lobe, without involvement of the frontal lobe-the regions that are indispensable for conscious language processing. However, previous studies employed a similarly designed masked priming paradigm with briefly presented single and contextually unrelated words. It is thus possible, that the stimulation level was insufficiently strong to be detected in the high-level frontal regions. Here, in a high-resolution fMRI and multivariate pattern analysis study we explored the neural correlates of subliminal language processing using a novel paradigm, where written meaningful sentences were suppressed from awareness for extended duration using continuous flash suppression. We found that subjectively and objectively invisible meaningful sentences and unpronounceable nonwords could be discriminated not only in the left posterior superior temporal sulcus (STS), but critically, also in the left middle frontal gyrus. We conclude that frontal lobes play a role in unconscious language processing and that activation of the frontal lobes per se might not be sufficient for achieving conscious awareness.
In this event-related fMRI study we investigated the effect of 5 days of implicit acquisition on preference classification by means of an artificial grammar learning (AGL) paradigm based on the structural mere-exposure effect and preference classification using a simple right-linear unification grammar. This allowed us to investigate implicit AGL in a proper learning design by including baseline measurements prior to grammar exposure. After 5 days of implicit acquisition, the fMRI results showed activations in a network of brain regions including the inferior frontal (centered on BA 44/45) and the medial prefrontal regions (centered on BA 8/32). Importantly, and central to this study, the inclusion of a naive preference fMRI baseline measurement allowed us to conclude that these fMRI findings were the intrinsic outcomes of the learning process itself and not a reflection of a preexisting functionality recruited during classification, independent of acquisition. Support for the implicit nature of the knowledge utilized during preference classification on day 5 come from the fact that the basal ganglia, associated with implicit procedural learning, were activated during classification, while the medial temporal lobe system, associated with explicit declarative memory, was consistently deactivated. Thus, preference classification in combination with structural mere-exposure can be used to investigate structural sequence processing (syntax) in unsupervised AGL paradigms with proper learning designs.
In this paper we propose a method for whole brain parcellation using the type of generative parametric models typically used in tissue classification. Compared to the non-parametric, multi-atlas segmentation techniques that have become popular in recent years, our method obtains state-of-the-art segmentation performance in both cortical and subcortical structures, while retaining all the benefits of generative parametric models, including high computational speed, automatic adaptiveness to changes in image contrast when different scanner platforms and pulse sequences are used, and the ability to handle multi-contrast (vector-valued intensities) MR data. We have validated our method by comparing its segmentations to manual delineations both within and across scanner platforms and pulse sequences, and show preliminary results on multi-contrast test-retest scans, demonstrating the feasibility of the approach.
There has been a long history of research on visual working memory. Whereas early studies have focused on the role of lateral prefrontal cortex in the storage of sensory information, this has been challenged by research in humans that has directly assessed the encoding of perceptual contents, pointing towards a role of visual and parietal regions during storage. In a previous study we used pattern classification to investigate the storage of complex visual color patterns across delay periods. This revealed coding of such contents in early visual and parietal brain regions. Here we aim to investigate whether the involvement of visual and parietal cortex is also observable for other types of complex, visuo-spatial pattern stimuli. Specifically, we used a combination of fMRI and multivariate classification to investigate the retention of complex flow-field stimuli defined by the spatial patterning of motion trajectories of random dots. Subjects were trained to memorize the precise spatial layout of these stimuli and to retain this information during an extended delay. We used a multivariate decoding approach to identify brain regions where spatial patterns of activity encoded the memorized stimuli. Content-specific memory signals were observable in motion sensitive visual area MT+ and in posterior parietal cortex that might encode spatial information in a modality independent manner. Interestingly, we also found information about the memorized visual stimulus in somatosensory cortex, suggesting a potential crossmodal contribution to memory. Our findings thus indicate that working memory storage of visual percepts might be distributed across unimodal, multimodal and even crossmodal brain regions.
What are the neural mechanisms underlying working memory (WM)? One influential theory posits that neurons in the lateral prefrontal cortex (lPFC) store WM information via persistent activity. In this review, we critically evaluate recent findings that together indicate that this model of WM needs revision. We argue that sensory cortex, not the lPFC, maintains high-fidelity representations of WM content. By contrast, the lPFC simultaneously maintains representations of multiple goal-related variables that serve to bias stimulus-specific activity in sensory regions. This work highlights multiple neural mechanisms supporting WM, including temporally dynamic population coding in addition to persistent activity. These new insights focus the question on understanding how the mechanisms that underlie WM are related, interact, and are coordinated in the lPFC and sensory cortex.
Visual attention is used to selectively filter relevant information depending on current task demands and goals. Visual attention is called object-based attention when it is directed to coherent forms or objects in the visual field. This study used real-time functional magnetic resonance imaging for moment-to-moment decoding of attention to spatially overlapped objects belonging to two different object categories. First, a whole-brain classifier was trained on pictures of faces and places. Subjects then saw transparently overlapped pictures of a face and a place, and attended to only one of them while ignoring the other. The category of the attended object, face or place, was decoded on a scan-by-scan basis using the previously trained decoder. The decoder performed at 77.6% accuracy indicating that despite competing bottom-up sensory input, object-based visual attention biased neural patterns towards that of the attended object. Furthermore, a comparison between different classification approaches indicated that the representation of faces and places is distributed rather than focal. This implies that real-time decoding of object-based attention requires a multivariate decoding approach that can detect these distributed patterns of cortical activity.
Estimating the size of a space and its degree of clutter are effortless and ubiquitous tasks of moving agents in a natural environment. Here, we examine how regions along the occipital-temporal lobe respond to pictures of indoor real-world scenes that parametrically vary in their physical "size" (the spatial extent of a space bounded by walls) and functional "clutter" (the organization and quantity of objects that fill up the space). Using a linear regression model on multivoxel pattern activity across regions of interest, we find evidence that both properties of size and clutter are represented in the patterns of parahippocampal cortex, while the retrosplenial cortex activity patterns are predominantly sensitive to the size of a space, rather than the degree of clutter. Parametric whole-brain analyses confirmed these results. Importantly, this size and clutter information was represented in a way that generalized across different semantic categories. These data provide support for a property-based representation of spaces, distributed across multiple scene-selective regions of the cerebral cortex.
We are endlessly fascinated by memory; we desire to improve it and fear its loss. While it has long been recognized that brain regions such as the hippocampus are vital for supporting memories of our past experiences (autobiographical memories), we still lack fundamental knowledge about the mechanisms involved. This is because the study of specific neural signatures of autobiographical memories in vivo in humans presents a significant challenge. However, recent developments in high-resolution structural and functional magnetic resonance imaging coupled with advanced analytical methods now permit access to the neural substrates of memory representations that has hitherto been precluded in humans. Here, I describe how the application of 'decoding' techniques to brain-imaging data is beginning to disclose how individual autobiographical memory representations evolve over time, deepening our understanding of systems-level consolidation. In particular, this prompts new questions about the roles of the hippocampus and ventromedial prefrontal cortex and offers new opportunities to interrogate the elusive memory trace that has for so long confounded neuroscientists.
While non-verbal affective voice cues are generally recognized as a crucial behavioral guide in any day-to-day conversation their role as a powerful source of information may extend well beyond close-up personal interactions and include other modes of communication such as written discourse or literature as well. Building on the assumption that similarities between the different 'modes' of voice cues may not only be limited to their functional role but may also include cerebral mechanisms engaged in the decoding process, the present functional magnetic resonance imaging study aimed at exploring brain responses associated with processing emotional voice signals described in literary texts. Emphasis was placed on evaluating 'voice' sensitive as well as task- and emotion-related modulations of brain activation frequently associated with the decoding of acoustic vocal cues. Obtained findings suggest that several similarities emerge with respect to the perception of acoustic voice signals: results identify the superior temporal, lateral and medial frontal cortex as well as the posterior cingulate cortex and cerebellum to contribute to the decoding process, with similarities to acoustic voice perception reflected in a 'voice'-cue preference of temporal voice areas as well as an emotion-related modulation of the medial frontal cortex and a task-modulated response of the lateral frontal cortex.
The predominant neurobiological model of working memory (WM) posits that stimulus information is stored via stable, elevated activity within highly selective neurons. On the basis of this model, which we refer to as the canonical model, the storage of stimulus information is largely associated with lateral PFC (lPFC). A growing number of studies describe results that cannot be fully explained by the canonical model, suggesting that it is in need of revision. In this study, we directly tested key elements of the canonical model. We analyzed fMRI data collected as participants performed a task requiring WM for faces and scenes. Multivariate decoding procedures identified patterns of activity containing information about the items maintained in WM (faces, scenes, or both). Although information about WM items was identified in extrastriate visual cortex (EC) and lPFC, only EC exhibited a pattern of results consistent with a sensory representation. Information in both regions persisted even in the absence of elevated activity, suggesting that elevated population activity may not represent the storage of information in WM. Additionally, we observed that WM information was distributed across EC neural populations that exhibited a broad range of selectivity for the WM items rather than restricted to highly selective EC populations. Finally, we determined that activity patterns coding for WM information were not stable, but instead varied over the course of a trial, indicating that the neural code for WM information is dynamic rather than static. Together, these findings challenge the canonical model of WM.
Humans naturally have a sense of humor. Experiencing humor not only encourages social interactions, but also produces positive physiological effects on the human body, such as lowering blood pressure. Recent neuro-imaging studies have shown evidence for distinct mental state changes at work in people experiencing humor. However, the temporal characteristics of these changes remain elusive. In this paper, we objectively measured humor-related mental states from single-trial functional magnetic resonance imaging (fMRI) data obtained while subjects viewed comedy TV programs. Measured fMRI data were labeled on the basis of the lag before or after the viewer's perception of humor (humor onset) determined by the viewer-reported humor experiences during the fMRI scans. We trained multiple binary classifiers, or decoders, to distinguish between fMRI data obtained at each lag from ones obtained during a neutral state in which subjects were not experiencing humor. As a result, in the right dorsolateral prefrontal cortex and the right temporal area, the decoders showed significant classification accuracies even at two seconds ahead of the humor onsets. Furthermore, given a time series of fMRI data obtained during movie viewing, we found that the decoders with significant performance were also able to predict the upcoming humor events on a volume-by-volume basis. Taking into account the hemodynamic delay, our results suggest that the upcoming humor events are encoded in specific brain areas up to about five seconds before the awareness of experiencing humor. Our results provide evidence that there exists a mental state lasting for a few seconds before actual humor perception, as if a viewer is expecting the future humorous events.
Neuroimaging studies in the last 20 years have tried to unravel the neural correlates of number processing across formats in humans and non-human primates. Results point to the intraparietal sulcus as the core area for an abstract representation of numerical quantity. On the other hand, there exist a variety of behavioral and neuroimaging data that are difficult to reconcile with the existence of such an abstract representation. In this study, we addressed this issue by applying multi-voxel pattern analysis (MVPA) to functional Magnetic Resonance Imaging (fMRI) data to unravel the neural representations of symbolic (digits) and non-symbolic (dots) numbers and their possible overlap on three different spatial scales (entire lobules, smaller regions of interest and a searchlight analysis with 2-voxel radius). Results showed that numbers in both formats are decodable in occipital, frontal, temporal and parietal regions. However, there were no overlapping representations between dots and digits on any of the spatial scales. These data suggest that the human brain does not contain an abstract representation of numerical magnitude.
The influence of external factors on food preferences and choices is poorly understood. Knowing which and how food-external cues impact the sensory processing and cognitive valuation of food would provide a strong benefit toward a more integrative understanding of food intake behavior and potential means of interfering with deviant eating patterns to avoid detrimental health consequences for individuals in the long run. We investigated whether written labels with positive and negative (as opposed to 'neutral') valence differentially modulate the spatio-temporal brain dynamics in response to the subsequent viewing of high- and low-energetic food images. Electrical neuroimaging analyses were applied to visual evoked potentials (VEPs) from 20 normal-weight participants. VEPs and source estimations in response to high- and low- energy foods were differentially affected by the valence of preceding word labels over the ~260-300 ms post-stimulus period. These effects were only observed when high-energy foods were preceded by labels with positive valence. Neural sources in occipital as well as posterior, frontal, insular and cingulate regions were down-regulated. These findings favor cognitive-affective influences especially on the visual responses to high-energetic food cues, potentially indicating decreases in cognitive control and goal-adaptive behavior. Inverse correlations between insular activity and effectiveness in food classification further indicate that this down-regulation directly impacts food-related behavior.
Successful realization of planned actions requires the brain to encode intentions over delays. Previous research has indicated that several regions in the rostral or anterior prefrontal cortex (PFC) encode delayed intentions. However, different processes may encode the same future task depending on task load during the delay. This difference may depend on the computational resources available when the delay is occupied with an ongoing task and when it is task-free. Here we directly investigated and compared the representation of delayed intentions in the human brain in the presence and absence of ongoing task load during the delay. We acquired fMRI data in combination with an event-based prospective memory design where human subjects remembered to perform the same future tasks over occupied and task-free delays. We used time-resolved multivoxel pattern classification and found that: (1) rostrolateral PFC (BA 46) encoded the delayed intention during both delay types; (2) rostromedial PFC (BA 10) encoded the intentions during occupied delays; whereas (3) a variety of more posterior regions, including the anterior cingulate cortex (BA 24), the supplementary motor area (BA 6), and the precuneus, encoded intentions during task-free delays. Overall, the medial PFC encoded delayed intentions more rostrally in the presence of an ongoing delay task and more caudally in its absence. Thus, rostromedial PFC may play a specialized role in the encoding of prospective memory that depends on higher computational demands (e.g., given higher task load during the delay). In contrast, the rostrolateral PFC is a more general area that encodes future intentions regardless of task load.
Most studies of conceptual knowledge in the brain focus on a narrow range of concrete conceptual categories, rely on the researchers' intuitions about which object belongs to these categories, and assume a broadly taxonomic organization of knowledge. In this fMRI study, we focus on concepts with a variety of concreteness levels; we use a state of the art lexical resource (WordNet 3.1) as the source for a relatively large number of category distinctions and compare a taxonomic style of organization with a domain-based model (an example domain is Law). Participants mentally simulated situations associated with concepts when cued by text stimuli. Using multivariate pattern analysis, we find evidence that all Taxonomic categories and Domains can be distinguished from fMRI data and also observe a clear concreteness effect: Tools and Locations can be reliably predicted for unseen participants, but less concrete categories (e.g., Attributes, Communications, Events, Social Roles) can only be reliably discriminated within participants. A second concreteness effect relates to the interaction of Domain and Taxonomic category membership: Domain (e.g., relation to Law vs. Music) can be better predicted for less concrete categories. We repeated the analysis within anatomical regions, observing discrimination between all/most categories in the left mid occipital and left mid temporal gyri, and more specialized discrimination for concrete categories Tool and Location in the left precentral and fusiform gyri, respectively. Highly concrete/abstract Taxonomic categories and Domain were segregated in frontal regions. We conclude that both Taxonomic and Domain class distinctions are relevant for interpreting neural structuring of concrete and abstract concepts.
A current view proposes that the right inferior frontal cortex (IFC) is particularly responsible for attentive decoding and cognitive evaluation of emotional cues in human vocalizations. Although some studies seem to support this view, an exhaustive review of all recent imaging studies points to an important functional role of both the right and the left IFC in processing vocal emotions. Second, besides a supposed predominant role of the IFC for an attentive processing and evaluation of emotional voices in IFC, these recent studies also point to a possible role of the IFC in preattentive and implicit processing of vocal emotions. The studies specifically provide evidence that both the right and the left IFC show a similar anterior-to-posterior gradient of functional activity in response to emotional vocalizations. This bilateral IFC gradient depends both on the nature or medium of emotional vocalizations (emotional prosody versus nonverbal expressions) and on the level of attentive processing (explicit versus implicit processing), closely resembling the distribution of terminal regions of distinct auditory pathways, which provide either global or dynamic acoustic information. Here we suggest a functional distribution in which several IFC subregions process different acoustic information conveyed by emotional vocalizations. Although the rostro-ventral IFC might categorize emotional vocalizations, the caudo-dorsal IFC might be specifically sensitive to their temporal features.
Macroanatomical right-left hemispheric differences in the brain are termed asymmetries, although there is no clear information on the global influence of gender and brain-regions. The aim of this study was to evaluate the main effects and interactions of these variables on the measurement of volumetric asymmetry indices (VAIs).
Using multivoxel pattern analysis (MVPA), we studied how distributed visual representations in human occipitotemporal cortex are modulated by attention and link their modulation to concurrent activity in frontal and parietal cortex. We detected similar occipitotemporal patterns during a simple visuoperceptual task and an attention-to-working-memory task in which one or two stimuli were cued before being presented among other pictures. Pattern strength varied from highest to lowest when the stimulus was the exclusive focus of attention, a conjoint focus, and when it was potentially distracting. Although qualitatively similar effects were seen inside regions relatively specialized for the stimulus category and outside, the former were quantitatively stronger. By regressing occipitotemporal pattern strength against activity elsewhere in the brain, we identified frontal and parietal areas exerting top-down control over, or reading information out from, distributed patterns in occipitotemporal cortex. Their interactions with patterns inside regions relatively specialized for that stimulus category were higher than those with patterns outside those regions and varied in strength as a function of the attentional condition. One area, the frontal operculum, was distinguished by selectively interacting with occipitotemporal patterns only when they were the focus of attention. There was no evidence that any frontal or parietal area actively inhibited occipitotemporal representations even when they should be ignored and were suppressed. Using MVPA to decode information within these frontal and parietal areas showed that they contained information about attentional context and/or readout information from occipitotemporal cortex to guide behavior but that frontal regions lacked information about category identity.
Neurons, even in the earliest sensory areas of cortex, are subject to a great deal of contextual influence from both within and across modality connections. In the present work, we investigated whether the earliest regions of somatosensory cortex (S1 and S2) would contain content-specific information about visual object categories. We reasoned that this might be possible due to the associations formed through experience that link different sensory aspects of a given object. Participants were presented with visual images of different object categories in 2 fMRI experiments. Multivariate pattern analysis revealed reliable decoding of familiar visual object category in bilateral S1 (i.e., postcentral gyri) and right S2. We further show that this decoding is observed for familiar but not unfamiliar visual objects in S1. In addition, whole-brain searchlight decoding analyses revealed several areas in the parietal lobe that could mediate the observed context effects between vision and somatosensation. These results demonstrate that even the first cortical stages of somatosensory processing carry information about the category of visually presented familiar objects.
Pain is a multidimensional experience emerging from the flow of information between multiple brain regions. A growing body of evidence suggests that pathological pain causes plastic changes of various brain regions. Here, we hypothesized that the induction of neuropathic pain alters distributed patterns of the resting-state brain activity in animal models, and capturing the altered pattern would enable identification of neuropathic pain at the individual level. We acquired micro-positron emission tomography with [(18)F]fluorodeoxyglucose (FDG micro-PET) images in awake rats with spinal nerve ligation (SNL) and without (sham) (SNL group, n=13; sham group, n=10). Multivariate pattern analysis (MVPA) with linear support vector machine (SVM) successfully identified the brain with SNL (92.31% sensitivity, 90.00% specificity, and 91.30% total accuracy). Predictive brain regions with increased metabolism were mainly located in prefrontal-limbic-brainstem areas including the anterior olfactory nucleus (AON), insular cortex (IC), piriform cortex (PC), septal area (SA), basal forebrain/preoptic area (BF/POA), amygdala (AMY), hypothalamus (HT), rostral ventromedial medulla (RVM) and the ventral midbrain (VMB). In contrast, predictive regions with decreased metabolism were observed in widespread cortical areas including secondary somatosensory cortex (S2), occipital cortex (OC), temporal cortex (TC), retrosplenial cortex (RSC), and the cerebellum (CBL). We also applied the univariate approach and obtained reduced prediction performance compared to MVPA. Our results suggest that developing neuroimaging-based diagnostic tools for pathological pain can be achieved by considering patterns of the resting-state brain activity.
Functional near infrared spectroscopy (fNIRS) is rapidly gaining interest in both the Neuroscience, as well as the Brain-Computer-Interface (BCI) community. Despite these efforts, most single-trial analysis of fNIRS data is focused on motor-imagery, or mental arithmetics. In this study, we investigate the suitability of different mental tasks, namely mental arithmetics, word generation and mental rotation for fNIRS based BCIs. We provide the first systematic comparison of classification accuracies achieved in a sample study. Data was collected from 10 subjects performing these three tasks.
Remembering a past event involves reactivation of distributed patterns of neural activity that represent the features of that event-a process that depends on associative mechanisms supported by medial temporal lobe structures. Although efficient use of memory requires prioritizing those features of a memory that are relevant to current behavioral goals (target features) over features that may be goal-irrelevant (incidental features), there remains ambiguity concerning how this is achieved. We tested the hypothesis that although medial temporal lobe structures may support reactivation of both target and incidental event features, frontoparietal cortex preferentially reactivates those features that match current goals. Here, human participants were cued to remember either the category (face/scene) to which a picture belonged (category trials) or the location (left/right) in which a picture appeared (location trials). Multivoxel pattern analysis of fMRI data were used to measure reactivation of category information as a function of its behavioral relevance (target vs incidental reactivation). In ventral/medial temporal lobe (VMTL) structures, incidental reactivation was as robust as target reactivation. In contrast, frontoparietal cortex exhibited stronger target than incidental reactivation; that is, goal-modulated reactivation. Reactivation was also associated with later memory. Frontoparietal biases toward target reactivation predicted subsequent memory for target features, whereas incidental reactivation in VMTL predicted subsequent memory for nontested features. These findings reveal a striking dissociation between goal-modulated reactivation in frontoparietal cortex and incidental reactivation in VMTL.
The neural mechanism underlying preparation for tasks that vary in difficulty has not been explored. This functional magnetic resonance imaging study manipulated task difficulty by varying the working memory (WM) load of the n-back task. Each n-back task block was preceded by a preparation period involving a screen that indicated the level of difficulty of the upcoming task. Consistent with previous work, activation in some brain regions depended on WM load in the task. These regions were used as regions of interest for the univariate and multivariate (classification) analyses of preparation periods. The findings were that the patterns of brain activation during task preparation contain information about the upcoming task difficulty. (1) A support vector machine classifier was able to decode the n-back task difficulty from the patterns of brain activation during task preparation. Those individuals whose activation patterns for anticipated 1- versus 2- versus 3-back conditions were classified with higher accuracy showed better behavioral performance on the task, suggesting that task performance depends on task preparation. (2) Left inferior frontal gyrus, intraparietal sulcus, and anterior cingulate cortex parametrically decreased activation as anticipated task difficulty increased. Taken together, these results suggest dynamic involvement of the WM network not only during WM task performance, but also during task preparation.
Since its development, the multivoxel pattern analysis (MVPA) method has been widely used to study high-level cognitive function in the brain. The results of the MVPA indicate that the spatial pattern of functional MRI data contains useful information. In addition to the spatial pattern analysis of the brain functions, effective connectivity can also be analyzed between the spatial pattern-based information. In this article, we propose a multivoxel pattern-based causality mapping method to explore influences between the spatial pattern-based information in the brain. The method applies the Granger causality to interested regions of the brain in terms of spatiotemporal pattern-based data, which are known to play an important role in dealing with high-level functions of the brain. The method can compose a causality map throughout the entire brain for any specified region of interest. Both simulations and experiments were performed to show the performance of the proposed method, and the existence and analyzability of the connectivity between pattern-based information in the brain were verified.
The conscious manipulation of mental representations is central to many creative and uniquely human abilities. How does the human brain mediate such flexible mental operations? Here, multivariate pattern analysis of functional MRI data reveals a widespread neural network that performs specific mental manipulations on the contents of visual imagery. Evolving patterns of neural activity within this mental workspace track the sequence of informational transformations carried out by these manipulations. The network switches between distinct connectivity profiles as representations are maintained or manipulated.
In recent decades, a lot of achievements have been obtained in imaging and cognitive neuroscience of human brain. Brain's activities can be shown by a number of different kinds of non-invasive technologies, such as: Near-Infrared Spectroscopy (NIRS), Magnetic Resonance Imaging (MRI), and ElectroEncephaloGraphy (EEG; Wolpaw et al., 2002; Weiskopf et al., 2004; Blankertz et al., 2006). NIRS has become the convenient technology for experimental brain purposes. The change of oxygenation changes (oxy-Hb) along task period depending on location of channel on the cortex has been studied: sustained activation in the motor cortex, transient activation during the initial segments in the somatosensory cortex, and accumulating activation in the frontal lobe (Gentili et al., 2010). Oxy-Hb concentration at the aforementioned sites in the brain can also be used as a predictive factor allows prediction of subject's investigation behavior with a considerable degree of precision (Shimokawa et al., 2009). In this paper, a study of recognition algorithm will be described for recognition whether one taps the left hand (LH) or the right hand (RH). Data with noises and artifacts collected from a multi-channel system will be pre-processed using a Savitzky-Golay filter for getting more smoothly data. Characteristics of the filtered signals during LH and RH tapping process will be extracted using a polynomial regression (PR) algorithm. Coefficients of the polynomial, which correspond to Oxygen-Hemoglobin (Oxy-Hb) concentration, will be applied for the recognition models of hand tapping. Support Vector Machines (SVM) will be applied to validate the obtained coefficient data for hand tapping recognition. In addition, for the objective of comparison, Artificial Neural Networks (ANNs) was also applied to recognize hand tapping side with the same principle. Experimental results have been done many trials on three subjects to illustrate the effectiveness of the proposed method.
Repeated exposure to a visual stimulus is associated with corresponding reductions in neural activity, particularly within visual cortical areas. It has been argued that this phenomenon of repetition suppression is related to increases in processing fluency or implicit memory. However, repetition of a visual stimulus can also be considered in terms of the similarity of the pattern of neural activity elicited at each exposure--a measure that has recently been linked to explicit memory. Despite the popularity of each of these measures, direct comparisons between the two have been limited, and the extent to which they differentially (or similarly) relate to behavioral measures of memory has not been clearly established. In the present study, we compared repetition suppression and pattern similarity as predictors of both implicit and explicit memory. Using functional magnetic resonance imaging, we scanned 20 participants while they viewed and categorized repeated presentations of scenes. Repetition priming (facilitated categorization across repetitions) was used as a measure of implicit memory, and subsequent scene recognition was used as a measure of explicit memory. We found that repetition priming was predicted by repetition suppression in prefrontal, parietal, and occipitotemporal regions; however, repetition priming was not predicted by pattern similarity. In contrast, subsequent explicit memory was predicted by pattern similarity (across repetitions) in some of the same occipitotemporal regions that exhibited a relationship between priming and repetition suppression; however, explicit memory was not related to repetition suppression. This striking double dissociation indicates that repetition suppression and pattern similarity differentially track implicit and explicit learning.
Human referential communication is often thought as coding-decoding a set of symbols, neglecting that establishing shared meanings requires a computational mechanism powerful enough to mutually negotiate them. Sharing the meaning of a novel symbol might rely on similar conceptual inferences across communicators or on statistical similarities in their sensorimotor behaviors. Using magnetoencephalography, we assess spectral, temporal, and spatial characteristics of neural activity evoked when people generate and understand novel shared symbols during live communicative interactions. Solving those communicative problems induced comparable changes in the spectral profile of neural activity of both communicators and addressees. This shared neuronal up-regulation was spatially localized to the right temporal lobe and the ventromedial prefrontal cortex and emerged already before the occurrence of a specific communicative problem. Communicative innovation relies on neuronal computations that are shared across generating and understanding novel shared symbols, operating over temporal scales independent from transient sensorimotor behavior.
When faced with ambiguous sensory inputs, subjective perception alternates between the different interpretations in a stochastic manner. Such multistable perception phenomena have intrigued scientists and laymen alike for over a century. Despite rigorous investigations, the underlying mechanisms of multistable perception remain elusive. Recent studies using multivariate pattern analysis revealed that activity patterns in posterior visual areas correlate with fluctuating percepts. However, increasing evidence suggests that vision--and perception at large--is an active inferential process involving hierarchical brain systems. We applied searchlight multivariate pattern analysis to functional magnetic resonance imaging signals across the human brain to decode perceptual content during bistable perception and simple unambiguous perception. Although perceptually reflective activity patterns during simple perception localized predominantly to posterior visual regions, bistable perception involved additionally many higher-order frontoparietal and temporal regions. Moreover, compared with simple perception, both top-down and bottom-up influences were dramatically enhanced during bistable perception. We further studied the intermittent presentation of ambiguous images--a condition that is known to elicit perceptual memory. Compared with continuous presentation, intermittent presentation recruited even more higher-order regions and was accompanied by further strengthened top-down influences but relatively weakened bottom-up influences. Taken together, these results strongly support an active top-down inferential process in perception.
Premature birth may be associated with white matter injury later developing with widening of the ventricles. However, population-based data on normal ventricular size by age are sparse, making the evaluation of possible ventricular dilatation difficult.
As a relatively recent cultural invention in human evolution, reading is an important gateway to personal development and socioeconomic success. Despite the well documented individual differences in reading ability, its neuroanatomical correlates have not been well understood, largely due to the fact that reading is a complex skill that consists of multiple components. Using a large sample of 416 college students and 7 reading tasks, the present study successfully identified three uncorrelated components of reading ability: phonological decoding, form-sound association, and naming speed. We then tried to predict individuals' scores in these components from their gray matter volume (GMV) on a subset of participants (N = 253) with high-quality structural images, adopting a multivariate support vector regression analysis with tenfold cross-validation. Our results revealed distinct neural regions that supported different aspects of reading ability: whereas phonological decoding was associated with the GMV in the left superior parietal lobe extending to the supramarginal gyrus, form-sound association was predicted by the GMV in the hippocampus and cerebellum. Naming speed was associated with GMV in distributed brain regions in the occipital, temporal, parietal, and frontal cortices. Phonological decoding and form-sound association were uncorrelated with general cognitive abilities. However, naming speed was correlated with intelligence and processing speed, and some of the regions that were predictive of naming speed also predicted these general cognitive abilities. These results provide further insights on the cognitive and neural architecture of reading and the structural basis of individual differences in reading abilities.
Brain-computer interfaces (BCIs) can convert mental states into signals to drive real-world devices, but it is not known if a given covert task is the same when performed with and without BCI-based control. Using a BCI likely involves additional cognitive processes, such as multitasking, attention, and conflict monitoring. In addition, it is challenging to measure the quality of covert task performance. We used whole-brain classifier-based real-time functional MRI to address these issues, because the method provides both classifier-based maps to examine the neural requirements of BCI and classification accuracy to quantify the quality of task performance. Subjects performed a covert counting task at fast and slow rates to control a visual interface. Compared with the same task when viewing but not controlling the interface, we observed that being in control of a BCI improved task classification of fast and slow counting states. Additional BCI control increased subjects' whole-brain signal-to-noise ratio compared with the absence of control. The neural pattern for control consisted of a positive network comprised of dorsal parietal and frontal regions and the anterior insula of the right hemisphere as well as an expansive negative network of regions. These findings suggest that real-time functional MRI can serve as a platform for exploring information processing and frontoparietal and insula network-based regulation of whole-brain task signal-to-noise ratio.
Behavior is governed by rules that associate stimuli with responses and outcomes. Human and monkey studies have shown that rule-specific information is widely represented in the frontoparietal cortex. However, it is not known how establishing a rule under different contexts affects its neural representation. Here, we use event-related functional MRI (fMRI) and multivoxel pattern classification methods to investigate the human brain's mechanisms of establishing and maintaining rules for multiple perceptual decision tasks. Rules were either chosen by participants or specifically instructed to them, and the fMRI activation patterns representing rule-specific information were compared between these contexts. We show that frontoparietal regions differ in the properties of their rule representations during active maintenance before execution. First, rule-specific information maintained in the dorsolateral and medial frontal cortex depends on the context in which it was established (chosen vs specified). Second, rule representations maintained in the ventrolateral frontal and parietal cortex are independent of the context in which they were established. Furthermore, we found that the rule-specific coding maintained in anticipation of stimuli may change with execution of the rule: representations in context-independent regions remain invariant from maintenance to execution stages, whereas rule representations in context-dependent regions do not generalize to execution stage. The identification of distinct frontoparietal systems with context-independent and context-dependent task rule representations, and the distinction between anticipatory and executive rule representations, provide new insights into the functional architecture of goal-directed behavior.
Consensus in the most recent literature indicates that psychoactive "bath salts" is a relatively new drug-combination that was added to Schedule I classification in October 2011. Common ingredients include the cathinone analogs: mephedrone and methylenedioxypyrovalerone (MDPV). The mechanism of action of these synthetic cathinone analogs has not yet been well studied. We propose an intensive systematic investigation to determine the potential for cathinones to produce neurotoxic effects in various brain regions. In spite of a lack of evidence, for neurotoxicity there are number of horrific cases now on record that suggest intensification of research is needed. For example, a suicide by hanging had high 3,4-MDPV concentration while a driver under the influence had the highest reported methylone (MEPH) concentration. More interestingly, there have been consistent case reports indicating delayed responses, including: severe agitation with possible psychosis, suicidal ideation, rhabdomyolysis, hypertension, tachycardia, and death. In animal studies, amphetamine (AMPH), methamphetamine (METH) and cocaine release dopamine (DA), similarly to the action of cathinone and particular cathinone analogues. Two components of bath salts, MEPH and MDPV produce opposite effects at human dopamine transporter (hDAT) comparable to METH and cocaine, respectively. Moreover, it has already been found by others that MEPH is almost as potent as METH; and MDPV is much more potent than cocaine with longer lasting effects. It has been conjectured correctly that bath salts containing MDPV and MEPH (or a similar drug) might be expected both, to initially release DA and subsequently prevent its reuptake via hDAT. The null hypothesis, that cathinones do not cause neurotoxicity to dopamine nerve endings of the striatum, seems parsimonious and requires intensive investigation. Our hypothesis is that when consumed by humans, cathinones may induce neurotoxic pathways involving the neuro-glial-microglia and/or specific inflammation, that may help explain the clinically observed delayed response. We intend to explore this hypothesis utilizing a novel proteomic and biomarker technique developed by scientists at the McKnight Brain Institute, University of Florida as well as magnetic-resonance imaging across pre-frontal orbital cortex-cingulate gyrus and mesolimbic pathways of the brain of rodents.
Information enters the cortex via modality-specific sensory regions, whereas actions are produced by modality-specific motor regions. Intervening central stages of information processing map sensation to behavior. Humans perform this central processing in a flexible, abstract manner such that sensory information in any modality can lead to response via any motor system. Cognitive theories account for such flexible behavior by positing amodal central information processing (e.g., "central executive," Baddeley and Hitch, 1974; "supervisory attentional system," Norman and Shallice, 1986; "response selection bottleneck," Pashler, 1994). However, the extent to which brain regions embodying central mechanisms of information processing are amodal remains unclear. Here we apply multivariate pattern analysis to functional magnetic resonance imaging (fMRI) data to compare response selection, a cognitive process widely believed to recruit an amodal central resource across sensory and motor modalities. We show that most frontal and parietal cortical areas known to activate across a wide variety of tasks code modality, casting doubt on the notion that these regions embody a central processor devoid of modality representation. Importantly, regions of anterior insula and dorsolateral prefrontal cortex consistently failed to code modality across four experiments. However, these areas code at least one other task dimension, process (instantiated as response selection vs response execution), ensuring that failure to find coding of modality is not driven by insensitivity of multivariate pattern analysis in these regions. We conclude that abstract encoding of information modality is primarily a property of subregions of the prefrontal cortex.
To determine the specific contribution of brain regions to working memory, human participants performed two distinct tasks on the same visually presented objects. During the maintenance of visual properties, object identity could be decoded from extrastriate, but not prefrontal, cortex, whereas the opposite held for nonvisual properties. Thus, the ability to maintain information during working memory is a general and flexible cortical property, with the role of individual regions being goal-dependent.
To what extent do the brain regions implicated in semantic processing contribute to the representation of amodal conceptual content rather than modality-specific mechanisms or mechanisms of semantic access and manipulation? Here, we propose that a brain region can be considered to represent amodal conceptual object knowledge if it is supramodal and plays a role in distinguishing among the conceptual representations of different objects. In an fMRI study, human participants made category typicality judgments about pictured objects or their names drawn from five different categories. Crossmodal multivariate pattern analysis revealed a network of six left-lateralized regions largely outside of category-selective visual cortex that showed a supramodal representation of object categories. These were located in the posterior middle/inferior temporal gyrus (pMTG/ITG), angular gyrus, ventral temporal cortex, posterior cingulate/precuneus (PC), and lateral and dorsomedial prefrontal cortex. Representational similarity analysis within these regions determined that the similarity between category-specific patterns of neural activity in the pMTG/ITG and the PC was consistent with the semantic similarity between these categories. This finding supports the PC and pMTG/ITG as candidate regions for the amodal representation of the conceptual properties of objects.
Understanding emotions in others engages specific brain regions in temporal and medial prefrontal cortices. These activations are often attributed to more general cognitive 'mentalizing' functions, associated with theory of mind and also necessary to represent people's non-emotional mental states, such as beliefs or intentions. Here, we directly investigated whether understanding emotional feelings recruit similar or specific brain systems, relative to other non-emotional mental states. We used functional magnetic resonance imaging with multivoxel pattern analysis in 46 volunteers to compare activation patterns in theory-of-mind tasks for emotions, relative to beliefs or somatic states accompanied with pain. We found a striking dissociation between the temporoparietal cortex, that exhibited a remarkable voxel-by-voxel pattern overlap between emotions and beliefs (but not pain), and the dorsomedial prefrontal cortex, that exhibited distinct (and yet nearby) patterns of activity during the judgment of beliefs and emotions in others. Pain judgment was instead associated with activity in the supramarginal gyrus, middle cingulate cortex and middle insular cortex. Our data reveal for the first time a functional dissociation within brain networks sub-serving theory of mind for different mental contents, with a common recruitment for cognitive and affective states in temporal regions, and distinct recruitment in prefrontal areas.
While the human medial prefrontal cortex (mPFC) is widely believed to be a key node of neural networks relevant for socio-emotional processing, its functional subspecialization is still poorly understood. We thus revisited the often assumed differentiation of the mPFC in social cognition along its ventral-dorsal axis. Our neuroinformatic analysis was based on a neuroimaging meta-analysis of perspective-taking that yielded two separate clusters in the ventral and dorsal mPFC, respectively. We determined each seed region's brain-wide interaction pattern by two complementary measures of functional connectivity: co-activation across a wide range of neuroimaging studies archived in the BrainMap database and correlated signal fluctuations during unconstrained ("resting") cognition. Furthermore, we characterized the functions associated with these two regions using the BrainMap database. Across methods, the ventral mPFC was more strongly connected with the nucleus accumbens, hippocampus, posterior cingulate cortex, and retrosplenial cortex, while the dorsal mPFC was more strongly connected with the inferior frontal gyrus, temporo-parietal junction, and middle temporal gyrus. Further, the ventral mPFC was selectively associated with reward related tasks, while the dorsal mPFC was selectively associated with perspective-taking and episodic memory retrieval. The ventral mPFC is therefore predominantly involved in bottom-up-driven, approach/avoidance-modulating, and evaluation-related processing, whereas the dorsal mPFC is predominantly involved in top-down-driven, probabilistic-scene-informed, and metacognition-related processing in social cognition.
We examined word-level reading circuits in skilled deaf readers whose primary language is American Sign Language, and hearing readers matched for reading ability (college level). During fMRI scanning, participants performed a semantic decision (concrete concept?), a phonological decision (two syllables?), and a false-font control task (string underlined?). The groups performed equally well on the semantic task, but hearing readers performed better on the phonological task. Semantic processing engaged similar left frontotemporal language circuits in deaf and hearing readers. However, phonological processing elicited increased neural activity in deaf, relative to hearing readers, in the left precentral gyrus, suggesting greater reliance on articulatory phonological codes, and in bilateral parietal cortex, suggesting increased phonological processing effort. Deaf readers also showed stronger anterior-posterior functional segregation between semantic and phonological processes in left inferior prefrontal cortex. Finally, weaker phonological decoding ability did not alter activation in the visual word form area for deaf readers.
The notion of a frontoparietal human mirror neuron system (HMNS) has been used to explain a range of social phenomena. However, most human neuroimaging studies of this system do not address critical 'mirror' properties: neural representations should be action specific and should generalise across visual and motor modalities. Studies using repetition suppression (RS) and, particularly, multivariate pattern analysis (MVPA) highlight the contribution to action perception of anterior parietal regions. Further, these studies add to mounting evidence that suggests the lateral occipitotemporal cortex plays a role in the HMNS, but they offer less support for the involvement of the premotor cortex. Neuroimaging, particularly through application of MVPA, has the potential to reveal the properties of the HMNS in further detail, which could challenge prevailing views about its neuroanatomical organisation.
Sophisticated tool use is a defining characteristic of the primate species but how is it supported by the brain, particularly the human brain? Here we show, using functional MRI and pattern classification methods, that tool use is subserved by multiple distributed action-centred neural representations that are both shared with and distinct from those of the hand. In areas of frontoparietal cortex we found a common representation for planned hand- and tool-related actions. In contrast, in parietal and occipitotemporal regions implicated in hand actions and body perception we found that coding remained selectively linked to upcoming actions of the hand whereas in parietal and occipitotemporal regions implicated in tool-related processing the coding remained selectively linked to upcoming actions of the tool. The highly specialized and hierarchical nature of this coding suggests that hand- and tool-related actions are represented separately at earlier levels of sensorimotor processing before becoming integrated in frontoparietal cortex. DOI:http://dx.doi.org/10.7554/eLife.00425.001.
Laughter is an ancient signal of social communication among humans and non-human primates. Laughter types with complex social functions (e.g., taunt and joy) presumably evolved from the unequivocal and reflex-like social bonding signal of tickling laughter already present in non-human primates. Here, we investigated the modulations of cerebral connectivity associated with different laughter types as well as the effects of attention shifts between implicit and explicit processing of social information conveyed by laughter using functional magnetic resonance imaging (fMRI). Complex social laughter types and tickling laughter were found to modulate connectivity in two distinguishable but partially overlapping parts of the laughter perception network irrespective of task instructions. Connectivity changes, presumably related to the higher acoustic complexity of tickling laughter, occurred between areas in the prefrontal cortex and the auditory association cortex, potentially reflecting higher demands on acoustic analysis associated with increased information load on auditory attention, working memory, evaluation and response selection processes. In contrast, the higher degree of socio-relational information in complex social laughter types was linked to increases of connectivity between auditory association cortices, the right dorsolateral prefrontal cortex and brain areas associated with mentalizing as well as areas in the visual associative cortex. These modulations might reflect automatic analysis of acoustic features, attention direction to informative aspects of the laughter signal and the retention of those in working memory during evaluation processes. These processes may be associated with visual imagery supporting the formation of inferences on the intentions of our social counterparts. Here, the right dorsolateral precentral cortex appears as a network node potentially linking the functions of auditory and visual associative sensory cortices with those of the mentalizing-associated anterior mediofrontal cortex during the decoding of social information in laughter.
Research on the "emotional brain" remains centered around the idea that emotions like fear, happiness, and sadness result from specialized and distinct neural circuitry. Accumulating behavioral and physiological evidence suggests, instead, that emotions are grounded in core affect--a person's fluctuating level of pleasant or unpleasant arousal. A neuroimaging study revealed that participants' subjective ratings of valence (i.e., pleasure/displeasure) and of arousal evoked by various fear, happiness, and sadness experiences correlated with neural activity in specific brain regions (orbitofrontal cortex and amygdala, respectively). We observed these correlations across diverse instances within each emotion category, as well as across instances from all three categories. Consistent with a psychological construction approach to emotion, the results suggest that neural circuitry realizes more basic processes across discrete emotions. The implicated brain regions regulate the body to deal with the world, producing the affective changes at the core of emotions and many other psychological phenomena.
Dopaminergic medications, used to treat neurochemical pathology and resultant symptoms in neuropsychiatric disorders, are of mixed efficacy and regularly associated with behavioural side effects. The possibility that dopamine exerts both linear and nonlinear ('inverted U-shaped') effects on cognitive neurocircuitry may explain this outcome variability. However, it has proven to be difficult to characterise neural manifestations of psychopharmacological effects in humans. We hypothesised that diverse effects of dopamine neuromodulation could be characterised using systems-level neuroimaging approaches. Using 'resting-state' functional magnetic resonance imaging (FMRI), combined with dopaminergic challenges, we examined the dopamine-dependent functional connectivity of brain 'resting-state networks' (RSNs). We compared RSN connectivity in 3 groups of healthy volunteers given dopamine antagonist (haloperidol; N=18) or agonistic (levodopa; N=16) drugs, or a placebo (N=15). As RSNs have been shown to be relevant for numerous psychological functions and dysfunctions, we investigated both linear and nonlinear effects on RSN connectivity of manipulating dopamine neurotransmission pharmacologically. A basal ganglia RSN displayed both linear and nonlinear effects of dopamine manipulation on functional connectivity, respectively, with lateral frontoparietal and medial frontal neocortical areas. Conversely, a cognitive 'default mode' network showed only linear dopaminergic effects on connectivity with lateral frontal and parietal cortices. Our findings highlight diverse functional effects of dopamine neuromodulations on systems-level neural interactions. The observation that dopamine modulates distinct large-scale network connectivity patterns differentially, in both linear and nonlinear fashions, provides support for the objective utility of RSN metrics in classifying the effects and efficacy of psychopharmacological medications.
An anterior pathway, concerned with extracting meaning from sound, has been identified in nonhuman primates. An analogous pathway has been suggested in humans, but controversy exists concerning the degree of lateralization and the precise location where responses to intelligible speech emerge. We have demonstrated that the left anterior superior temporal sulcus (STS) responds preferentially to intelligible speech (Scott SK, Blank CC, Rosen S, Wise RJS. 2000. Identification of a pathway for intelligible speech in the left temporal lobe. Brain. 123:2400-2406.). A functional magnetic resonance imaging study in Cerebral Cortex used equivalent stimuli and univariate and multivariate analyses to argue for the greater importance of bilateral posterior when compared with the left anterior STS in responding to intelligible speech (Okada K, Rong F, Venezia J, Matchin W, Hsieh IH, Saberi K, Serences JT,Hickok G. 2010. Hierarchical organization of human auditory cortex: evidence from acoustic invariance in the response to intelligible speech. 20: 2486-2495.). Here, we also replicate our original study, demonstrating that the left anterior STS exhibits the strongest univariate response and, in decoding using the bilateral temporal cortex, contains the most informative voxels showing an increased response to intelligible speech. In contrast, in classifications using local "searchlights" and a whole brain analysis, we find greater classification accuracy in posterior rather than anterior temporal regions. Thus, we show that the precise nature of the multivariate analysis used will emphasize different response profiles associated with complex sound to speech processing.
Previously identified neural correlates of deception, such as the prefrontal, anterior cingulate, and parietal regions, have proven to be unreliable neural markers of deception, most likely because activity in these regions reflects executive processes that are not specific to deception. Herein, we report the first fMRI study that provides strong preliminary evidence that the neural activity associated with perception but not executive processes could offer a better marker of deception with regard to face familiarity. Using a face-recognition task, activity in the left precuneus during the perception of familiar faces accurately marked 11 of 13 subjects who lied about not knowing faces that were in fact familiar to them. This level of classification accuracy is much higher than the level predicted by chance and agrees with other findings by experts in lie detection.
Previously, multi-voxel pattern analysis has been used to decode words referring to concrete object categories. In this study we investigated if single-trial-based brain activity was sufficient to distinguish abstract (e.g., mercy) versus concrete (e.g., barn) concept representations. Multiple neuroimaging studies have identified differences in the processing of abstract versus concrete concepts based on the averaged activity across time by using univariate methods. In this study we used multi-voxel pattern analysis to decode functional magnetic resonance imaging (fMRI) data when participants perform a semantic similarity judgment task on triplets of either abstract or concrete words with similar meanings. Classifiers were trained to identify individual trials as concrete or abstract. Cross-validated accuracies for classifying trials as abstract or concrete were significantly above chance (P < 0.05) for all participants. Discriminating information was distributed in multiple brain regions. Moreover, accuracy of identifying single trial data for any one participant as abstract or concrete was also reliably above chance (P < 0.05) when the classifier was trained solely on data from other participants. These results suggest abstract and concrete concepts differ in representations in terms of neural activity patterns during a short period of time across the whole brain.
Multivariate pattern analysis (MVPA) is a relatively recent innovation in functional magnetic resonance imaging (fMRI) methods. MVPA is increasingly widely used, as it is apparently more effective than classical general linear model analysis (GLMA) for detecting response patterns or representations that are distributed at a fine spatial scale. However, we demonstrate that widely used approaches to MVPA can systematically admit certain confounds that are appropriately eliminated by GLMA. Thus confounds rather than distributed representations may explain some cases in which MVPA produced positive results but GLMA did not. The issue is that it is common practice in MVPA to conduct group tests on single-subject summary statistics that discard the sign or direction of underlying effects, whereas GLMA group tests are conducted directly on single-subject effects themselves. We describe how this common MVPA practice undermines standard experiment design logic that is intended to control at the group level for certain types of confounds, such as time on task and individual differences. Furthermore, we note that a simple application of linear regression can restore experimental control when using MVPA in many situations. Finally, we present a case study with novel fMRI data in the domain of rule representations, or flexible stimulus-response mappings, which has seen several recent MVPA publications. In our new dataset, as with recent reports, standard MVPA appears to reveal rule representations in prefrontal cortex regions, whereas GLMA produces null results. However, controlling for a variable that is confounded with rule at the individual-subject level but not the group level (reaction time differences across rules) eliminates the MVPA results. This raises the question of whether recently reported results truly reflect rule representations, or rather the effects of confounds such as reaction time, difficulty, or other variables of no interest.
Real-time fMRI is especially vulnerable to task-correlated movement artifacts because statistical methods normally available in conventional analyses to remove such signals cannot be used in the context of real-time fMRI. Multi-voxel classifier-based methods, although advantageous in many respects, are particularly sensitive. Here we systematically studied various movements of the head and face to determine to what extent these can "masquerade" as signal in multi-voxel classifiers.
Unconscious neural activity has been repeatedly shown to precede and potentially even influence subsequent free decisions. However, to date, such findings have been mostly restricted to simple motor choices, and despite considerable debate, there is no evidence that the outcome of more complex free decisions can be predicted from prior brain signals. Here, we show that the outcome of a free decision to either add or subtract numbers can already be decoded from neural activity in medial prefrontal and parietal cortex 4 s before the participant reports they are consciously making their choice. These choice-predictive signals co-occurred with the so-called default mode brain activity pattern that was still dominant at the time when the choice-predictive signals occurred. Our results suggest that unconscious preparation of free choices is not restricted to motor preparation. Instead, decisions at multiple scales of abstraction evolve from the dynamics of preceding brain activity.
The everyday act of speaking involves the complex processes of speech motor control. An important component of control is monitoring, detection, and processing of errors when auditory feedback does not correspond to the intended motor gesture. Here we show, using fMRI and converging operations within a multivoxel pattern analysis framework, that this sensorimotor process is supported by functionally differentiated brain networks. During scanning, a real-time speech-tracking system was used to deliver two acoustically different types of distorted auditory feedback or unaltered feedback while human participants were vocalizing monosyllabic words, and to present the same auditory stimuli while participants were passively listening. Whole-brain analysis of neural-pattern similarity revealed three functional networks that were differentially sensitive to distorted auditory feedback during vocalization, compared with during passive listening. One network of regions appears to encode an "error signal" regardless of acoustic features of the error: this network, including right angular gyrus, right supplementary motor area, and bilateral cerebellum, yielded consistent neural patterns across acoustically different, distorted feedback types, only during articulation (not during passive listening). In contrast, a frontotemporal network appears sensitive to the speech features of auditory stimuli during passive listening; this preference for speech features was diminished when the same stimuli were presented as auditory concomitants of vocalization. A third network, showing a distinct functional pattern from the other two, appears to capture aspects of both neural response profiles. Together, our findings suggest that auditory feedback processing during speech motor control may rely on multiple, interactive, functionally differentiated neural systems.
The behaviors of other people are often central to envisioning the future. The ability to accurately predict the thoughts and actions of others is essential for successful social interactions, with far-reaching consequences. Despite its importance, little is known about how the brain represents people in order to predict behavior. In this functional magnetic resonance imaging study, participants learned the unique personality of 4 protagonists and imagined how each would behave in different scenarios. The protagonists' personalities were composed of 2 traits: Agreeableness and Extraversion. Which protagonist was being imagined was accurately inferred based solely on activity patterns in the medial prefrontal cortex using multivariate pattern classification, providing novel evidence that brain activity can reveal whom someone is thinking about. Lateral temporal and posterior cingulate cortex discriminated between different degrees of agreeableness and extraversion, respectively. Functional connectivity analysis confirmed that regions associated with trait-processing and individual identities were functionally coupled. Activity during the imagination task, and revealed by functional connectivity, was consistent with the default network. Our results suggest that distinct regions code for personality traits, and that the brain combines these traits to represent individuals. The brain then uses this "personality model" to predict the behavior of others in novel situations.
Paraphilia is a set of disorders characterized by abnormal sexual desires. Perhaps most discussed amongst them, pedophilia is a complex interaction of disturbances of the emotional, cognitive and sexual experience. Using new imaging techniques such as functional magnetic resonance imaging, neural correlates of emotional, sexual and cognitive abnormalities and interactions have been investigated. As described on the basis of current research, altered patterns of brain activity, especially in the frontal areas of the brain, are seen in pedophilia. Building on these results, the analysis of neural correlates of impaired psychological functions opens the opportunity to further explore sexual deviances, which may contribute ultimately to the development of tools for risk assessment, classification methods and new therapeutic approaches.
The rationale, research literature, and proposed changes to the dissociative disorders and conversion disorder in the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) are presented. Dissociative identity disorder will include reference to possession as well as identity fragmentation, to make the disorder more applicable to culturally diverse situations. Dissociative amnesia will include dissociative fugue as a subtype, since fugue is a rare disorder that always involves amnesia but does not always include confused wandering or loss of personality identity. Depersonalization disorder will include derealization as well, since the two often co-occur. A dissociative subtype of posttraumatic stress disorder (PTSD), defined by the presence of depersonalization or derealization in addition to other PTSD symptoms, is being recommended, based upon new epidemiological and neuroimaging evidence linking it to an early life history of adversity and a combination of frontal activation and limbic inhibition. Conversion disorder (functional neurological symptom disorder) will likely remain with the somatic symptom disorders, despite considerable dissociative comorbidity.
Planning object-directed hand actions requires successful integration of the movement goal with the acting limb. Exactly where and how this sensorimotor integration occurs in the brain has been studied extensively with neurophysiological recordings in nonhuman primates, yet to date, because of limitations of non-invasive methodologies, the ability to examine the same types of planning-related signals in humans has been challenging. Here we show, using a multivoxel pattern analysis of functional MRI (fMRI) data, that the preparatory activity patterns in several frontoparietal brain regions can be used to predict both the limb used and hand action performed in an upcoming movement. Participants performed an event-related delayed movement task whereby they planned and executed grasp or reach actions with either their left or right hand toward a single target object. We found that, although the majority of frontoparietal areas represented hand actions (grasping vs reaching) for the contralateral limb, several areas additionally coded hand actions for the ipsilateral limb. Notable among these were subregions within the posterior parietal cortex (PPC), dorsal premotor cortex (PMd), ventral premotor cortex, dorsolateral prefrontal cortex, presupplementary motor area, and motor cortex, a region more traditionally implicated in contralateral movement generation. Additional analyses suggest that hand actions are represented independently of the intended limb in PPC and PMd. In addition to providing a unique mapping of limb-specific and action-dependent intention-related signals across the human cortical motor system, these findings uncover a much stronger representation of the ipsilateral limb than expected from previous fMRI findings.
We propose a new way to identify the neural correlates of memory load in a delayed match-to-sample saccade task with a constant perceptual load. Two conditions were compared with low and high memory loads. In the low-load condition, a rectangular shaped probe defined by its color and orientation was presented centrally. After a delay period, four stimuli were presented peripherally, one in each quadrant. The participants were instructed to saccade to the stimulus that matched the previously viewed sample on both color and orientation. In the high-load condition, the order of stimulus presentation was reversed: first four eccentric stimuli were presented and after a delay the central probe. In the high-load condition, the participant executed a saccade to the remembered location of the stimulus that matched the central probe in color and orientation. The behavioral results indicate that greater working memory load is associated with prolonged saccadic reaction times. A general linear model revealed regions in prefrontal cortex (left anterior insula, right superior and middle frontal gyrus, anterior medial cingulum), and bilaterally along the intraparietal sulcus extending into surrounding areas (precuneus, superior and inferior parietal lobe) that were more activated when participants had to conjointly remember the locations, colors and orientations of four objects (load-4) compared to when they only had to remember the features of a single object (load-1). Specific responses for greater working memory load are focused on regions responsible for feature binding (occipital-temporal cortex) and allocation of attention (anterior insular cortex). Multivariate pattern analysis during the retrieval period of a trial revealed voxel clusters in the ventral visual pathway and the frontal eye fields that correctly classify the target location during the retrieval period of both tasks.
Eating behavior is crucial in the development of obesity and Type 2 diabetes. To further investigate its regulation, we studied the effects of glucose versus water ingestion on the neural processing of visual high and low caloric food cues in 12 lean and 12 overweight subjects by functional magnetic resonance imaging. We found body weight to substantially impact the brain's response to visual food cues after glucose versus water ingestion. Specifically, there was a significant interaction between body weight, condition (water versus glucose), and caloric content of food cues. Although overweight subjects showed a generalized reduced response to food objects in the fusiform gyrus and precuneus, the lean group showed a differential pattern to high versus low caloric foods depending on glucose versus water ingestion. Furthermore, we observed plasma insulin and glucose associated effects. The hypothalamic response to high caloric food cues negatively correlated with changes in blood glucose 30 min after glucose ingestion, while especially brain regions in the prefrontal cortex showed a significant negative relationship with increases in plasma insulin 120 min after glucose ingestion. We conclude that the postprandial neural processing of food cues is highly influenced by body weight especially in visual areas, potentially altering visual attention to food. Furthermore, our results underline that insulin markedly influences prefrontal activity to high caloric food cues after a meal, indicating that postprandial hormones may be potential players in modulating executive control.
Multivariate machine learning methods are increasingly used to analyze neuroimaging data, often replacing more traditional "mass univariate" techniques that fit data one voxel at a time. In the functional magnetic resonance imaging (fMRI) literature, this has led to broad application of "off-the-shelf" classification and regression methods. These generic approaches allow investigators to use ready-made algorithms to accurately decode perceptual, cognitive, or behavioral states from distributed patterns of neural activity. However, when applied to correlated whole-brain fMRI data these methods suffer from coefficient instability, are sensitive to outliers, and yield dense solutions that are hard to interpret without arbitrary thresholding. Here, we develop variants of the Graph-constrained Elastic-Net (GraphNet), a fast, whole-brain regression and classification method developed for spatially and temporally correlated data that automatically yields interpretable coefficient maps (Grosenick et al., 2009b). GraphNet methods yield sparse but structured solutions by combining structured graph constraints (based on knowledge about coefficient smoothness or connectivity) with a global sparsity-inducing prior that automatically selects important variables. Because GraphNet methods can efficiently fit regression or classification models to whole-brain, multiple time-point data sets and enhance classification accuracy relative to volume-of-interest (VOI) approaches, they eliminate the need for inherently biased VOI analyses and allow whole-brain fitting without the multiple comparison problems that plague mass univariate and roaming VOI ("searchlight") methods. As fMRI data are unlikely to be normally distributed, we (1) extend GraphNet to include robust loss functions that confer insensitivity to outliers, (2) equip them with "adaptive" penalties that asymptotically guarantee correct variable selection, and (3) develop a novel sparse structured Support Vector GraphNet classifier (SVGN). When applied to previously published data (Knutson et al., 2007), these efficient whole-brain methods significantly improved classification accuracy over previously reported VOI-based analyses on the same data (Grosenick et al., 2008; Knutson et al., 2007) while discovering task-related regions not documented in the original VOI approach. Critically, GraphNet estimates fit to the Knutson et al. (2007) data generalize well to out-of-sample data collected more than three years later on the same task but with different subjects and stimuli (Karmarkar et al., submitted for publication). By enabling robust and efficient selection of important voxels from whole-brain data taken over multiple time points (>100,000 "features"), these methods enable data-driven selection of brain areas that accurately predict single-trial behavior within and across individuals.
Mild cognitive impairment (MCI) is difficult to diagnose due to its subtlety. Recent emergence of advanced network analysis techniques utilizing resting-state functional Magnetic Resonance Imaging (rs-fMRI) has made the understanding of neurological disorders more comprehensively at a whole-brain connectivity level. However, inferring effective brain connectivity from fMRI data is a challenging task, particularly when the ultimate goal is to obtain good control-patient classification performance. Incorporating sparsity into connectivity modeling can potentially produce results that are biologically more meaningful since most biologically networks are formed by a relatively few number of connections. However, this constraint, when applied at an individual level, will degrade classification performance due to inter-subject variability. To address this problem, we consider a constrained sparse linear regression model associated with the least absolute shrinkage and selection operator (LASSO). Specifically, we introduced sparsity into brain connectivity via l1-norm penalization, and ensured consistent non-zero connections across subjects via l2-norm penalization. Our results demonstrate that the constrained sparse network gives better classification performance than the conventional correlation-based network, indicating its greater sensitivity to early stage brain pathologies.
The cued-trials task-switching paradigm is used to investigate the processes involved in preparation to change task. Task switch trials typically show poorer performance than task repeat trials, suggesting that additional or more time-consuming preparation processes are required to switch tasks. However, behavioral and neuroimaging studies have so far been unable to decipher whether preparing for a switch in task involves distinct cognitive processes to those required more generally on both switch and repeat trials. The current study addresses this question using a novel multivariate pattern misclassification analysis of frequency band-specific local topographical patterns in human EEG activity that was elicited by cues varying in information value. Within the alpha frequency band, misclassification analysis produced evidence for an early switch-related preparation process over right frontal cortex, as well as a later task readiness preparation process over right parietal cortex. This represents compelling evidence for dissociable switch-related and task readiness preparation processes that show distinct time course and spatial activation patterns.
Humans are able to flexibly devise and implement rules to reach their desired goals. For simple situations, we can use single rules, such as "if traffic light is green then cross the street." In most cases, however, more complex rule sets are required, involving the integration of multiple layers of control. Although it has been shown that prefrontal cortex is important for rule representation, it has remained unclear how the brain encodes more complex rule sets. Here, we investigate how the brain represents the order in which different parts of a rule set are evaluated. Participants had to follow compound rule sets that involved the concurrent application of two single rules in a specific order, where one of the rules always had to be evaluated first. The rules and their assigned order were independently manipulated. By applying multivariate decoding to fMRI data, we found that the identity of the current rule was encoded in a frontostriatal network involving right ventrolateral prefrontal cortex, right superior frontal gyrus, and dorsal striatum. In contrast, rule order could be decoded in the dorsal striatum and in the right premotor cortex. The nonhomogeneous distribution of information across brain areas was confirmed by follow-up analyses focused on relevant regions of interest. We argue that the brain encodes complex rule sets by "decomposing" them in their constituent features, which are represented in different brain areas, according to the aspect of information to be maintained.
Newell and Simon postulated that the basic steps in human problem-solving involve iteratively applying operators to transform the state of the problem to eventually achieve a goal. To check the neural basis of this framework, the present study focused on the basic processes in human heuristic problem-solving that the participants identified the current problem state and then recalled and applied the corresponding heuristic rules to change the problem state. A new paradigm, solving simplified Sudoku puzzles, was developed for an event-related functional magnetic resonance imaging (fMRI) study in problem solving. Regions of interest (ROIs), including the left prefrontal cortex, the bilateral posterior parietal cortex, the anterior cingulated cortex, the bilateral caudate nuclei, the bilateral fusiform, as well as the bilateral frontal eye fields, were found to be involved in the task. To obtain convergent evidence, in addition to traditional statistical analysis, we used the multivariate voxel classification method to check the accuracy of the predictions for the condition of the task from the blood oxygen level dependent (BOLD) response of the ROIs, using a new classifier developed in this study for fMRI data. To reveal the roles that the ROIs play in problem solving, we developed an ACT-R computational model of the information-processing processes in human problem solving, and tried to predict the BOLD response of the ROIs from the task. Advances in human problem-solving research after Newell and Simon are then briefly discussed.
How autobiographical memories are represented in the human brain and whether this changes with time are questions central to memory neuroscience. Two regions in particular have been consistently implicated, the ventromedial prefrontal cortex (vmPFC) and the hippocampus, although their precise contributions are still contested. The key question in this debate, when reduced to its simplest form, concerns where information about specific autobiographical memories is located. Here, we availed ourselves of the opportunity afforded by multivoxel pattern analysis to provide an alternative to conventional neuropsychological and fMRI approaches, by detecting representations of individual autobiographical memories in patterns of fMRI activity. We examined whether information about specific recent (two weeks old) and remote (10 years old) autobiographical memories was represented in vmPFC and hippocampus, and other medial temporal and neocortical regions. vmPFC contained information about recent and remote autobiographical memories, although remote memories were more readily detected there, indicating that consolidation or a change of some kind had occurred. Information about both types of memory was also present in the hippocampus, suggesting it plays a role in the retrieval of vivid autobiographical memories regardless of remoteness. Interestingly, we also found that while recent and remote memories were both represented within anterior and posterior hippocampus, the latter nevertheless contained more information about remote memories. Thus, like vmPFC, the hippocampus too respected the distinction between recent and remote memories. Overall, these findings clarify and extend our view of vmPFC and hippocampus while also informing systems-level consolidation and providing clear targets for future studies.
A critical component of decision making is the ability to adjust criteria for classifying stimuli. fMRI and drift diffusion models were used to explore the neural representations of perceptual criteria in decision making. The specific focus was on the relative engagement of perceptual- and decision-related neural systems in response to adjustments in perceptual criteria. Human participants classified visual stimuli as big or small based on criteria of different sizes, which effectively biased their choices toward one response over the other. A drift diffusion model was fit to the behavioral data to extract estimates of stimulus size, criterion size, and difficulty for each participant and condition. These parameter values were used as modulated regressors to create a highly constrained model for the fMRI analysis that accounted for several components of the decision process. The results show that perceptual criteria values were reflected by activity in left inferior temporal cortex, a region known to represent objects and their physical properties, whereas stimulus size was reflected by activation in occipital cortex. A frontoparietal network of regions, including dorsolateral prefrontal cortex and superior parietal lobule, corresponded to the decision variables resulting from the downstream stimulus-criterion comparison, independent of stimulus type. The results provide novel evidence that perceptual criteria are represented in stimulus space and serve as inputs to be compared with the presented stimulus, recruiting a common network of decision regions shown to be active in other simple decisions. This work advances our understanding of the neural correlates of decision flexibility and adjustments of behavioral bias.
Estimating the value of potential actions is crucial for learning and adaptive behavior. We know little about how the human brain represents action-specific value outside of motor areas. This is, in part, due to a difficulty in detecting the neural correlates of value using conventional (region of interest) functional magnetic resonance imaging (fMRI) analyses, due to a potential distributed representation of value. We address this limitation by applying a recently developed multivariate decoding method to high-resolution fMRI data in subjects performing an instrumental learning task. We found evidence for action-specific value signals in circumscribed regions, specifically ventromedial prefrontal cortex, putamen, thalamus, and insula cortex. In contrast, action-independent value signals were more widely represented across a large set of brain areas. Using multivariate Bayesian model comparison, we formally tested whether value-specific responses are spatially distributed or coherent. We found strong evidence that both action-specific and action-independent value signals are represented in a distributed fashion. Our results suggest that a surprisingly large number of classical reward-related areas contain distributed representations of action-specific values, representations that are likely to mediate between reward and adaptive behavior.
Interaction with everyday objects requires the representation of conceptual object properties, such as where and how an object is used. What are the neural mechanisms that support this knowledge? While research on semantic dementia has provided evidence for a critical role of the anterior temporal lobes (ATLs) in object knowledge, fMRI studies using univariate analysis have primarily implicated regions outside the ATL. In the present human fMRI study we used multivoxel pattern analysis to test whether activity patterns in ATLs carry information about conceptual object properties. Participants viewed objects that differed on two dimensions: where the object is typically found (in the kitchen or the garage) and how the object is commonly used (with a rotate or a squeeze movement). Anatomical region-of-interest analyses covering the ventral visual stream revealed that information about the location and action dimensions increased from posterior to anterior ventral temporal cortex, peaking in the temporal pole. Whole-brain multivoxel searchlight analysis confirmed these results, revealing highly significant and regionally specific information about the location and action dimensions in the anterior temporal lobes bilaterally. In contrast to conceptual object properties, perceptual and low-level visual properties of the objects were reflected in activity patterns in posterior lateral occipitotemporal cortex and occipital cortex, respectively. These results provide fMRI evidence that object representations in the anterior temporal lobes are abstracted away from perceptual properties, categorizing objects in semantically meaningful groups to support conceptual object knowledge.
Sinonasal malignancy is rare, and its presentation is commonly late. There is a wide variety of pathologies with varying natural histories and survival rates. Anatomy of the skull base is extremely complex and tumors are closely related to orbits, frontal lobes and cavernous sinus. Anatomical detail and the late presentation render surgical management a challenging task. A thorough understanding of anatomy and pathology combined with modern neuroimaging and reliable reconstruction within a multidisciplinary team is imperative to carry out skull base surgery effectively. While endoscopic approaches are gaining credibility, clearly, it will be some time before meaningful comparisons with craniofacial resection can be made. Until then, craniofacial resection will remain the gold standard for managing the sinonasal malignancies of the anterior skull base, as it has proved to be safe and effective.
An ability to decode semantic information from fMRI spatial patterns has been demonstrated in previous studies mostly for 1 specific input modality. In this study, we aimed to decode semantic category independent of the modality in which an object was presented. Using a searchlight method, we were able to predict the stimulus category from the data while participants performed a semantic categorization task with 4 stimulus modalities (spoken and written names, photographs, and natural sounds). Significant classification performance was achieved in all 4 modalities. Modality-independent decoding was implemented by training and testing the searchlight method across modalities. This allowed the localization of those brain regions, which correctly discriminated between the categories, independent of stimulus modality. The analysis revealed large clusters of voxels in the left inferior temporal cortex and in frontal regions. These voxels also allowed category discrimination in a free recall session where subjects recalled the objects in the absence of external stimuli. The results show that semantic information can be decoded from the fMRI signal independently of the input modality and have clear implications for understanding the functional mechanisms of semantic memory.
Current models of prosodic emotion comprehension propose a three stage cognition mediated by temporal lobe auditory regions through to inferior and orbitofrontal regions. Cumulative evidence suggests that its mediation may be more flexible though, with a facility to respond in a graded manner based on the need for executive control. The location of this fine-tuning system is unclear, as is its similarity to the cognitive control system.
Alcohol-induced blackouts are associated with the development of alcohol abuse and dependence, so it is important to consider potential neurobiological risk factors for experiencing this problem prior to the onset of substance use. This study examines whether neural activity during inhibitory processing might be atypical in substance-naive youth who later experience alcohol-induced blackouts.
Does the sustained, elevated neural activity observed during working memory tasks reflect the short-term retention of information? Functional magnetic resonance imaging (fMRI) data of delayed recognition of visual motion in human participants were analyzed with two methods: a general linear model (GLM) and multivoxel pattern analysis. Although the GLM identified sustained, elevated delay-period activity in superior and lateral frontal cortex and in intraparietal sulcus, pattern classifiers were unable to recover trial-specific stimulus information from these delay-active regions. The converse-no sustained, elevated delay-period activity but successful classification of trial-specific stimulus information-was true of posterior visual regions, including area MT+ (which contains both middle temporal area and medial superior temporal area) and calcarine and pericalcarine cortex. In contrast to stimulus information, pattern classifiers were able to extract trial-specific task instruction-related information from frontal and parietal areas showing elevated delay-period activity. Thus, the elevated delay-period activity that is measured with fMRI may reflect processes other than the storage, per se, of trial-specific stimulus information. It may be that the short-term storage of stimulus information is represented in patterns of (statistically) "subthreshold" activity distributed across regions of low-level sensory cortex that univariate methods cannot detect.
How content is stored in the human brain during visual short-term memory (VSTM) is still an open question. Different theories postulate storage of remembered stimuli in prefrontal, parietal, or visual areas. Aiming at a distinction between these theories, we investigated the content-specificity of BOLD signals from various brain regions during a VSTM task using multivariate pattern classification. To participate in memory maintenance, candidate regions would need to have information about the different contents held in memory. We identified two brain regions where local patterns of fMRI signals represented the remembered content. Apart from the previously established storage in visual areas, we also discovered an area in the posterior parietal cortex where activity patterns allowed us to decode the specific stimuli held in memory. Our results demonstrate that storage in VSTM extends beyond visual areas, but no frontal regions were found. Thus, while frontal and parietal areas typically coactivate during VSTM, maintenance of content in the frontoparietal network might be limited to parietal cortex.
The lateral frontal cortex (LFC) is thought to represent contextual and rule-based information that allows adaptive behavior according to circumstance. Recent progress has suggested that the representations of the LFC vary along its rostral-caudal axis with more abstract, higher level representations associated with rostral areas of the LFC and more concrete, lower level representations associated with caudal areas of the LFC. Here, we investigated this proposal. Subjects responded to stimuli based upon a nested series of contextual cues stored in working memory (WM) while being scanned with fMRI. Higher level context cues denoted an abstract rule set while lower level context cues provided more concrete information. Using multi-variate pattern analysis (MVPA), we found varying forms of representation along the rostral-caudal axis of the LFC depending on the type of information stored in WM. Rostral areas of frontal cortex in the lateral orbitofrontal cortex (OFC) represented the higher level context, but not more concrete information, and only when more concrete information was unavailable. Mid-level areas in the mid-dorsolateral prefrontal cortex (DLPFC) and inferior frontal junction (IFJ) represented more concrete rules, but only when the forthcoming response could not be anticipated. By contrast, the dorsal premotor cortex (PMd) and primary motor cortex (M1) represented contextual and response information when the forthcoming response could be anticipated on the basis of context. Collectively, these data indicate that representations dedicated to higher levels of abstraction become less discriminating when more concrete information becomes available. These patterns are consistent with rostral-caudal abstraction proposals of the LFC.
This study addresses a gap in the attachment literature by investigating maternal neural response to cry related to infant attachment classifications and behaviors. Twenty-two primiparous mothers and their 18-month old infants completed the Strange Situation (SS) procedure to elicit attachment behaviors. During a separate functional MRI session, mothers were exposed to their own infant's cry sound, as well as an unfamiliar infant's cry and control sound. Maternal neural response to own infant cry related to both overall attachment security and specific infant behaviors. Mothers of less secure infants maintained greater activation to their cry in left parahippocampal and amygdala regions and the right posterior insula consistent with a negative schematic response bias. Mothers of infants exhibiting more avoidant or contact maintaining behaviors during the SS showed diminished response across left prefrontal, parietal, and cerebellar areas involved in attentional processing and cognitive control. Mothers of infants exhibiting more disorganized behavior showed reduced response in bilateral temporal and subcallosal areas relevant to social cognition and emotion regulation. No differences by attachment classification were found. Implications for attachment transmission models are discussed.
When interacting with someone from another social group, one's responses may be influenced by both stereotypes and evaluations. Given behavioral results suggesting that stereotypes and evaluative associations operate independently, we used fMRI to test whether these biases are mediated by distinct brain systems. White participants viewed pairs of Black or White faces and judged them based on an evaluation (who would you befriend?) or a stereotype-relevant trait (who is more likely to enjoy athletic activities?). Multi-voxel pattern analysis revealed that a predominantly occipital network represented race in a context-invariant manner. However, lateral orbitofrontal cortex preferentially represented race during friendship judgments, whereas anterior medial prefrontal cortex preferentially represented race during trait judgments. Furthermore, representation of race in left temporal pole correlated with a behavioral measure of evaluative bias during friendship judgments and, independently, a measure of stereotyping during trait judgments. Whereas early sensory regions represent race in an apparently invariant manner, representations in higher-level regions are multi-componential and context-dependent.
A fundamental principle in memory research is that memory is a function of the similarity between encoding and retrieval operations. Consistent with this principle, many neurobiological models of declarative memory assume that memory traces are stored in cortical regions, and the hippocampus facilitates the reactivation of these traces during retrieval. The present investigation tested the novel prediction that encoding-retrieval similarity can be observed and related to memory at the level of individual items. Multivariate representational similarity analysis was applied to functional magnetic resonance imaging data collected during encoding and retrieval of emotional and neutral scenes. Memory success tracked fluctuations in encoding-retrieval similarity across frontal and posterior cortices. Importantly, memory effects in posterior regions reflected increased similarity between item-specific representations during successful recognition. Mediation analyses revealed that the hippocampus mediated the link between cortical similarity and memory success, providing crucial evidence for hippocampal-cortical interactions during retrieval. Finally, because emotional arousal is known to modulate both perceptual and memory processes, similarity effects were compared for emotional and neutral scenes. Emotional arousal was associated with enhanced similarity between encoding and retrieval patterns. These findings speak to the promise of pattern similarity measures for evaluating memory representations and hippocampal-cortical interactions.
Humans can attend to different objects independent of their spatial locations. While selecting an object has been shown to modulate object processing in high-level visual areas in occipitotemporal cortex, where/how behavioral importance (i.e., priority) for objects is represented is unknown. Here we examined the patterns of distributed neural activity during an object-based selection task. We measured brain activity with functional magnetic resonance imaging (fMRI), while participants viewed two superimposed, dynamic objects (left- and right-pointing triangles) and were cued to attend to one of the triangle objects. Enhanced fMRI response was observed for the attention conditions compared to a neutral condition, but no significant difference was found in overall response amplitude between two attention conditions. By using multi-voxel pattern classification (MVPC), however, we were able to distinguish the neural patterns associated with attention to different objects in early visual cortex (V1 to hMT+) and lateral occipital complex (LOC). Furthermore, distinct multi-voxel patterns were also observed in frontal and parietal areas. Our results demonstrate that object-based attention has a wide-spread modulation effect along the visual hierarchy and suggest that object-specific priority information is represented by patterned neural activity in the dorsal frontoparietal network.
Cognitive control is necessary to flexibly act in changing environments. Sequence processing is needed in language comprehension to build the syntactic structure in sentences. Functional imaging studies suggest that sequence processing engages the left ventrolateral prefrontal cortex (PFC). In contrast, cognitive control processes additionally recruit bilateral rostral lateral PFC regions. The present study aimed to investigate these two types of processes in one experimental paradigm. Sequence processing was manipulated using two different sequencing rules varying in complexity. Cognitive control was varied with different cue-sets that determined the choice of a sequencing rule. Univariate analyses revealed distinct PFC regions for the two types of processing (i.e. sequence processing: left ventrolateral PFC and cognitive control processing: bilateral dorsolateral and rostral PFC). Moreover, in a common brain network (including left lateral PFC and intraparietal sulcus) no interaction between sequence and cognitive control processing was observed. In contrast, a multivariate pattern analysis revealed an interaction of sequence and cognitive control processing, such that voxels in left lateral PFC and parietal cortex showed different tuning functions for tasks involving different sequencing and cognitive control demands. These results suggest that the difference between the process of rule selection (i.e. cognitive control) and the process of rule-based sequencing (i.e. sequence processing) find their neuronal underpinnings in distinct activation patterns in lateral PFC. Moreover, the combination of rule selection and rule sequencing can shape the response of neurons in lateral PFC and parietal cortex.
Expectations and prior knowledge are thought to support the perceptual analysis of incoming sensory stimuli, as proposed by the predictive-coding framework. The current fMRI study investigated the effect of prior information on brain activity during the decoding of degraded speech stimuli. When prior information enabled the comprehension of the degraded sentences, the left middle temporal gyrus and the left angular gyrus were activated, highlighting a role of these areas in meaning extraction. In contrast, the activation of the left inferior frontal gyrus (area 44/45) appeared to reflect the search for meaningful information in degraded speech material that could not be decoded because of mismatches with the prior information. Our results show that degraded sentences evoke instantaneously different percepts and activation patterns depending on the type of prior information, in line with prediction-based accounts of perception.
Pain is known to comprise sensory, cognitive, and affective aspects. Despite numerous previous fMRI studies, however, it remains open which spatial distribution of activity is sufficient to encode whether a stimulus is perceived as painful or not. In this study, we analyzed fMRI data from a perceptual decision-making task in which participants were exposed to near-threshold laser pulses. Using multivariate analyses on different spatial scales, we investigated the predictive capacity of fMRI data for decoding whether a stimulus had been perceived as painful. Our analysis yielded a rank order of brain regions: during pain anticipation, activity in the periaqueductal gray (PAG) and orbitofrontal cortex (OFC) afforded the most accurate trial-by-trial discrimination between painful and non-painful experiences; whereas during the actual stimulation, primary and secondary somatosensory cortex, anterior insula, dorsolateral and ventrolateral prefrontal cortex, and OFC were most discriminative. The most accurate prediction of pain perception from the stimulation period, however, was enabled by the combined activity in pain regions commonly referred to as the 'pain matrix'. Our results demonstrate that the neural representation of (near-threshold) pain is spatially distributed and can be best described at an intermediate spatial scale. In addition to its utility in establishing structure-function mappings, our approach affords trial-by-trial predictions and thus represents a step towards the goal of establishing an objective neuronal marker of pain perception.
Perceptual decision-making entails the transformation of graded sensory signals into categorical judgments. Often, there is a direct mapping between these judgments and specific motor responses. However, when stimulus-response mappings are fixed, neural activity underlying decision-making cannot be separated from neural activity reflecting motor planning. Several human neuroimaging studies have reported changes in brain activity associated with perceptual decisions. Nevertheless, to date it has remained unknown where and how specific choices are encoded in the human brain when motor planning is decoupled from the decision process. We addressed this question by having subjects judge the direction of motion of dynamic random dot patterns at various levels of motion strength while measuring their brain activity with fMRI. We used multivariate decoding analyses to search the whole brain for patterns of brain activity encoding subjects' choices. To decouple the decision process from motor planning, subjects were informed about the required motor response only after stimulus presentation. Patterns of fMRI signals in early visual and inferior parietal cortex predicted subjects' perceptual choices irrespective of motor planning. This was true across several levels of motion strength and even in the absence of any coherent stimulus motion. We also found that the cortical distribution of choice-selective brain signals depended on stimulus strength: While visual cortex carried most choice-selective information for strong motion, information in parietal cortex decreased with increasing motion coherence. These results demonstrate that human visual and inferior parietal cortex carry information about the visual decision in a more abstract format than can be explained by simple motor intentions. Both brain regions may be differentially involved in perceptual decision-making in the face of strong and weak sensory evidence.
Item-specific spatial information is essential for interacting with objects and for binding multiple features of an object together. Spatial relational information is necessary for implicit tasks such as recognizing objects or scenes from different views but also for explicit reasoning about space such as planning a route with a map and for other distinctively human traits such as tool construction. To better understand how the brain supports these two different kinds of information, we used functional MRI to directly contrast the neural encoding and maintenance of spatial relations with that for item locations in equivalent visual scenes. We found a double dissociation between the two: whereas item-specific processing implicates a frontoparietal attention network, including the superior frontal sulcus and intraparietal sulcus, relational processing preferentially recruits a cognitive control network, particularly lateral prefrontal cortex (PFC) and inferior parietal lobule. Moreover, pattern classification revealed that the actual meaning of the relation can be decoded within these same regions, most clearly in rostrolateral PFC, supporting a hierarchical, representational account of prefrontal organization.
Decoding others' intentions is a crucial aspect of social cognition. Neuroimaging studies suggest that inferring immediate goals engages the neural system for action understanding (i.e. mirror system), while the decoding of long-term intentions requires the system subserving the attribution of mental states (i.e. mentalizing). A controversial issue, stimulated by recent inconsistent results, concerns whether the two systems are concurrently vs. exclusively involved in intention understanding. This issue is particularly relevant in the case of social interactions, whose processing has been mostly, but not uncontroversially, associated with the mentalizing system. We tested the alternative hypothesis that the relative contribution of the two systems in intention understanding may also depend on the shared goal of interacting agents. To this purpose, 27 participants observed social interactions differing in their cooperative vs. affective shared goal during functional-Magnetic-Resonance-Imaging. The processing of both types of interactions activated the right temporo-parietal junction involved in mentalizing on action goals. Additionally, whole-brain and regions-of-interest analyses showed that the action understanding system (inferior prefrontal-parietal cortex) was more strongly activated by cooperative interactions, while the mentalizing-proper system (medial prefrontal cortex) was more strongly engaged by affective interactions. These differences were modulated by individual differences in empathizing. Both systems can thus be involved in understanding social intentions, with a relative weighting depending on the specific shared goal of the interaction.
Neuro-imaging holds great potential for predicting choice behavior from brain responses. In this study we used both traditional mass-univariate and state-of-the-art multivariate pattern analysis to establish which brain regions respond to preferred packages and to what extent neural activation patterns can predict realistic low-involvement consumer choices. More specifically, this was assessed in the context of package-induced binary food choices. Mass-univariate analyses showed that several regions, among which the bilateral striatum, were more strongly activated in response to preferred food packages. Food choices could be predicted with an accuracy of up to 61.2% by activation patterns in brain regions previously found to be involved in healthy food choices (superior frontal gyrus) and visual processing (middle occipital gyrus). In conclusion, this study shows that mass-univariate analysis can detect small package-induced differences in product preference and that MVPA can successfully predict realistic low-involvement consumer choices from functional MRI data.
To make adaptive decisions in a social context, humans must identify relevant agents in the environment, infer their underlying strategies and motivations, and predict their upcoming actions. We used functional magnetic resonance imaging, in conjunction with combinatorial multivariate pattern analysis, to predict human participants' subsequent decisions in an incentive-compatible poker game. We found that signals from the temporal-parietal junction provided unique information about the nature of the upcoming decision, and that information was specific to decisions against agents who were both social and relevant for future behavior.
Humans survive in environments that contain a vast quantity and variety of visual information. All items of perceived visual information must be represented within a limited number of brain networks. The human brain requires mechanisms for selecting only a relevant fraction of perceived information for more in-depth processing, where neural representations of that information may be actively maintained and utilized for goal-directed behavior. Object-based attention is crucial for goal-directed behavior and yet remains poorly understood. Thus, in the study we investigate how neural representations of visual object information are guided by selective attention. The magnitude of activation in human extrastriate cortex has been shown to be modulated by attention; however, object-based attention is not likely to be fully explained by a localized gain mechanism. Thus, we measured information coded in spatially distributed patterns of brain activity with fMRI while human participants performed a task requiring selective processing of a relevant visual object category that differed across conditions. Using pattern classification and spatial correlation techniques, we found that the direction of selective attention is implemented as a shift in the tuning of object-based information representations within extrastriate cortex. In contrast, we found that representations within lateral prefrontal cortex (PFC) coded for the attention condition rather than the concrete representations of object category. In sum, our findings are consistent with a model of object-based selective attention in which representations coded within extrastriate cortex are tuned to favor the representation of goal-relevant information, guided by more abstract representations within lateral PFC.
Vocal expressions commonly elicit activity in superior temporal and inferior frontal cortices, indicating a distributed network to decode vocally expressed emotions. We examined the involvement of this fronto-temporal network for the decoding of angry voices during attention towards (explicit attention) or away from emotional cues in voices (implicit attention) based on a reanalysis of previous data (Fruhholz, S., Ceravolo, L., Grandjean, D., 2012. Cerebral Cortex 22, 1107-1117). The general network revealed high interconnectivity of bilateral inferior frontal gyrus (IFG) to different bilateral voice-sensitive regions in mid and posterior superior temporal gyri. Right superior temporal gyrus (STG) regions showed connectivity to the left primary auditory cortex and secondary auditory cortex (AC) as well as to high-level auditory regions. This general network revealed differences in connectivity depending on the attentional focus. Explicit attention to angry voices revealed a specific right-left STG network connecting higher-level AC. During attention to a nonemotional vocal feature we also found a left-right STG network implicitly elicited by angry voices that also included low-level left AC. Furthermore, only during this implicit processing there was widespread interconnectivity between bilateral IFG and bilateral STG. This indicates that while implicit attention to angry voices recruits extended bilateral STG and IFG networks for the sensory and evaluative decoding of voices, explicit attention to angry voices solely involves a network of bilateral STG regions probably for the integrative recognition of emotional cues from voices.
The conceptual notion of the so-called resting state of the brain has been recently challenged by studies indicating a continuing effect of cognitive processes on subsequent rest. In particular, activity in posterior parietal and medial prefrontal areas has been found to be modulated by preceding experimental conditions. In this study, we investigated which brain areas show working memory dependent patterns in subsequent baseline periods and how specific they are for the preceding experimental condition. During functional magnetic resonance imaging, 94 subjects performed a letter-version of the n-back task with the conditions 0-back and 2-back followed by a low-level baseline in which subjects had to passively observe the letters appearing. In a univariate analysis, 2-back served as control condition while 0-back, baseline after 0-back and baseline after 2-back were modeled as regressors to test for activity changes between both baseline conditions. Additionally, we tested, using Gaussian process classifiers, the recognition of task condition from functional images acquired during baseline. Besides the expected activity changes in the precuneus and medial prefrontal cortex, we found differential activity in the thalamus, putamen, and postcentral gyrus that were affected by the preceding task. The multivariate analysis revealed that images of the subsequent baseline block contain task related patterns that yield a recognition rate of 70%. The results suggest that the influence of a cognitive task on subsequent baseline is strong and specific for some areas but not restricted to areas of the so-called default mode network.
The brain network underlying speech comprehension is usually described as encompassing fronto-temporal-parietal regions while neuroimaging studies of speech intelligibility have focused on a more spatially restricted network dominated by the superior temporal cortex. Here we use functional magnetic resonance imaging with a novel whole-brain multivariate pattern analysis (MVPA) to more fully characterize neural responses and connectivity to intelligible speech. Consistent with previous univariate findings, intelligible speech elicited greater activity in bilateral superior temporal cortex relative to unintelligible speech. However, MVPA identified a more extensive network that discriminated between intelligible and unintelligible speech, including left-hemisphere middle temporal gyrus, angular gyrus, inferior temporal cortex, and inferior frontal gyrus pars triangularis. These fronto-temporal-parietal areas also showed greater functional connectivity during intelligible, compared with unintelligible, speech. Our results suggest that speech intelligibly is encoded by distinct fine-grained spatial representations and within-task connectivity, rather than differential engagement or disengagement of brain regions, and they provide a more complete view of the brain network serving speech comprehension. Our findings bridge a divide between neural models of speech comprehension and the neuroimaging literature on speech intelligibility, and suggest that speech intelligibility relies on differential multivariate response and connectivity patterns in Wernicke's, Broca's, and Geschwind's areas.
Impulsivity is a complex trait associated with a range of maladaptive behaviors, including many forms of psychopathology. Previous research has implicated multiple neural circuits and neurotransmitter systems in impulsive behavior, but the relationship between impulsivity and organization of whole-brain networks has not yet been explored. Using graph theory analyses, we characterized the relationship between impulsivity and the functional segregation ("modularity") of the whole-brain network architecture derived from resting-state functional magnetic resonance imaging (fMRI) data. These analyses revealed remarkable differences in network organization across the impulsivity spectrum. Specifically, in highly impulsive individuals, regulatory structures including medial and lateral regions of the prefrontal cortex were isolated from subcortical structures associated with appetitive drive, whereas these brain areas clustered together within the same module in less impulsive individuals. Further exploration of the modular organization of whole-brain networks revealed novel shifts in the functional connectivity between visual, sensorimotor, cortical, and subcortical structures across the impulsivity spectrum. The current findings highlight the utility of graph theory analyses of resting-state fMRI data in furthering our understanding of the neurobiological architecture of complex behaviors.
When we have a rich and vivid memory for a past experience, it often feels like we are transported back in time to witness once again this event. Indeed, a perfect memory would exactly mimic the experiential quality of direct sensory perception. We used fMRI and multivoxel pattern analysis to map and quantify the similarity between patterns of activation evoked by direct perception of a diverse set of short video clips and the vivid remembering, with closed eyes, of these clips. We found that the patterns of distributed brain activation during vivid memory mimicked the patterns evoked during sensory perception. Using whole-brain patterns of activation evoked by perception of the videos, we were able to accurately classify brain patterns that were elicited when participants tried to vividly recall those same videos. A discriminant analysis of the activation patterns associated with each video revealed a high degree (explaining over 80% of the variance) of shared representational similarity between perception and memory. These results show that complex, multifeatured memory involves a partial reinstatement of the whole pattern of brain activity that is evoked during initial perception of the stimulus.
Recent evidence from functional magnetic resonance imaging suggests that cortical hemodynamic responses coincide in different subjects experiencing a common naturalistic stimulus. Here we utilize neural responses in the electroencephalogram (EEG) evoked by multiple presentations of short film clips to index brain states marked by high levels of correlation within and across subjects. We formulate a novel signal decomposition method which extracts maximally correlated signal components from multiple EEG records. The resulting components capture correlations down to a one-second time resolution, thus revealing that peak correlations of neural activity across viewings can occur in remarkable correspondence with arousing moments of the film. Moreover, a significant reduction in neural correlation occurs upon a second viewing of the film or when the narrative is disrupted by presenting its scenes scrambled in time. We also probe oscillatory brain activity during periods of heightened correlation, and observe during such times a significant increase in the theta band for a frontal component and reductions in the alpha and beta frequency bands for parietal and occipital components. Low-resolution EEG tomography of these components suggests that the correlated neural activity is consistent with sources in the cingulate and orbitofrontal cortices. Put together, these results suggest that the observed synchrony reflects attention- and emotion-modulated cortical processing which may be decoded with high temporal resolution by extracting maximally correlated components of neural activity.
Humans have an unparalleled ability to represent objects as members of multiple categories. A given object, such as a pillow may be-depending on current task demands-represented as an instance of something that is soft, as something that contains feathers, as something that is found in bedrooms, or something that is larger than a toaster. This type of processing requires the individual to dynamically highlight task-relevant properties and abstract over or suppress object properties that, although salient, are not relevant to the task at hand. Neuroimaging and neuropsychological evidence suggests that this ability may depend on cognitive control processes associated with the left inferior prefrontal gyrus. Here, we show that stimulating the left inferior frontal cortex using transcranial direct current stimulation alters performance of healthy subjects on a simple categorization task. Our task required subjects to select pictures matching a description, e.g., "click on all the round things." Cathodal stimulation led to poorer performance on classification trials requiring attention to specific dimensions such as color or shape as opposed to trials that required selecting items belonging to a more thematic category such as objects that hold water. A polarity reversal (anodal stimulation) lowered the threshold for selecting items that were more weakly associated with the target category. These results illustrate the role of frontally-mediated control processes in categorization and suggest potential interactions between categorization, cognitive control, and language.
Areas involved in social cognition, such as the medial prefrontal cortex (mPFC) and the left temporo-parietal junction (TPJ) appear to be active during the classification of sentences according to emotional criteria (happy, angry or sad, [Beaucousin et al., 2007]). These two regions are frequently co-activated in studies about theory of mind (ToM). To confirm that these regions constitute a coherent network during affective speech comprehension, new event-related functional magnetic resonance imaging data were acquired, using the emotional and grammatical-person sentence classification tasks on a larger sample of 51 participants. The comparison of the emotional and grammatical tasks confirmed the previous findings. Functional connectivity analyses established a clear demarcation between a "Medial" network, including the mPFC and TPJ regions, and a bilateral "Language" network, which gathered inferior frontal and temporal areas. These findings suggest that emotional speech comprehension results from interactions between language, ToM and emotion processing networks. The language network, active during both tasks, would be involved in the extraction of lexical and prosodic emotional cues, while the medial network, active only during the emotional task, would drive the making of inferences about the sentences' emotional content, based on their meanings. The left and right amygdalae displayed a stronger response during the emotional condition, but were seldom correlated with the other regions, and thus formed a third entity. Finally, distinct regions belonging to the Language and Medial networks were found in the left angular gyrus, where these two systems could interface.
It is currently unclear to what extent cortical structures are required for and engaged during subconscious processing of biologically salient affective stimuli (i.e. the 'low-road' vs. 'many-roads' hypotheses). Here we show that cortical-cortical and cortical-subcortical functional connectivity (FC) contain substantially more information, relative to subcortical-subcortical FC (i.e. 'subcortical alarm' and other limbic regions), that predicts subliminal fearful face processing within individuals using training data from separate subjects. A plot of classification accuracy vs. number of selected whole-brain FC features revealed 92% accuracy when learning was based on the top 8 features from each training set. The most informative FC was between right amygdala and precuneus, which increased during subliminal fear conditions, while left and right amygdala FC decreased, suggesting a bilateral decoupling of this key limbic region during processing of subliminal fear-related stimuli. Other informative FC included angular gyrus, middle temporal gyrus and cerebellum. These findings identify FC that decodes subliminally perceived, task-irrelevant affective stimuli, and suggest that cortical structures are actively engaged by and appear to be essential for subliminal fear processing.
Processing of unattended threat-related stimuli, such as fearful faces, has been previously examined using group functional magnetic resonance (fMRI) approaches. However, the identification of features of brain activity containing sufficient information to decode, or "brain-read", unattended (implicit) fear perception remains an active research goal. Here we test the hypothesis that patterns of large-scale functional connectivity (FC) decode the emotional expression of implicitly perceived faces within single individuals using training data from separate subjects. fMRI and a blocked design were used to acquire BOLD signals during implicit (task-unrelated) presentation of fearful and neutral faces. A pattern classifier (linear kernel Support Vector Machine, or SVM) with linear filter feature selection used pair-wise FC as features to predict the emotional expression of implicitly presented faces. We plotted classification accuracy vs. number of top N selected features and observed that significantly higher than chance accuracies (between 90-100%) were achieved with 15-40 features. During fearful face presentation, the most informative and positively modulated FC was between angular gyrus and hippocampus, while the greatest overall contributing region was the thalamus, with positively modulated connections to bilateral middle temporal gyrus and insula. Other FCs that predicted fear included superior-occipital and parietal regions, cerebellum and prefrontal cortex. By comparison, patterns of spatial activity (as opposed to interactivity) were relatively uninformative in decoding implicit fear. These findings indicate that whole-brain patterns of interactivity are a sensitive and informative signature of unattended fearful emotion processing. At the same time, we demonstrate and propose a sensitive and exploratory approach for the identification of large-scale, condition-dependent FC. In contrast to model-based, group approaches, the current approach does not discount the multivariate, joint responses of multiple functional connections and is not hampered by signal loss and the need for multiple comparisons correction.
THIS FUNCTIONAL MAGNETIC RESONANCE IMAGING STUDY EXAMINES SHARED AND DISTINCT CORTICAL AREAS INVOLVED IN THE AUDITORY PERCEPTION OF SONG AND SPEECH AT THE LEVEL OF THEIR UNDERLYING CONSTITUENTS: words and pitch patterns. Univariate and multivariate analyses were performed to isolate the neural correlates of the word- and pitch-based discrimination between song and speech, corrected for rhythmic differences in both. Therefore, six conditions, arranged in a subtractive hierarchy were created: sung sentences including words, pitch and rhythm; hummed speech prosody and song melody containing only pitch patterns and rhythm; and as a control the pure musical or speech rhythm. Systematic contrasts between these balanced conditions following their hierarchical organization showed a great overlap between song and speech at all levels in the bilateral temporal lobe, but suggested a differential role of the inferior frontal gyrus (IFG) and intraparietal sulcus (IPS) in processing song and speech. While the left IFG coded for spoken words and showed predominance over the right IFG in prosodic pitch processing, an opposite lateralization was found for pitch in song. The IPS showed sensitivity to discrete pitch relations in song as opposed to the gliding pitch in speech. Finally, the superior temporal gyrus and premotor cortex coded for general differences between words and pitch patterns, irrespective of whether they were sung or spoken. Thus, song and speech share many features which are reflected in a fundamental similarity of brain areas involved in their perception. However, fine-grained acoustic differences on word and pitch level are reflected in the IPS and the lateralized activity of the IFG.
Although much effort has been directed toward understanding the neural basis of speech processing, the neural processes involved in the categorical perception of speech have been relatively less studied, and many questions remain open. In this functional magnetic resonance imaging (fMRI) study, we probed the cortical regions mediating categorical speech perception using an advanced brain-mapping technique, whole-brain multivariate pattern-based analysis (MVPA). Normal healthy human subjects (native English speakers) were scanned while they listened to 10 consonant-vowel syllables along the /ba/-/da/ continuum. Outside of the scanner, individuals' own category boundaries were measured to divide the fMRI data into /ba/ and /da/ conditions per subject. The whole-brain MVPA revealed that Broca's area and the left pre-supplementary motor area evoked distinct neural activity patterns between the two perceptual categories (/ba/ vs /da/). Broca's area was also found when the same analysis was applied to another dataset (Raizada and Poldrack, 2007), which previously yielded the supramarginal gyrus using a univariate adaptation-fMRI paradigm. The consistent MVPA findings from two independent datasets strongly indicate that Broca's area participates in categorical speech perception, with a possible role of translating speech signals into articulatory codes. The difference in results between univariate and multivariate pattern-based analyses of the same data suggest that processes in different cortical areas along the dorsal speech perception stream are distributed on different spatial scales.
Emotional responses can be induced by external sensory stimuli. For severely disabled nonverbal individuals who have no means of communication, the decoding of emotion may offer insight into an individual's state of mind and his/her response to events taking place in the surrounding environment. Near-infrared spectroscopy (NIRS) provides an opportunity for bed-side monitoring of emotions via measurement of hemodynamic activity in the prefrontal cortex, a brain region known to be involved in emotion processing. In this paper, prefrontal cortex activity of ten able-bodied participants was monitored using NIRS as they listened to 78 music excerpts with different emotional content and a control acoustic stimuli consisting of the Brown noise. The participants rated their emotional state after listening to each excerpt along the dimensions of valence (positive versus negative) and arousal (intense versus neutral). These ratings were used to label the NIRS trial data. Using a linear discriminant analysis-based classifier and a two-dimensional time-domain feature set, trials with positive and negative emotions were discriminated with an average accuracy of 71.94%  8.19%. Trials with audible Brown noise representing a neutral response were differentiated from high arousal trials with an average accuracy of 71.93%  9.09% using a two-dimensional feature set. In nine out of the ten participants, response to the neutral Brown noise was differentiated from high arousal trials with accuracies exceeding chance level, and positive versus negative emotional differentiation accuracies exceeded the chance level in seven out of the ten participants. These results illustrate that NIRS recordings of the prefrontal cortex during presentation of music with emotional content can be automatically decoded in terms of both valence and arousal encouraging future investigation of NIRS-based emotion detection in individuals with severe disabilities.
On a daily basis we form numerous intentions to perform specific actions. However, we often have to delay the execution of intended actions while engaging in other demanding activities. Previous research has shown that patterns of activity in human prefrontal cortex (PFC) can reveal our current intentions. However, two fundamental questions have remained unresolved: (a) how does the PFC encode information about future tasks while we are busy engaging in other activities, and (b) how does the PFC enable us to commence a stored task at the intended time? Here we investigate how the brain stores and retrieves future intentions during occupied delays, i.e. while a person is busy performing a different task. For this purpose, we conducted a neuroimaging study with a time-based prospective memory paradigm. Using multivariate pattern classification and fMRI we show that during an occupied delay, activity patterns in the anterior PFC encode the content of 'what' subjects intend to do next, and 'when' they intend to do it. Importantly, distinct anterior PFC regions store the 'what' and 'when' components of future intentions during occupied maintenance and self-initiated retrieval. These results show a role for anterior PFC activity patterns in storing future action plans and ensuring their timely retrieval.
In two replication studies we examined response bias and dependencies in voluntary decisions. We trained a linear classifier to predict "spontaneous decisions" and in the second study "hidden intentions" from responses in preceding trials and achieved comparable prediction accuracies as reported for multivariate pattern classification based on voxel activities in frontopolar cortex. We discuss implications of our findings and suggest ways to improve classification analyses of fMRI BOLD signals that may help to reduce effects of response dependencies between trials.
Our ability to remember new information is often compromised by competition from prior learning, leading to many instances of forgetting. One of the challenges in studying why these lapses occur and how they can be prevented is that it is methodologically difficult to "see" competition between memories as it occurs. Here, we used multi-voxel pattern analysis of human fMRI data to measure the neural reactivation of both older (competing) and newer (target) memories during individual attempts to retrieve newer memories. Of central interest was the following: (1) whether older memories were reactivated during retrieval of newer memories; (2) how reactivation of older memories related to retrieval performance; and (3) whether neural mechanisms engaged during the encoding of newer memories were predictive of neural competition experienced during retrieval. Our results indicate that older and newer visual memories were often simultaneously reactivated in ventral temporal cortex--even when target memories were successfully retrieved. Importantly, stronger reactivation of older memories was associated with less accurate retrieval of newer memories, slower mnemonic decisions, and increased activity in anterior cingulate cortex. Finally, greater activity in the inferior frontal gyrus during the encoding of newer memories (memory updating) predicted lower competition in ventral temporal cortex during subsequent retrieval. Together, these results provide novel insight into how older memories compete with newer memories and specify neural mechanisms that allow competition to be overcome and memories to be updated.
Speech is an important carrier of emotional information. However, little is known about how different vocal emotion expressions are recognized in a receiver's brain. We used multivariate pattern analysis of functional magnetic resonance imaging data to investigate to which degree distinct vocal emotion expressions are represented in the receiver's local brain activity patterns. Specific vocal emotion expressions are encoded in a right fronto-operculo-temporal network involving temporal regions known to subserve suprasegmental acoustic processes and a fronto-opercular region known to support emotional evaluation, and, moreover, in left temporo-cerebellar regions covering sequential processes. The right inferior frontal region, in particular, was found to differentiate distinct emotional expressions. The present analysis reveals vocal emotion to be encoded in a shared cortical network reflected by distinct brain activity patterns. These results shed new light on theoretical and empirical controversies about the perception of distinct vocal emotion expressions at the level of large-scale human brain signals.
Previous research on artificial grammar has indicated that the human ability to classify sentences or letter strings according to grammaticality relies on two types of knowledge. One is a superficial, familiarity-based understanding of a grammar the other is the knowledge of rules and critical features underlying a grammar. The fundamentally different characteristics of these systems permit an analysis of receiver-operating characteristics (ROC), which measures the extent to which each type of knowledge is used in grammaticality judgments. Furthermore, violations of a grammar can be divided into hierarchical and local violations. The present study is the first to combine the use of ROC analyses, fMRI and a grammaticality dichotomy. Based on previous neuroimaging studies, it was hypothesized that judgments based on rule knowledge, as extracted from individual ROC analyses, involve the left inferior frontal gyrus (IFG), whereas similarity would involve right IFG, as well as left hippocampal regions. With regards to violation types, it was hypothesized that hierarchical violations would recruit the opercular part of the left IFG as well as the posterior operculum, whereas local violations would bilaterally activate the premotor cortex (PMC). Results indicated that for greater reliance on rule knowledge, a ventral part of the left PMC was activated for ungrammatical items, whereas other PMC areas show a differentiated response for grammaticality for individuals less reliant on similarity. The right IFG was related to ungrammatical items as a function of similarity. Results are discussed with regards to possible error detection systems and differentiated efficiencies for respective classification strategies.
Unlike natural numbers, negative numbers do not have natural physical referents. How does the brain represent such abstract mathematical concepts? Two competing hypotheses regarding representational systems for negative numbers are a rule-based model, in which symbolic rules are applied to negative numbers to translate them into positive numbers when assessing magnitudes, and an expanded magnitude model, in which negative numbers have a distinct magnitude representation. Using an event-related functional magnetic resonance imaging design, we examined brain responses in 22 adults while they performed magnitude comparisons of negative and positive numbers that were quantitatively near (difference <4) or far apart (difference >6). Reaction times (RTs) for negative numbers were slower than positive numbers, and both showed a distance effect whereby near pairs took longer to compare. A network of parietal, frontal, and occipital regions were differentially engaged by negative numbers. Specifically, compared to positive numbers, negative number processing resulted in greater activation bilaterally in intraparietal sulcus (IPS), middle frontal gyrus, and inferior lateral occipital cortex. Representational similarity analysis revealed that neural responses in the IPS were more differentiated among positive numbers than among negative numbers, and greater differentiation among negative numbers was associated with faster RTs. Our findings indicate that despite negative numbers engaging the IPS more strongly, the underlying neural representation are less distinct than that of positive numbers. We discuss our findings in the context of the two theoretical models of negative number processing and demonstrate how multivariate approaches can provide novel insights into abstract number representation.
Comparing two stimuli that occur at different times demands the coordination of bottom-up and top-down processes. It has been hypothesized that the dorsolateral prefrontal (PFC) cortex, the likely source of top-down cortical influences, plays a key role in such tasks, contributing to both maintenance and sensory comparisons. We examined this hypothesis by recording from the PFC of monkeys comparing directions of two moving stimuli, S1 and S2, separated by a memory delay. We determined the contribution of the two principal cell types to these processes by classifying neurons into broad-spiking (BS) putative pyramidal cells and narrow-spiking (NS) putative local interneurons. During the delay, BS cells were more likely to exhibit anticipatory modulation and represent the remembered direction. While this representation was transient, appearing at different times in different neurons, it weakened when direction was not task relevant, suggesting its utility. During S2, both putative cell types showed comparison-related activity modulations. These modulations were of two types, each carried by different neurons, which either preferred trials with stimuli moving in the same direction or trials with stimuli of different directions. These comparison effects were strongly correlated with choice, suggesting their role in circuitry underlying decision making. These results provide the first demonstration of distinct contributions made by principal cell types to memory-guided perceptual decisions. During sensory stimulation both cell types represent behaviorally relevant stimulus features contributing to comparison and decision-related activity. However in the absence of sensory stimulation, putative pyramidal cells dominated, carrying information about the elapsed time and the preceding direction.
Decision-making in an uncertain environment is driven by two major needs: exploring the environment to gather information or exploiting acquired knowledge to maximize reward. The neural processes underlying exploratory decision-making have been mainly studied by means of functional magnetic resonance imaging, overlooking any information about the time when decisions are made. Here, we carried out an electroencephalography (EEG) experiment, in order to detect the time when the brain generators responsible for these decisions have been sufficiently activated to lead to the next decision. Our analyses, based on a classification scheme, extract time-unlocked voltage topographies during reward presentation and use them to predict the type of decisions made on the subsequent trial. Classification accuracy, measured as the area under the Receiver Operator's Characteristic curve was on average 0.65 across 7 subjects. Classification accuracy was above chance levels already after 516 ms on average, across subjects. We speculate that decisions were already made before this critical period, as confirmed by a positive correlation with reaction times across subjects. On an individual subject basis, distributed source estimations were performed on the extracted topographies to statistically evaluate the neural correlates of decision-making. For trials leading to exploration, there was significantly higher activity in dorsolateral prefrontal cortex and the right supramarginal gyrus; areas responsible for modulating behavior under risk and deduction. No area was more active during exploitation. We show for the first time the temporal evolution of differential patterns of brain activation in an exploratory decision-making task on a single-trial basis.
Recent advances in neuroimaging demonstrate the potential of functional near-infrared spectroscopy (fNIRS) for use in brain-computer interfaces (BCIs). fNIRS uses light in the near-infrared range to measure brain surface haemoglobin concentrations and thus determine human neural activity. Our primary goal in this study is to analyse brain haemodynamic responses for application in a BCI. Specifically, we develop an efficient signal processing algorithm to extract important mental-task-relevant neural features and obtain the best possible classification performance. We recorded brain haemodynamic responses due to frontal cortex brain activity from nine subjects using a 19-channel fNIRS system. Our algorithm is based on continuous wavelet transforms (CWTs) for multi-scale decomposition and a soft thresholding algorithm for de-noising. We adopted three machine learning algorithms and compared their performance. Good performance can be achieved by using the de-noised wavelet coefficients as input features for the classifier. Moreover, the classifier performance varied depending on the type of mother wavelet used for wavelet decomposition. Our quantitative results showed that CWTs can be used efficiently to extract important brain haemodynamic features at multiple frequencies if an appropriate mother wavelet function is chosen. The best classification results were obtained by a specific combination of input feature type and classifier.
Over the last two decades numerous functional imaging studies have shown that higher order cognitive functions are crucially dependent on the formation of distributed, large-scale neuronal assemblies (neurocognitive networks), often for very short durations. This has fueled the development of a vast number of functional connectivity measures that attempt to capture the spatiotemporal evolution of neurocognitive networks. Unfortunately, interpreting the neural basis of goal directed behavior using connectivity measures on neuroimaging data are highly dependent on the assumptions underlying the development of the measure, the nature of the task, and the modality of the neuroimaging technique that was used. This paper has two main purposes. The first is to provide an overview of some of the different measures of functional/effective connectivity that deal with high temporal resolution neuroimaging data. We will include some results that come from a recent approach that we have developed to identify the formation and extinction of task-specific, large-scale neuronal assemblies from electrophysiological recordings at a ms-by-ms temporal resolution. The second purpose of this paper is to indicate how to partially validate the interpretations drawn from this (or any other) connectivity technique by using simulated data from large-scale, neurobiologically realistic models. Specifically, we applied our recently developed method to realistic simulations of MEG data during a delayed match-to-sample (DMS) task condition and a passive viewing of stimuli condition using a large-scale neural model of the ventral visual processing pathway. Simulated MEG data using simple head models were generated from sources placed in V1, V4, IT, and prefrontal cortex (PFC) for the passive viewing condition. The results show how closely the conclusions obtained from the functional connectivity method match with what actually occurred at the neuronal network level.
Although developmental stuttering has been extensively studied with structural and task-based functional magnetic resonance imaging (fMRI), few studies have focused on resting-state brain activity in this disorder. We investigated resting-state brain activity of stuttering subjects by analyzing the amplitude of low-frequency fluctuation (ALFF), region of interest (ROI)-based functional connectivity (FC) and independent component analysis (ICA)-based FC. Forty-four adult males with developmental stuttering and 46 age-matched fluent male controls were scanned using resting-state fMRI. ALFF, ROI-based FCs and ICA-based FCs were compared between male stuttering subjects and fluent controls in a voxel-wise manner. Compared with fluent controls, stuttering subjects showed increased ALFF in left brain areas related to speech motor and auditory functions and bilateral prefrontal cortices related to cognitive control. However, stuttering subjects showed decreased ALFF in the left posterior language reception area and bilateral non-speech motor areas. ROI-based FC analysis revealed decreased FC between the posterior language area involved in the perception and decoding of sensory information and anterior brain area involved in the initiation of speech motor function, as well as increased FC within anterior or posterior speech- and language-associated areas and between the prefrontal areas and default-mode network (DMN) in stuttering subjects. ICA showed that stuttering subjects had decreased FC in the DMN and increased FC in the sensorimotor network. Our findings support the concept that stuttering subjects have deficits in multiple functional systems (motor, language, auditory and DMN) and in the connections between them.
In a recent study we found that multivariate pattern analysis (MVPA) of functional magnetic resonance imaging (fMRI) data could predict which of several touch-implying video clips a subject saw, only using voxels from primary somatosensory cortex. Here, we re-analyzed the same dataset using cross-individual MVPA to locate patterns of information that were common across participants' brains. In this procedure a classifier learned to distinguish the neural patterns evoked by each stimulus based on the data from a sub-group of the subjects and was then tested on data from an individual that was not part of that sub-group. We found prediction performance to be significantly above chance both when using voxels from the whole brain and when only using voxels from the postcentral gyrus. SVM voxel weight maps established based on the whole-brain analysis as well as a separate searchlight analysis suggested foci of especially high information content in medial and lateral occipital cortex and around the intraparietal sulcus. Classification across individuals appeared to rely on similar brain areas as classification within individuals. These data show that observing touch leads to stimulus-specific patterns of activity in sensorimotor networks and that these patterns are similar across individuals. More generally, the results suggest that cross-individual MVPA can succeed even when applied to restricted regions of interest.
Neurons in macaque primary motor cortex and dorsal premotor cortex are tuned to movement direction. In humans, neuronal populations tuned to movement direction have recently been described using multivoxel pattern analysis and functional magnetic resonance imaging adaptation. It is unclear, however, to what extent directionally tuned neuronal populations are sensitive to movement amplitude. Here we used functional magnetic resonance imaging adaptation to determine whether directionally tuned neuronal populations are modulated by movement amplitude. In different blocks, participants were adapted to small- or large-amplitude hand-reaching movements. On occasional test trials, we parametrically varied the angular difference between adaptation and test direction and the congruency between adapted and tested amplitude (same or different). We predicted that the blood oxygen level-dependent signal in directionally tuned regions should be adapted in proportion to the angular difference between adaptation and test direction. Directionally tuned regions insensitive to movement amplitude should show a transfer of adaptation from the adapted to the nonadapted amplitude. In contrast, regions sensitive to the specific combination of movement direction and amplitude should show directional tuning only for the adapted amplitude. We identified a network of parietal and frontal regions tuned to movement direction. We found that parietal areas contain neuronal populations sensitive to specific combinations of movement direction and amplitude, while frontal areas show transfer from the adapted to the nonadapted amplitude during small-amplitude movements after adaptation to large amplitude, but not vice versa. Our results thus imply different processing of movement amplitude in directionally tuned frontal and parietal areas.
Previous studies have shown the involvement of the ventrolateral prefrontal cortex (PFC) and the caudate nucleus when performing a set shift. However, the effect of set shifting on the frontostriatal activity observed during the later trials within a series of same-set classifications has yet to be determined. Here, young healthy adults underwent the functional magnetic resonance imaging while performing a card-sorting task in which the classification rule was provided prior to each trial. We observed a significant activation in the dorsolateral PFC, regardless of whether a set shift occurred or not. By contrast, the ventrolateral PFC and caudate nucleus showed an increased activity in both the shifting trials versus the control and in trials where the same rule was applied for a few trials before a set shift occurred, unlike trials where the same rule was applied for a longer period. Finally, decreased activity in the caudate nucleus correlated with an increasing trial position in trials where no set shift occurred, suggesting that the more a rule is executed, the better it is established. We argue that a new rule needs to be performed multiple times until the brain areas usually associated with the set shifting are no longer significantly required anymore.
The discovery of regions in the human brain (e.g., insula and cingulate cortex) that activate both under direct exposure to pain and when perceiving pain in others has been interpreted as a neural signature of empathy. However, this overlap raises the question of whether it may reflect a unique distributed population of bimodal neurons or, alternatively, the activity of intermingled but independent populations. We used fMRI on 28 female volunteers and used multivariate pattern analysis techniques to probe for more fine-grain spatial representations of seen and felt pain. Using a whole-brain approach, we found that only in the anterior insula (bilaterally) the distribution of cortical activity evoked by seeing another person's hand in pain was spatially similar to that of pain felt on one's own hand. Subsequent region of interest analyses also implicated the middle insula (right hemisphere) and the middle cingulate cortex. Furthermore, for the anterior insula, the spatial distribution of activity associated with one's pain also replicates that of the perception of negative but painless stimuli. Our data show how the neural representations of aversive events affecting oneself are also recruited when the same events affect others, and provide the stronger evidence thus far of a unique distributed cortical ensemble coding for aversive events regardless of the subject who is affected.
Behavioral and brain responses to identical stimuli can vary with experimental and task parameters, including the context of stimulus presentation or attention. More surprisingly, computational models suggest that noise-related random fluctuations in brain responses to stimuli would alone be sufficient to engender perceptual differences between physically identical stimuli. In two experiments combining psychophysics and EEG in healthy humans, we investigated brain mechanisms whereby identical stimuli are (erroneously) perceived as different (higher vs lower in pitch or longer vs shorter in duration) in the absence of any change in the experimental context. Even though, as expected, participants' percepts to identical stimuli varied randomly, a classification algorithm based on a mixture of Gaussians model (GMM) showed that there was sufficient information in single-trial EEG to reliably predict participants' judgments of the stimulus dimension. By contrasting electrical neuroimaging analyses of auditory evoked potentials (AEPs) to the identical stimuli as a function of participants' percepts, we identified the precise timing and neural correlates (strength vs topographic modulations) as well as intracranial sources of these erroneous perceptions. In both experiments, AEP differences first occurred ~100 ms after stimulus onset and were the result of topographic modulations following from changes in the configuration of active brain networks. Source estimations localized the origin of variations in perceived pitch of identical stimuli within right temporal and left frontal areas and of variations in perceived duration within right temporoparietal areas. We discuss our results in terms of providing neurophysiologic evidence for the contribution of random fluctuations in brain activity to conscious perception.
Musical emotions, such as happiness and sadness, have been investigated using instrumental music devoid of linguistic content. However, pop and rock, the most common musical genres, utilize lyrics for conveying emotions. Using participants' self-selected musical excerpts, we studied their behavior and brain responses to elucidate how lyrics interact with musical emotion processing, as reflected by emotion recognition and activation of limbic areas involved in affective experience. We extracted samples from subjects' selections of sad and happy pieces and sorted them according to the presence of lyrics. Acoustic feature analysis showed that music with lyrics differed from music without lyrics in spectral centroid, a feature related to perceptual brightness, whereas sad music with lyrics did not diverge from happy music without lyrics, indicating the role of other factors in emotion classification. Behavioral ratings revealed that happy music without lyrics induced stronger positive emotions than happy music with lyrics. We also acquired functional magnetic resonance imaging data while subjects performed affective tasks regarding the music. First, using ecological and acoustically variable stimuli, we broadened previous findings about the brain processing of musical emotions and of songs versus instrumental music. Additionally, contrasts between sad music with versus without lyrics recruited the parahippocampal gyrus, the amygdala, the claustrum, the putamen, the precentral gyrus, the medial and inferior frontal gyri (including Broca's area), and the auditory cortex, while the reverse contrast produced no activations. Happy music without lyrics activated structures of the limbic system and the right pars opercularis of the inferior frontal gyrus, whereas auditory regions alone responded to happy music with lyrics. These findings point to the role of acoustic cues for the experience of happiness in music and to the importance of lyrics for sad musical emotions.
Flexible, adaptive behavior is thought to rely on abstract rule representations within lateral prefrontal cortex (LPFC), yet it remains unclear how these representations provide such flexibility. We recently demonstrated that humans can learn complex novel tasks in seconds. Here we hypothesized that this impressive mental flexibility may be possible due to rapid transfer of practiced rule representations within LPFC to novel task contexts. We tested this hypothesis using functional MRI and multivariate pattern analysis, classifying LPFC activity patterns across 64 tasks. Classifiers trained to identify abstract rules based on practiced task activity patterns successfully generalized to novel tasks. This suggests humans can transfer practiced rule representations within LPFC to rapidly learn new tasks, facilitating cognitive performance in novel circumstances.
The functioning of neural systems supporting emotion processing and regulation in youth with bipolar disorder not otherwise specified (BP-NOS) remains poorly understood. We sought to examine patterns of activity and connectivity in youth with BP-NOS relative to youth with bipolar disorder type I (BP-I) and healthy controls (HC).
Our present understanding of the neural mechanisms and sensorimotor transformations that govern the planning of arm and eye movements predominantly come from invasive parieto-frontal neural recordings in nonhuman primates. While functional MRI (fMRI) has motivated investigations on much of these same issues in humans, the highly distributed and multiplexed organization of parieto-frontal neurons necessarily constrain the types of intention-related signals that can be detected with traditional fMRI analysis techniques. Here we employed multivoxel pattern analysis (MVPA), a multivariate technique sensitive to spatially distributed fMRI patterns, to provide a more detailed understanding of how hand and eye movement plans are coded in human parieto-frontal cortex. Subjects performed an event-related delayed movement task requiring that a reach or saccade be planned and executed toward one of two spatial target positions. We show with MVPA that, even in the absence of signal amplitude differences, the fMRI spatial activity patterns preceding movement onset are predictive of upcoming reaches and saccades and their intended directions. Within certain parieto-frontal regions we show that these predictive activity patterns reflect a similar spatial target representation for the hand and eye. Within some of the same regions, we further demonstrate that these preparatory spatial signals can be discriminated from nonspatial, effector-specific signals. In contrast to the largely graded effector- and direction-related planning responses found with fMRI subtraction methods, these results reveal considerable consensus with the parieto-frontal network organization suggested from primate neurophysiology and specifically show how predictive spatial and nonspatial movement information coexists within single human parieto-frontal areas.
Multivariate pattern analysis (MVPA) has recently received increasing attention in functional neuroimaging due to its ability to decode mental states from fMRI signals. However, questions remain regarding both the empirical and conceptual relationships between results from MVPA and standard univariate analyses. In the current study, whole-brain univariate and searchlight MVPAs of parametric manipulations of monetary gain and loss in a decision making task (Tom et al., 2007) were compared to identify the differences in the results across these methods and the implications for understanding the underlying mental processes. The MVPA and univariate results did identify some overlapping regions in whole brain analyses. However, an analysis of consistency revealed that in many regions the effect size estimates obtained from MVPA and univariate analysis were uncorrelated. Moreover, comparison of sensitivity showed a general trend towards greater sensitivity to task manipulations by MVPA compared to univariate analysis. These results demonstrate that MVPA methods may provide a different view of the functional organization of mental processing compared to univariate analysis, wherein MVPA is more sensitive to distributed coding of information whereas univariate analysis is more sensitive to global engagement in ongoing tasks. The results also highlight the need for better ways to integrate these methods.
The assessment of ictal consciousness has been the landmark criterion for the differentiation between simple and complex partial seizures over the last three decades. After review of the historical development of the concept of "complex partial seizure," the difficulties surrounding the simple versus complex dichotomy are addressed from theoretical, phenomenological, and neurophysiological standpoints. With respect to consciousness, careful analysis of ictal semiology shows that both the general level of vigilance and the specific contents of the conscious state can be selectively involved during partial seizures. Moreover, recent neuroimaging findings, coupled with classic electrophysiological studies, suggest that the neural substrate of ictal alterations of consciousness is twofold: focal hyperactivity in the limbic structures generates the complex psychic phenomena responsible for the altered contents of consciousness, and secondary disruption of the network involving the thalamus and the frontoparietal association cortices affects the level of awareness. These data, along with the localization information they provide, should be taken into account in the formulation of new criteria for the classification of seizures with focal onset.
The question of hemispheric lateralization of neural processes is one that is pertinent to a range of subdisciplines of cognitive neuroscience. Language is often assumed to be left-lateralized in the human brain, but there has been a long running debate about the underlying reasons for this. We addressed this problem with fMRI by identifying the neural responses to amplitude and spectral modulations in speech and how these interact with speech intelligibility to test previous claims for hemispheric asymmetries in acoustic and linguistic processes in speech perception. We used both univariate and multivariate analyses of the data, which enabled us to both identify the networks involved in processing these acoustic and linguistic factors and to test the significance of any apparent hemispheric asymmetries. We demonstrate bilateral activation of superior temporal cortex in response to speech-derived acoustic modulations in the absence of intelligibility. However, in a contrast of amplitude-modulated and spectrally modulated conditions that differed only in their intelligibility (where one was partially intelligible and the other unintelligible), we show a left dominant pattern of activation in STS, inferior frontal cortex, and insula. Crucially, multivariate pattern analysis showed that there were significant differences between the left and the right hemispheres only in the processing of intelligible speech. This result shows that the left hemisphere dominance in linguistic processing does not arise because of low-level, speech-derived acoustic factors and that multivariate pattern analysis provides a method for unbiased testing of hemispheric asymmetries in processing.
Stimulus repetition often leads to facilitated processing, resulting in neural decreases (repetition suppression) and faster RTs (repetition priming). Such repetition-related effects have been attributed to the facilitation of repeated cognitive processes and/or the retrieval of previously encoded stimulus-response (S-R) bindings. Although previous research has dissociated these two forms of learning, their interaction in the brain is not fully understood. Utilizing the spatial and temporal resolutions of fMRI and EEG, respectively, we examined a long-lag classification priming paradigm that required response repetitions or reversals at multiple levels of response representation. We found a repetition effect in occipital/temporal cortex (fMRI) that was time-locked to stimulus onset (EEG) and robust to switches in response, together with a repetition effect in inferior pFC (fMRI) that was time-locked to response onset (EEG) and sensitive to switches in response. The response-sensitive effect occurred even when changing from object names (words) to object pictures between repetitions, suggesting that S-R bindings can code abstract representations of stimuli. Most importantly, we found evidence for interference effects when incongruent S-R bindings were retrieved, with increased neural activity in inferior pFC, demonstrating that retrieval of S-R bindings can result in facilitation or interference, depending on the congruency of response between repetitions.
Inferotemporal cortex (IT) is believed to be directly involved in object processing and necessary for accurate and efficient object recognition. The frontal eye field (FEF) is an area in the primate prefrontal cortex that is involved in visual spatial selection and is thought to guide spatial attention and eye movements. We show that object-selective responses of IT neurons and behavioral performance are affected by changes in frontal eye field activity. This was found in monkeys performing a search classification task by temporarily inactivating subregions of FEF while simultaneously recording the activity from single neurons in IT. The effect on object selectivity and performance was specific, occurring in a predictable spatially dependent manner and was strongest when the IT neuron's preferred target was presented in the presence of distractors. FEF inactivation did not affect IT responses on trials in which the nonpreferred target was presented in the search array.
Numbers and space are two semantic primitives that interact with each other. Both recruit brain regions along the dorsal pathway, notably parietal cortex. This makes parietal cortex a candidate for the origin of numerical-spatial interaction. The underlying cognitive architecture of the interaction is still under scrutiny. Two classes of explanations can be distinguished. The early interaction approach assumes that numerical and spatial information are integrated into a single representation at a semantic level. A second approach postulates independent semantic representations. Only at the stage of response selection and preparation these two streams interact. In this study we used a numerical landmark task to identify the locus of the interaction between numbers and space. While lying in an MR scanner participants decided on the smaller of two numerical intervals in a visually presented number triplet. The spatial position of the middle number was varied; hence spatial intervals were congruent or incongruent with the numerical intervals. Responses in incongruent trials were slower and less accurate than in congruent trials. By combining across-vertex correlations (micro pattern) with a cluster analysis (macro pattern) we identified large-scale networks that were devoted to number processing, eye movements, and sensory-motor functions. Using support vector classification in different regions of interest along the intraparietal sulcus, the frontal eye fields, and supplementary motor area we were able to distinguish between congruent and incongruent trials in each of the networks. We suggest that the identified networks participate in the integration of numerical and spatial information and that the exclusive assumption of either an early or a late interaction between numerical and spatial information does not do justice to the complex interaction between both dimensions.
We used fMRI to identify brain areas that adapted to either animals or manipulable artifacts while participants classified highly-rendered color photographs into subcategories. Several key brain areas adapted more strongly to one class of objects compared to the other. Namely, we observed stronger adaptation for animals in the lingual gyrus bilaterally, which are known to analyze the color of objects, and in the right frontal operculum and in the anterior insular cortex bilaterally, which are known to process emotional content. In contrast, the left anterior intraparietal sulcus, which is important for configuring the hand to match the three-dimensional structure of objects during grasping, adapted more strongly to manipulable artifacts. Contrary to what a previous study has found using gray-scale photographs, we did not replicate categorical-specific adaptation in the lateral fusiform gyrus for animals and categorical-specific adaptation in the medial fusiform gyrus for manipulable artifacts. Both categories of objects adapted strongly in the fusiform gyrus without any clear preference in location along its medial-lateral axis. We think that this is because the fusiform gyrus has an important role to play in color processing and hence its responsiveness to color stimuli could be very different than its responsiveness to gray-scale photographs. Nevertheless, on the basis of what we found, we propose that the recognition and subsequent classification of animals may depend primarily on perceptual properties, such as their color, and on their emotional content whereas other factors, such as their function, may play a greater role for classifying manipulable artifacts.
How is working memory for different visual categories supported in the brain? Do the same principles of cortical specialization that govern the initial processing and encoding of visual stimuli also apply to their short-term maintenance? We investigated these questions with a delayed discrimination paradigm for faces, bodies, flowers, and scenes and applied both univariate and multivariate analyses to functional magnetic resonance imaging (fMRI) data. Activity during encoding followed the well-known specialization in posterior areas. During the delay interval, activity shifted to frontal and parietal regions but was not specialized for category. Conversely, activity in visual areas returned to baseline during that interval but showed some evidence of category specialization on multivariate pattern analysis (MVPA). We conclude that principles of cortical activation differ between encoding and maintenance of visual material. Whereas perceptual processes rely on specialized regions in occipitotemporal cortex, maintenance involves the activation of a frontoparietal network that seems to require little specialization at the category level. We also confirm previous findings that MVPA can extract information from fMRI signals in the absence of suprathreshold activation and that such signals from visual areas can reflect the material stored in memory.
Negotiation and trade typically require a mutual interaction while simultaneously resting in uncertainty which decision the partner ultimately will make at the end of the process. Assessing already during the negotiation in which direction one's counterpart tends would provide a tremendous advantage. Recently, neuroimaging techniques combined with multivariate pattern classification of the acquired data have made it possible to discriminate subjective states of mind on the basis of their neuronal activation signature. However, to enable an online-assessment of the participant's mind state both approaches need to be extended to a real-time technique. By combining real-time functional magnetic resonance imaging (fMRI) and online pattern classification techniques, we show that it is possible to predict human behavior during social interaction before the interacting partner communicates a specific decision. Average accuracy reached approximately 70% when we predicted online the decisions of volunteers playing the ultimatum game, a well-known paradigm in economic game theory. Our results demonstrate the successful online analysis of complex emotional and cognitive states using real-time fMRI, which will enable a major breakthrough for social fMRI by providing information about mental states of partners already during the mutual interaction. Interestingly, an additional whole brain classification across subjects confirmed the online results: anterior insula, ventral striatum, and lateral orbitofrontal cortex, known to act in emotional self-regulation and reward processing for adjustment of behavior, appeared to be strong determinants of later overt behavior in the ultimatum game. Using whole brain classification we were also able to discriminate between brain processes related to subjective emotional and motivational states and brain processes related to the evaluation of objective financial incentives.
The predicted reward of different behavioral options plays an important role in guiding decisions. Previous research has identified reward predictions in prefrontal and striatal brain regions. Moreover, it has been shown that the neural representation of a predicted reward is similar to the neural representation of the actual reward outcome. However, it has remained unknown how these representations emerge over the course of learning and how they relate to decision making. Here, we sought to investigate learning of predicted reward representations using functional magnetic resonance imaging and multivariate pattern classification. Using a pavlovian conditioning procedure, human subjects learned multiple novel cue-outcome associations in each scanning run. We demonstrate that across learning activity patterns in the orbitofrontal cortex, the dorsolateral prefrontal cortex (DLPFC), and the dorsal striatum, coding the value of predicted rewards become similar to the patterns coding the value of actual reward outcomes. Furthermore, we provide evidence that predicted reward representations in the striatum precede those in prefrontal regions and that representations in the DLPFC are linked to subsequent value-based choices. Our results show that different brain regions represent outcome predictions by eliciting the neural representation of the actual outcome. Furthermore, they suggest that reward predictions in the DLPFC are directly related to value-based choices.
Frontoparietal cortex is thought to be essential for flexible behavior, but the mechanism for control remains elusive. Here, we demonstrate a potentially critical property of this cortex: its dynamic configuration for coding of task-critical information. Using multivoxel pattern analysis of human functional imaging data, we demonstrate an adaptive change in the patterns of activation coding task-relevant stimulus distinctions. When task demands made perceptual information more difficult to discriminate, frontoparietal regions showed increased coding of this information. Visual cortices showed the opposite result: a weaker representation of perceptual information in line with the physical change in the stimulus. On a longer timescale, a rebalancing of coding was also seen after practice, with a diminished representation of task rules as they became familiar. The results suggest a flexible neural system, exerting cognitive control in a wide range of tasks by adaptively representing the task features most challenging for successful goal-directed behavior.
Reinforcements and punishments facilitate adaptive behavior in diverse domains ranging from perception to social interactions. A conventional approach to understanding the corresponding neural substrates focuses on the basal ganglia and its dopaminergic projections. Here, we show that reinforcement and punishment signals are surprisingly ubiquitous in the gray matter of nearly every subdivision of the human brain. Humans played either matching-pennies or rock-paper-scissors games against computerized opponents while being scanned using fMRI. Multivoxel pattern analysis was used to decode previous choices and their outcomes, and to predict upcoming choices. Whereas choices were decodable from a confined set of brain structures, their outcomes were decodable from nearly all cortical and subcortical structures. In addition, signals related to both reinforcements and punishments were recovered reliably in many areas and displayed patterns not consistent with salience-based explanations. Thus, reinforcement and punishment might play global modulatory roles in the entire brain.
An important requirement for vision is to identify interesting and relevant regions of the environment for further processing. Some models assume that salient locations from a visual scene are encoded in a dedicated spatial saliency map [1, 2]. Then, a winner-take-all (WTA) mechanism [1, 2] is often believed to threshold the graded saliency representation and identify the most salient position in the visual field. Here we aimed to assess whether neural representations of graded saliency and the subsequent WTA mechanism can be dissociated. We presented images of natural scenes while subjects were in a scanner performing a demanding fixation task, and thus their attention was directed away. Signals in early visual cortex and posterior intraparietal sulcus (IPS) correlated with graded saliency as defined by a computational saliency model. Multivariate pattern classification [3, 4] revealed that the most salient position in the visual field was encoded in anterior IPS and frontal eye fields (FEF), thus reflecting a potential WTA stage. Our results thus confirm that graded saliency and WTA-thresholded saliency are encoded in distinct neural structures. This could provide the neural representation required for rapid and automatic orientation toward salient events in natural environments.
Fluent readers process written text rapidly and accurately, and comprehend what they read. Historically, reading fluency has been modeled as the product of discrete skills such as single word decoding. More recent conceptualizations emphasize that fluent reading is the product of competency in, and the coordination of, multiple cognitive sub-skills (a multi-componential view). In this study, we examined how the pattern of activation in core reading regions changes as the ability to read fluently is manipulated through reading speed. We evaluated 13 right-handed adults with a novel fMRI task assessing fluent sentence reading and lower-order letter reading at each participant's normal fluent reading speed, as well as constrained (slowed) and accelerated reading speeds. Comparing fluent reading conditions with rest revealed regions including bilateral occipito-fusiform, left middle temporal, and inferior frontal gyral clusters across reading speeds. The selectivity of these regions' responses to fluent sentence reading was shown by comparison with the letter reading task. Region of interest analyses showed that at constrained and accelerated speeds these regions responded significantly more to fluent sentence reading. Critically, as reading speed increased, activation increased in a single reading-related region: occipital/fusiform cortex (left > right). These results demonstrate that while brain regions engaged in reading respond selectively during fluent reading, these regions respond differently as the ability to read fluently is manipulated. Implications for our understanding of reading fluency, reading development, and reading disorders are discussed.
Supramodal representation of emotion and its neural substrates have recently attracted attention as a marker of social cognition. However, the question whether perceptual integration of facial and vocal emotions takes place in primary sensory areas, multimodal cortices, or in affective structures remains unanswered yet. Using novel computer-generated stimuli, we combined emotional faces and voices in congruent and incongruent ways and assessed functional brain data (fMRI) during an emotional classification task. Both congruent and incongruent audiovisual stimuli evoked larger responses in thalamus and superior temporal regions compared with unimodal conditions. Congruent emotions were characterized by activation in amygdala, insula, ventral posterior cingulate (vPCC), temporo-occipital, and auditory cortices; incongruent emotions activated a frontoparietal network and bilateral caudate nucleus, indicating a greater processing load in working memory and emotion-encoding areas. The vPCC alone exhibited differential reactions to congruency and incongruency for all emotion categories and can thus be considered a central structure for supramodal representation of complex emotional information. Moreover, the left amygdala reflected supramodal representation of happy stimuli. These findings document that emotional information does not merge at the perceptual audiovisual integration level in unimodal or multimodal areas, but in vPCC and amygdala.
Successful encoding of episodic memories is thought to depend on contributions from prefrontal and temporal lobe structures. Neural processes that contribute to successful encoding have been extensively explored through univariate analyses of neuroimaging data that compare mean activity levels elicited during the encoding of events that are subsequently remembered vs. those subsequently forgotten. Here, we applied pattern classification to fMRI data to assess the degree to which distributed patterns of activity within prefrontal and temporal lobe structures elicited during the encoding of word-image pairs were diagnostic of the visual category (Face or Scene) of the encoded image. We then assessed whether representation of category information was predictive of subsequent memory. Classification analyses indicated that temporal lobe structures contained information robustly diagnostic of visual category. Information in prefrontal cortex was less diagnostic of visual category, but was nonetheless associated with highly reliable classifier-based evidence for category representation. Critically, trials associated with greater classifier-based estimates of category representation in temporal and prefrontal regions were associated with a higher probability of subsequent remembering. Finally, consideration of trial-by-trial variance in classifier-based measures of category representation revealed positive correlations between prefrontal and temporal lobe representations, with the strength of these correlations varying as a function of the category of image being encoded. Together, these results indicate that multi-voxel representations of encoded information can provide unique insights into how visual experiences are transformed into episodic memories.
Proactive interference (PI), in which irrelevant information from prior learning disrupts memory performance, is widely viewed as a major cause of forgetting. However, the hypothesized spontaneous recovery (i.e., automatic retrieval) of interfering information presumed to be at the base of PI remains to be demonstrated directly. Moreover, it remains unclear at what point during learning and/or retrieval interference impacts memory performance. In order to resolve these open questions, we employed a machine-learning algorithm to identify distributed patterns of brain activity associated with retrieval of interfering information that engenders PI and causes forgetting. Participants were scanned using functional magnetic resonance imaging during an item recognition task. We induced PI by constructing sets of three consecutive study lists from the same semantic category. The classifier quantified the magnitude of category-related activity at encoding and retrieval. Category-specific activity during retrieval increased across lists, consistent with the category information becoming increasingly available and producing interference. Critically, this increase was correlated with individual differences in forgetting and the deployment of frontal lobe mechanisms that resolve interference. Collectively, these findings suggest that distributed patterns of brain activity pertaining to the interfering information during retrieval contribute to forgetting. The prefrontal cortex mediates the relationship between the spontaneous recovery of interfering information at retrieval and individual differences in memory performance.
Humans and monkeys can learn to classify perceptual information in a statistically optimal fashion if the functional groupings remain stable over many hundreds of trials, but little is known about categorization when the environment changes rapidly. Here, we used a combination of computational modeling and functional neuroimaging to understand how humans classify visual stimuli drawn from categories whose mean and variance jumped unpredictably. Models based on optimal learning (Bayesian model) and a cognitive strategy (working memory model) both explained unique variance in choice, reaction time, and brain activity. However, the working memory model was the best predictor of performance in volatile environments, whereas statistically optimal performance emerged in periods of relative stability. Bayesian and working memory models predicted decision-related activity in distinct regions of the prefrontal cortex and midbrain. These findings suggest that perceptual category judgments, like value-guided choices, may be guided by multiple controllers.
Associative learning is a dynamic process that allows us to incorporate new knowledge within existing semantic networks. Even after years, a seemingly stable association can be altered by a single significant experience. Here, we investigate whether the acquisition of new associations affects the neural representation of stimuli and how the brain categorizes stimuli according to preexisting and emerging associations. Functional MRI data were collected during a differential fear conditioning procedure and at test (4-5 weeks later). Two pictures of faces and two pictures of houses served as stimuli. One of each pair coterminated with a shock in half of the trials (partial reinforcement). Applying Multivoxel Pattern Analysis (MVPA) in a trial-by-trial manner, we quantified changes in the similarity of neural representations of stimuli over the course of conditioning. Our findings show an increase in similarity of neural patterns throughout the cortex on consecutive trials of the reinforced stimuli. Furthermore, neural pattern similarity reveals a shift from original categories (faces/houses) toward new categories (reinforced/unreinforced) over the course of conditioning. This effect was differentially represented in the cortex, with visual areas primarily reflecting similarity of low-level stimulus properties (original categories) and frontal areas reflecting similarity of stimulus significance (new categories). Effects were not dependent on overall response amplitude and were still present during follow-up. We conclude that trial-by-trial MVPA is a useful tool for examining how the human brain encodes relevant associations and forms new associative networks.
Although much work has implicated the contributions of frontostriatal and medial temporal lobe (MTL) systems during probabilistic classification learning, the impact of emotion on these learning circuits is unknown. We used a modified version of the weather prediction task in which two participant groups were scanned with identical neutral cue cards probabilistically linked to either emotional (snake/spider) or neutral (mushroom/flower) outcomes. Owing to the differences in visual information shown as outcomes, analyses were restricted to the cue phase of the trials. Learning rates did not differ between the two groups, although the Emotional group was more likely to use complex strategies and to respond more slowly during initial learning. The Emotional group had reduced frontostriatal and MTL activation relative to the Neutral group, especially for participants who scored higher on snake/spider phobia questionnaires. Accurate performance was more tied to medial prefrontal activity in the Emotional group early in training, and to MTL activity in the Neutral group later in training. Trial-by-trial fluctuations in functional connectivity between the caudate and MTL were also reduced in the Emotional group compared to the Neutral group. Across groups, reaction time indexed a switch in learning systems, with faster trials mediated by the caudate and slower trials mediated by the MTL and frontal lobe. The extent to which the caudate was activated early in training predicted later performance improvements. These results reveal insights into how emotional outcomes modulate procedural learning systems, and the dynamics of MTL-striatal engagement across training trials.
Rules are widely used in everyday life to organize actions and thoughts in accordance with our internal goals. At the simplest level, single rules can be used to link individual sensory stimuli to their appropriate responses. However, most tasks are more complex and require the concurrent application of multiple rules. Experiments on humans and monkeys have shown the involvement of a frontoparietal network in rule representation. Yet, a fundamental issue still needs to be clarified: Is the neural representation of multiple rules compositional, that is, built on the neural representation of their simple constituent rules? Subjects were asked to remember and apply either simple or compound rules. Multivariate decoding analyses were applied to functional magnetic resonance imaging data. Both ventrolateral frontal and lateral parietal cortex were involved in compound representation. Most importantly, we were able to decode the compound rules by training classifiers only on the simple rules they were composed of. This shows that the code used to store rule information in prefrontal cortex is compositional. Compositional coding in rule representation suggests that it might be possible to decode other complex action plans by learning the neural patterns of the known composing elements.
Brain activity was monitored while participants viewed picture sets that reflected high or low levels of arousal and positive, neutral, or negative valence. Pictures within a set were presented rapidly in an incidental viewing task while fMRI data were collected. The primary purpose of the study was to determine if multi-voxel pattern analysis could be used within and between participants to predict valence, arousal and combined affective states elicited by pictures based on distributed patterns of whole brain activity. A secondary purpose was to determine if distributed patterns of whole brain activity can be used to derive a lower dimensional representation of affective states consistent with behavioral data. Results demonstrated above chance prediction of valence, arousal and affective states that was robust across a wide range of number of voxels used in prediction. Additionally, individual differences multidimensional scaling based on fMRI data clearly separated valence and arousal levels and was consistent with a circumplex model of affective states.
The posterior parietal cortex, including the medial superior parietal lobule (mSPL), becomes transiently more active during acts of cognitive control in a wide range of domains, including shifts of spatial and nonspatial visual attention, shifts between working memory representations, and shifts between categorization rules. Furthermore, spatial patterns of activity within mSPL, identified using multivoxel pattern analysis (MVPA), reliably distinguish between different acts of control. Here we describe a novel multivoxel pattern-based analysis that uses fluctuations in cognitive state over time to reveal inter-regional functional connectivity. First, we used MVPA to model patterns of activity in mSPL associated with shifting or maintaining spatial attention. We then computed a multivoxel pattern time course (MVPTC) that reflects, moment-by-moment, the degree to which the pattern of activity in mSPL more closely matches an attention-shift pattern or a sustained-attention pattern. We then entered the MVPTC as a regressor in a univariate (i.e., voxelwise) general linear model (GLM) to identify voxels whose BOLD activity covaried with the MVPTC. This analysis revealed several regions, including the striatum of the basal ganglia and bilateral middle frontal gyrus, whose activity was significantly correlated with the MVPTC in mSPL. For comparison, we also conducted a conventional functional connectivity analysis, entering the mean BOLD time course in mSPL as a regressor in a univariate GLM. The latter analysis revealed correlations in extensive regions of the frontal lobes but not in any subcortical area. The MVPTC analysis provides greater sensitivity (e.g., revealing the striatal-mSPL connectivity) and greater specificity (i.e., revealing more-focal clusters) than a conventional functional connectivity analysis. We discuss the broad applicability of MVPTC analysis to a variety of neuroimaging contexts.
Recently, we demonstrated using functional magnetic resonance imaging (fMRI) that the outcome of free decisions can be decoded from brain activity several seconds before reaching conscious awareness. Activity patterns in anterior frontopolar cortex (BA 10) were temporally the first to carry intention-related information and thus a candidate region for the unconscious generation of free decisions. In the present study, the original paradigm was replicated and multivariate pattern classification was applied to functional images of frontopolar cortex, acquired using ultra-high field fMRI at 7 Tesla. Here, we show that predictive activity patterns recorded before a decision was made became increasingly stable with increasing temporal proximity to the time point of the conscious decision. Furthermore, detailed questionnaires exploring subjects' thoughts before and during the decision confirmed that decisions were made spontaneously and subjects were unaware of the evolution of their decision outcomes. These results give further evidence that FPC stands at the top of the prefrontal executive hierarchy in the unconscious generation of free decisions.
To better define the underlying brain network for the decoding of emotional prosody, we recorded high-resolution brain scans during an implicit and explicit decoding task of angry and neutral prosody. Several subregions in the right superior temporal gyrus (STG) and bilateral in the inferior frontal gyrus (IFG) were sensitive to emotional prosody. Implicit processing of emotional prosody engaged regions in the posterior superior temporal gyrus (pSTG) and bilateral IFG subregions, whereas explicit processing relied more on mid STG, left IFG, amygdala, and subgenual anterior cingulate cortex. Furthermore, whereas some bilateral pSTG regions and the amygdala showed general sensitivity to prosody-specific acoustical features during implicit processing, activity in inferior frontal brain regions was insensitive to these features. Together, the data suggest a differentiated STG, IFG, and subcortical network of brain regions, which varies with the levels of processing and shows a higher specificity during explicit decoding of emotional prosody.
The ability to evaluate spontaneity in human behavior is called upon in the esthetic appreciation of dramatic arts and music. The current study addresses the behavioral and brain mechanisms that mediate the perception of spontaneity in music performance. In a functional magnetic resonance imaging experiment, 22 jazz musicians listened to piano melodies and judged whether they were improvised or imitated. Judgment accuracy (mean 55%; range 44-65%), which was low but above chance, was positively correlated with musical experience and empathy. Analysis of listeners' hemodynamic responses revealed that amygdala activation was stronger for improvisations than imitations. This activation correlated with the variability of performance timing and intensity (loudness) in the melodies, suggesting that the amygdala is involved in the detection of behavioral uncertainty. An analysis based on the subjective classification of melodies according to listeners' judgments revealed that a network including the pre-supplementary motor area, frontal operculum, and anterior insula was most strongly activated for melodies judged to be improvised. This may reflect the increased engagement of an action simulation network when melodic predictions are rendered challenging due to perceived instability in the performer's actions. Taken together, our results suggest that, while certain brain regions in skilled individuals may be generally sensitive to objective cues to spontaneity in human behavior, the ability to evaluate spontaneity accurately depends upon whether an individual's action-related experience and perspective taking skills enable faithful internal simulation of the given behavior.
How and where in the human brain high-level sensorimotor processes such as intentions and decisions are coded remain important yet essentially unanswered questions. This is in part because, to date, decoding intended actions from brain signals has been primarily constrained to invasive neural recordings in nonhuman primates. Here we demonstrate using functional MRI (fMRI) pattern recognition techniques that we can also decode movement intentions from human brain signals, specifically object-directed grasp and reach movements, moments before their initiation. Subjects performed an event-related delayed movement task toward a single centrally located object (consisting of a small cube attached atop a larger cube). For each trial, after visual presentation of the object, one of three hand movements was instructed: grasp the top cube, grasp the bottom cube, or reach to touch the side of the object (without preshaping the hand). We found that, despite an absence of fMRI signal amplitude differences between the planned movements, the spatial activity patterns in multiple parietal and premotor brain areas accurately predicted upcoming grasp and reach movements. Furthermore, the patterns of activity in a subset of these areas additionally predicted which of the two cubes were to be grasped. These findings offer new insights into the detailed movement information contained in human preparatory brain activity and advance our present understanding of sensorimotor planning processes through a unique description of parieto-frontal regions according to the specific types of hand movements they can predict.
The aim of this functional magnetic resonance imaging (fMRI) study was to identify human brain areas that are sensitive to the direction of auditory motion. Such directional sensitivity was assessed in a hypothesis-free manner by analyzing fMRI response patterns across the entire brain volume using a spherical-searchlight approach. In addition, we assessed directional sensitivity in three predefined brain areas that have been associated with auditory motion perception in previous neuroimaging studies. These were the primary auditory cortex, the planum temporale and the visual motion complex (hMT/V5+). Our whole-brain analysis revealed that the direction of sound-source movement could be decoded from fMRI response patterns in the right auditory cortex and in a high-level visual area located in the right lateral occipital cortex. Our region-of-interest-based analysis showed that the decoding of the direction of auditory motion was most reliable with activation patterns of the left and right planum temporale. Auditory motion direction could not be decoded from activation patterns in hMT/V5+. These findings provide further evidence for the planum temporale playing a central role in supporting auditory motion perception. In addition, our findings suggest a cross-modal transfer of directional information to high-level visual cortex in healthy humans.
While several studies have focused on identifying common brain mechanisms governing the decoding of emotional speech melody, interindividual variations in the cerebral processing of prosodic information, in comparison, have received only little attention to date: Albeit, for instance, differences in personality among individuals have been shown to modulate emotional brain responses, personality influences on the neural basis of prosody decoding have not been investigated systematically yet. Thus, the present study aimed at delineating relationships between interindividual differences in personality and hemodynamic responses evoked by emotional speech melody. To determine personality-dependent modulations of brain reactivity, fMRI activation patterns during the processing of emotional speech cues were acquired from 24 healthy volunteers and subsequently correlated with individual trait measures of extraversion and neuroticism obtained for each participant. Whereas correlation analysis did not indicate any link between brain activation and extraversion, strong positive correlations between measures of neuroticism and hemodynamic responses of the right amygdala, the left postcentral gyrus as well as medial frontal structures including the right anterior cingulate cortex emerged, suggesting that brain mechanisms mediating the decoding of emotional speech melody may vary depending on differences in neuroticism among individuals. Observed trait-specific modulations are discussed in the light of processing biases as well as differences in emotion control or task strategies which may be associated with the personality trait of neuroticism.
Decoding specific cognitive states from brain activity constitutes a major goal of neuroscience. Previous studies of brain-state classification have focused largely on decoding brief, discrete events and have required the timing of these events to be known. To date, methods for decoding more continuous and purely subject-driven cognitive states have not been available. Here, we demonstrate that free-streaming subject-driven cognitive states can be decoded using a novel whole-brain functional connectivity analysis. Ninety functional regions of interest (ROIs) were defined across 14 large-scale resting-state brain networks to generate a 3960 cell matrix reflecting whole-brain connectivity. We trained a classifier to identify specific patterns of whole-brain connectivity as subjects rested quietly, remembered the events of their day, subtracted numbers, or (silently) sang lyrics. In a leave-one-out cross-validation, the classifier identified these 4 cognitive states with 84% accuracy. More critically, the classifier achieved 85% accuracy when identifying these states in a second, independent cohort of subjects. Classification accuracy remained high with imaging runs as short as 30-60 s. At all temporal intervals assessed, the 90 functionally defined ROIs outperformed a set of 112 commonly used structural ROIs in classifying cognitive states. This approach should enable decoding a myriad of subject-driven cognitive states from brief imaging data samples.
Previous neuroimaging studies that have examined autobiographical memory specificity have utilized retrieval cues associated with prior searches of the event, potentially changing the retrieval processes being investigated. In the current study, musical cues were used to naturally elicit memories from multiple levels of specificity (i.e., lifetime period, general event, and event-specific). Sixteen young adults participated in a neuroimaging study in which they retrieved autobiographical memories associated with musical cues. These musical cues led to the retrieval of highly emotional memories that had low levels of prior retrieval. Retrieval of all autobiographical memory levels was associated with activity in regions in the autobiographical memory network, specifically the ventromedial prefrontal cortex, posterior cingulate, and right medial temporal lobe. Owing to the use of music, memories from varying levels of specificity were retrieved, allowing for comparison of event memory and abstract personal knowledge, as well as comparison of specific and general event memory. Dorsolateral and dorsomedial prefrontal regions were engaged during event retrieval relative to personal knowledge retrieval, and retrieval of specific event memories was associated with increased activity in the bilateral medial temporal lobe and dorsomedial prefrontal cortex relative to retrieval of general event memories. These results suggest that the initial search processes for memories of different specificity levels preferentially engage different components of the autobiographical memory network. The potential underlying causes of these neural differences are discussed.
The eyes provide important information for decoding the mental states of others. In this fMRI study we examined how reading the mind in the eyes develops across adolescence and we tested the developmental trajectories of brain regions involved in this basic perceptual mind-reading ability. Participants from three age groups (early adolescents, mid adolescents and young adults) participated in the study and performed an adapted version of the 'Reading the Mind in the Eyes task', in which photographs of the eye region of faces were presented. Behavioral results show that the ability to decode the feelings and thoughts of others from the eyes develops before early adolescence. For all ages, brain activity was found in the posterior superior temporal sulcus during reading the mind in the eyes relative to a control condition requiring age and gender judgments using the same eyes stimuli. Only early adolescents showed additional involvement of the medial prefrontal cortex, the inferior frontal gyrus and the temporal pole. The results are discussed in the light of recent findings on the development of the social brain network.
Emblematic (or symbolic) gestures allow individuals to convey a variety of thoughts and emotions ranging from approval to hostility. The use of such gestures involves the execution of a codified motor act by the addresser and its perception and decoding by the addressee. To examine underlying common and distinct neural correlates, we used fMRI tasks in which subjects viewed video clips of emblematic one-hand gestures. They were asked to (1) take the perspective of the addresser and imagine executing the gestures ("expression" condition), and to (2) take the perspective of the addressee and imagine being confronted with the gestures ("reception" condition). Common areas of activation were found in inferior frontal, medial frontal, and posterior temporal cortices with left-hemispheric predominance as well as in the cerebellum. The distinction between regions specifically involved in the expression or reception condition partly resembled the dorsal and ventral stream dichotomy of visual processing with junctions in inferior frontal and medial prefrontal cortices. Imagery of gesture expression involved the dorsal visual stream as well as higher-order motor areas. In contrast, gesture reception encompassed regions related to semantic processing, and medial prefrontal areas known to be involved in the process of understanding the intentions of others. In conclusion, our results provide evidence for a dissociation in representations of emblematic gesture processing between addresser and addressee in addition to shared components in language-related areas.
Studies using event related potentials have shown that men are more likely than women to rely on semantic cues when understanding emotional speech. In a previous functional Magnetic Resonance Imaging (fMRI) study, using an affective sentence classification task, we were able to separate areas involved in semantic processing and areas involved in the processing of affective prosody (Beaucousin et al., 2007). Here we searched for sex-related differences in the neural networks active during emotional speech processing in groups of men and women. The ortholinguistic abilities of the participants did not differ when evaluated with a large battery of tests. Although the neural networks engaged by men and women during emotional sentence classification were largely overlapping, sex-dependent modulations were detected during emotional sentence classification, but not during grammatical sentence classification. Greater activity was observed in men, compared with women, in inferior frontal cortical areas involved in emotional labeling and in attentional areas. In conclusion, at equivalent linguistic abilities and performances, men activate semantic and attentional cortical areas to a larger extent than women during emotional speech processing.
Remembering an event from the past is often complicated by the fact that our memories are cluttered with similar events. Though competition is a fundamental part of remembering, there is little evidence of how mnemonic competition is neurally represented. Here, we assessed whether competition between visual memories is captured in the relative degree to which target vs. competing memories are reactivated within the ventral occipitotemporal cortex (VOTC). To assess reactivation, we used multivoxel pattern analysis of fMRI data, quantifying the degree to which retrieval events elicited patterns of neural activity that matched those elicited during encoding. Consistent with recent evidence, we found that retrieval of visual memories was associated with robust VOTC reactivation and that the degree of reactivation scaled with behavioral expressions of target memory retrieval. Critically, competitive remembering was associated with more ambiguous patterns of VOTC reactivation, putatively reflecting simultaneous reactivation of target and competing memories. Indeed, the more weakly that target memories were reactivated, the more likely that competing memories were later remembered. Moreover, when VOTC reactivation indicated that conflict between target and competing memories was high, frontoparietal mechanisms were markedly engaged, revealing specific neural mechanisms that tracked competing mnemonic evidence. Together, these findings provide unique evidence that neural reactivation captures competition between individual memories, providing insight into how well target memories are retrieved in the present and how likely competing memories will be remembered in the future.
Human can flexibly attend to a variety of stimulus dimensions, including spatial location and various features such as color and direction of motion. Although the locus of spatial attention has been hypothesized to be represented by priority maps encoded in several dorsal frontal and parietal areas, it is unknown how the brain represents attended features. Here we examined the distribution and organization of neural signals related to deployment of feature-based attention. Subjects viewed a compound stimulus containing two superimposed motion directions (or colors) and were instructed to perform an attention-demanding task on one of the directions (or colors). We found elevated and sustained functional magnetic resonance imaging response for the attention task compared with a neutral condition, without reliable differences in overall response amplitude between attending to different features. However, using multivoxel pattern analysis, we were able to decode the attended feature in both early visual areas (primary visual cortex to human motion complex hMT+) and frontal and parietal areas (e.g., intraparietal sulcus areas IPS1-IPS4 and frontal eye fields) that are commonly associated with spatial attention. Furthermore, analysis of the classifier weight maps showed that attending to motion and color evoked different patterns of activity, suggesting that different neuronal subpopulations in these regions are recruited for attending to different feature dimensions. Thus, our finding suggests that, rather than a purely spatial representation of priority, frontal and parietal cortical areas also contain multiplexed signals related to the priority of different nonspatial features.
Rostrolateral prefrontal cortex (RLPFC) plays a key role in our ability to postpone the execution of intended behaviors until after another activity has been performed. However, it is poorly understood in computational terms. One crucial question is whether RLPFC represents the content of delayed intentions or plays a nonspecific role. In this human functional magnetic resonance imaging study (n = 32), RLPFC was active while participants stored delayed intentions during a distracting ongoing task. Multivariate analysis showed that the intended cue for future action and the intended behavior could be decoded from distinct posterior brain regions. However, the content of intentions could not be decoded from RLPFC itself. Functional connectivity analysis showed that RLPFC increased its coupling with content-representing regions during intention storage. Furthermore, trials with relatively high RLPFC activity were associated with enhanced decoding. Thus, RLPFC may enable realization of delayed intentions via interactions with posterior brain regions, which represent their content.
The rewarding attributes of foods containing fat are associated with the increase in fat consumption, but little is known of how the complex physical and chemical properties of orally ingested fats are represented and decoded in the brain nor how this impacts feeding behavior within the population. Here, functional MRI (fMRI) is used to assess the brain response to isoviscous, isosweet fat emulsions of increasing fat concentration and to investigate the correlation of behavioral and neuroimaging responses with taster status (TS). Cortical areas activated in response to fat, and those areas positively correlated with fat concentration, were identified. Significant responses that positively correlated with increasing fat concentration were found in the anterior insula, frontal operculum and secondary somatosensory cortex (SII), anterior cingulate cortex, and amygdala. Assessing the effect of TS revealed a strong correlation with self-reported preference of the samples and with cortical response in somatosensory areas [primary somatosensory cortex (SI), SII, and midinsula] and the primary taste area (anterior insula) and a trend in reward areas (amygdala and orbitofrontal cortex). This finding of a strong correlation with TS in somatosensory areas supports the theory of increased mechanosensory trigeminal innervation in high 6-n-propyl-2-thiouracil (PROP) tasters and has been linked to a higher risk of obesity. The interindividual differences in blood oxygenation level-dependent (BOLD) amplitude with TS indicates that segmenting populations by TS will reduce the heterogeneity of BOLD responses, improving signal detection power.
Music perception generally involves processing the frequency relationships between successive pitches and extraction of the melodic contour. Previous evidence has suggested that the 'ups' and 'downs' of melodic contour are categorically and automatically processed, but knowledge of the brain regions that discriminate different types of contour is limited. Here, we examined melodic contour discrimination using multivariate pattern analysis (MVPA) of fMRI data. Twelve non-musicians were presented with various ascending and descending melodic sequences while being scanned. Whole-brain MVPA was used to identify regions in which the local pattern of activity accurately discriminated between contour categories. We identified three distinct cortical loci: the right superior temporal sulcus (rSTS), the left inferior parietal lobule (lIPL), and the anterior cingulate cortex (ACC). These results complement previous findings of melodic processing within the rSTS, and extend our understanding of the way in which abstract auditory sequences are categorized by the human brain.
Organisms operate within both a perceptual domain of objects and events, and a mnemonic domain of past experiences and future goals. Each domain requires a deliberate selection of task-relevant information, through deployments of external (perceptual) and internal (mnemonic) attention, respectively. Little is known about the control of attention shifts in working memory, or whether voluntary control of attention in these two domains is subserved by a common or by distinct functional networks. We used human fMRI to examine the neural basis of cognitive control while participants shifted attention in vision and in working memory. We found that these acts of control recruit in common a subset of the dorsal fronto-parietal attentional control network, including the medial superior parietal lobule, intraparietal sulcus, and superior frontal sulcus/gyrus. Event-related multivoxel pattern classification reveals, however, that these regions exhibit distinct spatio-temporal patterns of neural activity during internal and external shifts of attention, respectively. These findings constrain theoretical accounts of selection in working memory and perception by showing that populations of neurons in dorsal fronto-parietal network regions exhibit selective tuning for acts of cognitive control in different cognitive domains.
Face perception in humans is mediated by activation in a network of brain areas. Conventional univariate fMRI data analysis has not localized differential responses to viewing male as compared with viewing female faces within this network. We tested whether we could detect neural response patterns specific to viewing male vs. female faces in 40 participants. Replicating earlier work, face stimuli evoked activation in the core (inferior occipital gyrus, IOG; fusiform gyrus, FG; and superior temporal sulcus, STS), as well as extended (amygdala, inferior frontal gyrus, IFG; insula, INS; and orbitofrontal cortex, OFC) regions of the face network. Multivariate pattern classification of activity within these regions revealed successful decoding of gender information, significantly above chance, in the IOG, FG, STS, IFG, INS, and OFC, but not in the amygdala. Multiple control regions indicated that this result might be restricted to face-responsive regions. Our findings suggest that gender information is distributed across the face network and is represented in the core regions that process invariant facial features, as well as the extended regions that process changeable aspects of faces.
To guide our behavior in successful ways, we often need to rely on information that is no longer in view, but maintained in visual short-term memory (VSTM). While VSTM is usually broken down into iconic memory (brief and high-capacity store) and visual working memory (sustained, yet limited-capacity store), recent studies have suggested the existence of an additional and intermediate form of VSTM that depends on activity in extrastriate cortex. In previous work, we have shown that this fragile form of VSTM can be dissociated from iconic memory. In the present study, we provide evidence that fragile VSTM is different from visual working memory as magnetic stimulation of the right dorsolateral prefrontal cortex (DLPFC) disrupts visual working memory, while leaving fragile VSTM intact. In addition, we observed that people with high DLPFC activity had superior working memory capacity compared to people with low DLPFC activity, and only people with high DLPFC activity really showed a reduction in working memory capacity in response to magnetic stimulation. Altogether, this study shows that VSTM consists of three stages that have clearly different characteristics and rely on different neural structures. On the methodological side, we show that it is possible to predict individual susceptibility to magnetic stimulation based on functional MRI activity.
Music and speech are complex sound streams with hierarchical rules of temporal organization that become elaborated over time. Here, we use functional magnetic resonance imaging to measure brain activity patterns in 20 right-handed nonmusicians as they listened to natural and temporally reordered musical and speech stimuli matched for familiarity, emotion, and valence. Heart rate variability and mean respiration rates were simultaneously measured and were found not to differ between musical and speech stimuli. Although the same manipulation of temporal structure elicited brain activation level differences of similar magnitude for both music and speech stimuli, multivariate classification analysis revealed distinct spatial patterns of brain responses in the 2 domains. Distributed neuronal populations that included the inferior frontal cortex, the posterior and anterior superior and middle temporal gyri, and the auditory brainstem classified temporal structure manipulations in music and speech with significant levels of accuracy. While agreeing with previous findings that music and speech processing share neural substrates, this work shows that temporal structure in the 2 domains is encoded differently, highlighting a fundamental dissimilarity in how the same neural resources are deployed.
A controversial question in cognitive neuroscience is whether comprehension of words and sentences engages brain mechanisms specific for decoding linguistic meaning or whether language comprehension occurs through more domain-general sensorimotor processes. Accumulating behavioral and neuroimaging evidence suggests a role for cortical motor and premotor areas in passive action-related language tasks, regions that are known to be involved in action execution and observation. To examine the involvement of these brain regions in language and nonlanguage tasks, we used functional magnetic resonance imaging (fMRI) on a group of 21 healthy adults. During the fMRI session, all participants 1) watched short object-related action movies, 2) looked at pictures of man-made objects, and 3) listened to and produced short sentences describing object-related actions and man-made objects. Our results are among the first to reveal, in the human brain, a functional specialization within the ventral premotor cortex (PMv) for observing actions and for observing objects, and a different organization for processing sentences describing actions and objects. These findings argue against the strongest version of the simulation theory for the processing of action-related language.
The degree to which the MTL system contributes to effective language skills is not well delineated. We sought to determine if the MTL plays a role in single-word decoding in healthy, normal skilled readers. The experiment follows from the implications of the dual-process model of single-word decoding, which provides distinct predictions about the nature of MTL involvement. The paradigm utilized word (regular and irregularly spelled words) and pseudoword (phonetically regular) stimuli that differed in their demand for non-lexical as opposed lexical decoding. The data clearly showed that the MTL system was not involved in single word decoding in skilled, native English readers. Neither the hippocampus nor the MTL system as a whole showed significant activation during lexical or non-lexical based decoding. The results provide evidence that lexical and non-lexical decoding are implemented by distinct but overlapping neuroanatomical networks. Non-lexical decoding appeared most uniquely associated with cuneus and fusiform gyrus activation biased toward the left hemisphere. In contrast, lexical decoding appeared associated with right middle frontal and supramarginal, and bilateral cerebellar activation. Both these decoding operations appeared in the context of a shared widespread network of activations including bilateral occipital cortex and superior frontal regions. These activations suggest that the absence of MTL involvement in either lexical or non-lexical decoding appears likely a function of the skilled reading ability of our sample such that whole-word recognition and retrieval processes do not utilize the declarative memory system, in the case of lexical decoding, and require only minimal analysis and recombination of the phonetic elements of a word, in the case of non-lexical decoding.
Fronto-striatal circuits in set-shifting have been examined in neuroimaging studies using the Wisconsin Card Sorting Task (WCST) that requires changing the classification rule for cards containing visual stimuli that differ in color, shape, and number. The present study examined whether this fronto-striatal contribution to the planning and execution of set-shifts is similar in a modified sorting task in which lexical rules are applied to word stimuli. Young healthy adults were scanned with functional magnetic resonance imaging while performing the newly developed lexical version of the WCST: the Wisconsin Word Sorting Task. Significant activation was found in a cortico-striatal loop that includes area 47/12 of the ventrolateral prefrontal cortex (PFC), and the caudate nucleus during the planning of a set-shift, and in another that includes the posterior PFC and the putamen during the execution of a set-shift. However, in the present lexical task, additional activation peaks were observed in area 45 of the ventrolateral PFC area during both matching periods. These results provide evidence that the functional contributions of the various fronto-striatal loops are not dependent on the modality of the information to be manipulated but rather on the specific executive processes required.
Repeated study improves memory, but the underlying neural mechanisms of this improvement are not well understood. Using functional magnetic resonance imaging and representational similarity analysis of brain activity, we found that, compared with forgotten items, subsequently remembered faces and words showed greater similarity in neural activation across multiple study in many brain regions, including (but not limited to) the regions whose mean activities were correlated with subsequent memory. This result addresses a longstanding debate in the study of memory by showing that successful episodic memory encoding occurs when the same neural representations are more precisely reactivated across study episodes, rather than when patterns of activation are more variable across time.
Basic emotional states (such as anger, fear, and joy) can be similarly conveyed by the face, the body, and the voice. Are there human brain regions that represent these emotional mental states regardless of the sensory cues from which they are perceived? To address this question, in the present study participants evaluated the intensity of emotions perceived from face movements, body movements, or vocal intonations, while their brain activity was measured with functional magnetic resonance imaging (fMRI). Using multivoxel pattern analysis, we compared the similarity of response patterns across modalities to test for brain regions in which emotion-specific patterns in one modality (e.g., faces) could predict emotion-specific patterns in another modality (e.g., bodies). A whole-brain searchlight analysis revealed modality-independent but emotion category-specific activity patterns in medial prefrontal cortex (MPFC) and left superior temporal sulcus (STS). Multivoxel patterns in these regions contained information about the category of the perceived emotions (anger, disgust, fear, happiness, sadness) across all modality comparisons (face-body, face-voice, body-voice), and independently of the perceived intensity of the emotions. No systematic emotion-related differences were observed in the overall amplitude of activation in MPFC or STS. These results reveal supramodal representations of emotions in high-level brain areas previously implicated in affective processing, mental state attribution, and theory-of-mind. We suggest that MPFC and STS represent perceived emotions at an abstract, modality-independent level, and thus play a key role in the understanding and categorization of others' emotional mental states.
Most juridical systems recognize intentional non-actions - the failure to render assistance - as intentional acts by regarding them as in principle culpable. This raises the fundamental question whether intentional non-actions can be distinguished from simply not doing anything. Classical GLM analysis on functional magnetic resonance imaging (fMRI) data reveals that not doing anything is associated with resting state brain areas whereas intentionally non-acting is associated with brain activity in left inferior parietal lobe and left dorsal premotor cortex. By means of pattern classification we quantify the accuracy with which we can distinguish these two mental states on the basis of brain activity. In order to identify brain regions that harbour a distributed, overlapping representation of voluntary non-actions and the decision not to act we performed pattern classification on brain areas that did not appear in the GLM contrasts. The prediction rate is not reduced and we show that the prediction relies mostly on brain areas that have been associated with action production and motor imagery as supplementary motor area, right inferior frontal gyrus and right middle temporal area (V5/MT). Hence our data support the implicit assumption of legal practice that voluntary non-action shares important features with overt voluntary action.
The formation of new perceptual categories involves learning to extract that information from a wide range of often noisy sensory inputs, which is critical for selecting between a limited number of responses. To identify brain regions involved in visual classification learning under noisy conditions, we developed a task on the basis of the classical dot pattern prototype distortion task [M. I. Posner, Journal of Experimental Psychology, 68, 113-118, 1964]. Twenty-seven healthy young adults were required to assign distorted patterns of dots into one of two categories, each defined by its prototype. Categorization uncertainty was modulated parametrically by means of Shannon's entropy formula and set to the levels of 3, 7, and 8.5 bits/dot within subsets of the stimuli. Feedback was presented after each trial, and two parallel versions of the task were developed to contrast practiced and unpracticed performance within a single session. Using event-related fMRI, areas showing increasing activation with categorization uncertainty and decreasing activation with training were identified. Both networks largely overlapped and included areas involved in visuospatial processing (inferior temporal and posterior parietal areas), areas involved in cognitive processes requiring a high amount of cognitive control (posterior medial wall), and a cortico-striatal-thalamic loop through the body of the caudate nucleus. Activity in the medial prefrontal wall was increased when subjects received negative as compared with positive feedback, providing further evidence for its important role in mediating the error signal. This study characterizes the cortico-striatal network underlying the classification of distorted visual patterns that is directly related to decision uncertainty.
Previous research on the superior temporal sulcus (STS) has shown that it responds more to facial expressions than to neutral faces. Here, we extend our understanding of the STS in two ways. First, using targeted high-resolution fMRI measurements of the lateral cortex and multivoxel pattern analysis, we show that the response to seven categories of dynamic facial expressions can be decoded in both the posterior STS (pSTS) and anterior STS (aSTS). We were also able to decode patterns corresponding to these expressions in the frontal operculum (FO), a structure that has also been shown to respond to facial expressions. Second, we measured the similarity structure of these representations and found that the similarity structure in the pSTS significantly correlated with the perceptual similarity structure of the expressions. This was the case regardless of whether we used pattern classification or more traditional correlation techniques to extract the neural similarity structure. These results suggest that distributed representations in the pSTS could underlie the perception of facial expressions.
Language consists of sequences of words, but comprehending phrases involves more than concatenating meanings: A boat house is a shelter for boats, whereas a summer house is a house used during summer, and a ghost house is typically uninhabited. Little is known about the brain bases of combinatorial semantic processes. We performed two fMRI experiments using familiar, highly meaningful phrases (lake house) and unfamiliar phrases with minimal meaning created by reversing the word order of the familiar items (house lake). The first experiment used a 1-back matching task to assess implicit semantic processing, and the second used a classification task to engage explicit semantic processing. These conditions required processing of the same words, but with more effective combinatorial processing in the meaningful condition. The contrast of meaningful versus reversed phrases revealed activation primarily during the classification task, to a greater extent in the right hemisphere, including right angular gyrus, dorsomedial prefrontal cortex, and bilateral posterior cingulate/precuneus, areas previously implicated in semantic processing. Positive correlations of fMRI signal with lexical (word-level) frequency occurred exclusively with the 1-back task and to a greater spatial extent on the left, including left posterior middle temporal gyrus and bilateral parahippocampus. These results reveal strong effects of task demands on engagement of lexical versus combinatorial processing and suggest a hemispheric dissociation between these levels of semantic representation.
Previous work has implicated prefrontal cortices in selecting among and retrieving conceptual information stored elsewhere. However, recent neurophysiological work in monkeys suggests that prefrontal cortex may play a more direct role in representing conceptual information in a flexible context-specific manner. Here, we investigate the nature of visual object representations from perceptual to conceptual levels in an unbiased data-driven manner using a functional magnetic resonance imaging adaptation paradigm with pictures of animals. Throughout much of occipital cortex, activity was highly sensitive to changes in 2D stimulus form, consistent with tuning to form and position within retinotopic coordinates and matching an automated measure of shape similarity. Broad superordinate conceptual information was represented as early as extrastriate and posterior ventral temporal cortex. These regions were not completely invariant to form, suggesting that form similarity remains an important organizational constraint into the temporal cortex. Separate sites within prefrontal cortex represented broad and narrow conceptual tuning, with more anterior sites tuned narrowly to close conceptual associates in a manner that was invariant to stimulus form/position and that matched independent similarity ratings of the stimuli. The combination of broad and narrow conceptual tuning within prefrontal cortex may support flexible selection, retrieval, and classification of objects at different levels of categorical abstraction.
Using functional MRI we examined the task-dependency of brain activation patterns evoked by vibrotactile stimulation. For this purpose, we measured activations after identical stimulation of the fingers of the right hand in three different task conditions: passive attention, localization of the vibrations, and discrimination of temporal noise within the vibrations. Further, we investigated whether, regardless of task demands, the characteristics of the vibrations - periodic versus noisy - had an effect on brain topography. Vibrotactile processing was associated with activation in a variety of cortical areas including contralateral primary somatosensory cortex (SI), bilateral posterior parietal cortex, parietal operculum (second somatosensory cortex, SII), insula, and superior temporal gyrus, as well as ipsilateral middle temporal gyrus, precentral, and middle frontal gyrus. However, identical stimuli evoked different brain activity patterns in different task conditions: significantly stronger activity in the hand representation of SI was found for stimulus localization than for noise detection. In contrast, significantly higher activation for noise detection than for finger localization was found in the thalamus. Activation tended to be lower for noisy stimuli in both hemispheres. Significant stimulus-related differences, however, could be found only in the contralateral postcentral and parietal cortex, particularly during noise discrimination. In summary, in response to vibrotactile stimulation, the level of activation in processing circuits ranging across thalamus and many cortical regions is dictated by the perceptual operation carried out on the vibration. We speculate that different nodes in the network carry signals that can be optimally decoded for either spatial or temporal information and that the degree of activation reflects those nodes' relative contributions to the decoding process.
Many lines of evidence point to a tight linkage between the perceptual and motoric representations of actions. Numerous demonstrations show how the visual perception of an action engages compatible activity in the observer's motor system. This is seen for both intransitive actions (e.g., in the case of unconscious postural imitation) and transitive actions (e.g., grasping an object). Although the discovery of "mirror neurons" in macaques has inspired explanations of these processes in human action behaviors, the evidence for areas in the human brain that similarly form a crossmodal visual/motor representation of actions remains incomplete. To address this, in the present study, participants performed and observed hand actions while being scanned with functional MRI. We took a data-driven approach by applying whole-brain information mapping using a multivoxel pattern analysis (MVPA) classifier, performed on reconstructed representations of the cortical surface. The aim was to identify regions in which local voxelwise patterns of activity can distinguish among different actions, across the visual and motor domains. Experiment 1 tested intransitive, meaningless hand movements, whereas experiment 2 tested object-directed actions (all right-handed). Our analyses of both experiments revealed crossmodal action regions in the lateral occipitotemporal cortex (bilaterally) and in the left postcentral gyrus/anterior parietal cortex. Furthermore, in experiment 2 we identified a gradient of bias in the patterns of information in the left hemisphere postcentral/parietal region. The postcentral gyrus carried more information about the effectors used to carry out the action (fingers vs. whole hand), whereas anterior parietal regions carried more information about the goal of the action (lift vs. punch). Taken together, these results provide evidence for common neural coding in these areas of the visual and motor aspects of actions, and demonstrate further how MVPA can contribute to our understanding of the nature of distributed neural representations.
Imagine you are standing at a street with heavy traffic watching someone on the other side of the road. Do you think your brain is implicitly registering your willingness to buy any of the cars passing by outside your focus of attention? To address this question, we measured brain responses to consumer products (cars) in two experimental groups using functional magnetic resonance imaging. Participants in the first group (high attention) were instructed to closely attend to the products and to rate their attractiveness. Participants in the second group (low attention) were distracted from products and their attention was directed elsewhere. After scanning, participants were asked to state their willingness to buy each product. During the acquisition of neural data, participants were not aware that consumer choices regarding these cars would subsequently be required. Multivariate decoding was then applied to assess the choice-related predictive information encoded in the brain during product exposure in both conditions. Distributed activation patterns in the insula and the medial prefrontal cortex were found to reliably encode subsequent choices in both the high and the low attention group. Importantly, consumer choices could be predicted equally well in the low attention as in the high attention group. This suggests that neural evaluation of products and associated choice-related processing does not necessarily depend on attentional processing of available items. Overall, the present findings emphasize the potential of implicit, automatic processes in guiding even important and complex decisions.
Transmit and receive RF coil arrays have proven to be particularly beneficial for ultra-high-field MR. Transmit coil arrays enable such techniques as B(1) (+) shimming to substantially improve transmit B(1) homogeneity compared to conventional volume coil designs, and receive coil arrays offer enhanced parallel imaging performance and SNR. Concentric coil arrangements hold promise for developing transceiver arrays incorporating large numbers of coil elements. At magnetic field strengths of 7 tesla and higher where the Larmor frequencies of interest can exceed 300 MHz, the coil array design must also overcome the problem of the coil conductor length approaching the RF wavelength. In this study, a novel concentric arrangement of resonance elements built from capacitively-shortened half-wavelength transmission lines is presented. This approach was utilized to construct an array with whole-brain coverage using 16 transceiver elements and 16 receive-only elements, resulting in a coil with a total of 16 transmit and 32 receive channels.
In everyday life, successful decision making requires precise representations of expected values. However, for most behavioral options more than one attribute can be relevant in order to predict the expected reward. Thus, to make good or even optimal choices the reward predictions of multiple attributes need to be integrated into a combined expected value. Importantly, the individual attributes of such multi-attribute objects can agree or disagree in their reward prediction. Here we address where the brain encodes the combined reward prediction (averaged across attributes) and where it encodes the variability of the value predictions of the individual attributes. We acquired fMRI data while subjects performed a task in which they had to integrate reward predictions from multiple attributes into a combined value. Using time-resolved pattern recognition techniques (support vector regression) we find that (1) the combined value is encoded in distributed fMRI patterns in the ventromedial prefrontal cortex (vmPFC) and that (2) the variability of value predictions of the individual attributes is encoded in the dorsolateral prefrontal cortex (dlPFC). The combined value could be used to guide choices, whereas the variability of the value predictions of individual attributes indicates an ambiguity that results in an increased difficulty of the value-integration. These results demonstrate that the different features defining multi-attribute objects are encoded in non-overlapping brain regions and therefore suggest different roles for vmPFC and dlPFC in multi-attribute decision making.
Decision neuroscience suggests that there exists a core network for the subjective valuation of rewards from a range of different domains, encompassing the ventral striatum and regions of the orbitofrontal cortex (OFC), in particular the ventromedial aspect of the OFC. Here we first review ways to measure subjective value experimentally in a cognitive neuroscience context, and provide a brief overview over different types of value (outcome, goal and decision value). We then compare results of functional neuroimaging studies of subjective value representations across these different types of value. Our analysis suggests that the same region of the mOFC represents the outcome values of primary reinforcers, but also more complex decision values in which multiple dimensions of the reward need to be integrated. The subjective (hedonic) experience of processing highly valued decision options (regardless of whether they refer to actually experienced rewards or merely potential future rewards) appears to be what is reflected in value-related mOFC activity.
In human functional magnetic resonance imaging (fMRI), a characteristic pattern of frontal and parietal activity is produced by many different cognitive demands. Although frontoparietal cortex has been shown to represent a variety of task features in different contexts, little is known about detailed representation of different task features within and across different regions. We used multi-voxel pattern analysis (MVPA) of human fMRI data to assess the representational content of frontoparietal cortex in a simple stimulus-response task. Stimulus-response mapping rule was the most strongly represented task feature, significantly coded in a lateral frontal region surrounding the inferior frontal sulcus, a more ventral region including the anterior insula/frontal operculum, and the intraparietal sulcus. Next strongest was coding of the instruction cue (screen color) indicating which rule should be applied. Coding of individual stimuli and responses was weaker, approaching significance in a subset of regions. In line with recent single unit data, the results show a broad representation of task-relevant information across human frontoparietal cortex, with strong representation of a general rule or cognitive context, and weaker coding of individual stimulus/response instances.
Semantic priming, a well-established technique to study conceptual representation, has thus far produced variable fMRI results, both regarding the type of priming effects and their correlation with brain activation. The aims of the current study were (a) to investigate two types of semantic relations--categorical versus associative--under controlled processing conditions and (b) to investigate whether categorical and associative relations between words are correlated with response enhancement or response suppression. We used fMRI to examine neural correlates of semantic priming as subjects performed a lexical decision task with a long SOA (800 msec). Four experimental conditions were compared: categorically related trials (couch-bed), associatively related trials (couch-pillow), unrelated trials (couch-bridge), and nonword trials (couch-sibor). We found similar behavioral priming effects for both categorically and associatively related pairs. However, the neural priming effects differed: Categorically related pairs resulted in a neural suppression effect in the right MFG, whereas associatively related pairs resulted in response enhancement in the left IFG. A direct contrast between them revealed activation for categorically related trials in the right insular lobe. We conclude that perceptual and functional similarity of categorically related words may lead to response suppression within right-lateralized frontal regions that represent more retrieval effort and the recruitment of a broader semantic field. Associatively related pairs that require a different processing of the related target compared to the prime may lead to the response enhancement within left inferior frontal regions. Nevertheless, the differences between associative and categorical relations might be parametrical rather than absolutely distinct as both relationships recruit similar regions to a different degree.
Analyzing distributed patterns of brain activation using multivariate pattern analysis (MVPA) has become a popular approach for using functional magnetic resonance imaging (fMRI) data to predict mental states. While the majority of studies currently build separate classifiers for each participant in the sample, in principle a single classifier can be derived from and tested on data from all participants. These two approaches, within- and cross-participant classification, rely on potentially different sources of variability and thus may provide distinct information about brain function. Here, we used both approaches to identify brain regions that contain information about passively received monetary rewards (i.e., images of currency that influenced participant payment) and social rewards (i.e., images of human faces). Our within-participant analyses implicated regions in the ventral visual processing stream-including fusiform gyrus and primary visual cortex-and ventromedial prefrontal cortex (VMPFC). Two key results indicate these regions may contain statistically discriminable patterns that contain different informational representations. First, cross-participant analyses implicated additional brain regions, including striatum and anterior insula. The cross-participant analyses also revealed systematic changes in predictive power across brain regions, with the pattern of change consistent with the functional properties of regions. Second, individual differences in classifier performance in VMPFC were related to individual differences in preferences between our two reward modalities. We interpret these results as reflecting a distinction between patterns showing participant-specific functional organization and those indicating aspects of brain organization that generalize across individuals.
Perceptual categorization is a fundamental cognitive process that gives meaning to an often graded sensory environment. Previous research has subdivided the visual pathway into posterior regions that processes the physical properties of a stimulus, and frontal regions that process more abstract properties such as category information. The superior temporal sulcus (STS) is known to be involved in face and emotion perception, but the nature of its processing remains unknown. Here, we used targeted fMRI measurements of the STS to investigate whether its representations of facial expressions are categorical or noncategorical. Multivoxel pattern analysis showed that even though subjects were performing a categorization task, the left STS contained graded, noncategorical representations. In the right STS, representations showed evidence for both stimulus-related gradations and a categorical boundary.
An optimal choice among alternative behavioral options requires precise anticipatory representations of their possible outcomes. A fundamental question is how such anticipated outcomes are represented in the brain. Reward coding at the level of single cells in the orbitofrontal cortex (OFC) follows a more heterogeneous coding scheme than suggested by studies using functional MRI (fMRI) in humans. Using a combination of multivariate pattern classification and fMRI we show that the reward value of sensory cues can be decoded from distributed fMRI patterns in the OFC. This distributed representation is compatible with previous reports from animal electrophysiology that show that reward is encoded by different neural populations with opposing coding schemes. Importantly, the fMRI patterns representing specific values during anticipation are similar to those that emerge during the receipt of reward. Furthermore, we show that the degree of this coding similarity is related to subjects' ability to use value information to guide behavior. These findings narrow the gap between reward coding in humans and animals and corroborate the notion that value representations in OFC are independent of whether reward is anticipated or actually received.
We examined the influence of emotional valence and type of item to be remembered on brain activity during recognition, using faces and scenes. We used multivariate analyses of event-related fMRI data to identify whole-brain patterns, or networks of activity. Participants demonstrated better recognition for scenes vs faces and for negative vs neutral and positive items. Activity was increased in extrastriate cortex and inferior frontal gyri for emotional scenes, relative to neutral scenes and all face types. Increased activity in these regions also was seen for negative faces relative to positive faces. Correct recognition of negative faces and scenes (hits vs correct rejections) was associated with increased activity in amygdala, hippocampus, extrastriate, frontal and parietal cortices. Activity specific to correctly recognized emotional faces, but not scenes, was found in sensorimotor areas and rostral prefrontal cortex. These results suggest that emotional valence and type of visual stimulus both modulate brain activity at recognition, and influence multiple networks mediating visual, memory and emotion processing. The contextual information in emotional scenes may facilitate memory via additional visual processing, whereas memory for emotional faces may rely more on cognitive control mediated by rostrolateral prefrontal regions.
Multivariate pattern recognition methods are increasingly being used to identify multiregional brain activity patterns that collectively discriminate one cognitive condition or experimental group from another, using fMRI data. The performance of these methods is often limited because the number of regions considered in the analysis of fMRI data is large compared to the number of observations (trials or participants). Existing methods that aim to tackle this dimensionality problem are less than optimal because they either over-fit the data or are computationally intractable. Here, we describe a novel method based on logistic regression using a combination of L1 and L2 norm regularization that more accurately estimates discriminative brain regions across multiple conditions or groups. The L1 norm, computed using a fast estimation procedure, ensures a fast, sparse and generalizable solution; the L2 norm ensures that correlated brain regions are included in the resulting solution, a critical aspect of fMRI data analysis often overlooked by existing methods. We first evaluate the performance of our method on simulated data and then examine its effectiveness in discriminating between well-matched music and speech stimuli. We also compared our procedures with other methods which use either L1-norm regularization alone or support vector machine-based feature elimination. On simulated data, our methods performed significantly better than existing methods across a wide range of contrast-to-noise ratios and feature prevalence rates. On experimental fMRI data, our methods were more effective in selectively isolating a distributed fronto-temporal network that distinguished between brain regions known to be involved in speech and music processing. These findings suggest that our method is not only computationally efficient, but it also achieves the twin objectives of identifying relevant discriminative brain regions and accurately classifying fMRI data.
Discrimination tasks require processing, interpreting, and linking sensory information to the appropriate motor response. We report that neurons in prefrontal cortex (PFC) represent visual motion with precision comparable to cortical neurons at early stages of motion processing, and readily adapt this representation to behavioral context. We found that direction selectivity, recorded while the monkeys discriminated directions, decreased when they judged motion speed and ignored its direction. This decrease was more pronounced in neurons classified as narrow-spiking (NS) putative interneurons than in broad-spiking (BS) putative pyramidal neurons. However, during passive fixation, when the link between motion and its behavioral relevance was removed, both cell types showed a severe selectivity loss. Our results show that flexible sensory representation during active discrimination tasks is achieved in the PFC by a specialized neuronal network of both NS neurons readily adjusting their selectivity to behavioral context, and BS neurons capable of maintaining relatively stable sensory representation.
Artificial grammar learning constitutes a well-established model for the acquisition of grammatical knowledge in a natural setting. Previous neuroimaging studies demonstrated that Broca's area (left BA 44/45) is similarly activated by natural syntactic processing and artificial grammar learning. The current study was conducted to investigate the causal relationship between Broca's area and learning of an artificial grammar by means of transcranial direct current stimulation (tDCS). Thirty-eight healthy subjects participated in a between-subject design, with either anodal tDCS (20 min, 1 mA) or sham stimulation, over Broca's area during the acquisition of an artificial grammar. Performance during the acquisition phase, presented as a working memory task, was comparable between groups. In the subsequent classification task, detecting syntactic violations, and specifically, those where no cues to superficial similarity were available, improved significantly after anodal tDCS, resulting in an overall better performance. A control experiment where 10 subjects received anodal tDCS over an area unrelated to artificial grammar learning further supported the specificity of these effects to Broca's area. We conclude that Broca's area is specifically involved in rule-based knowledge, and here, in an improved ability to detect syntactic violations. The results cannot be explained by better tDCS-induced working memory performance during the acquisition phase. This is the first study that demonstrates that tDCS may facilitate acquisition of grammatical knowledge, a finding of potential interest for rehabilitation of aphasia.
Supervised machine learning (ML) algorithms are increasingly popular tools for fMRI decoding due to their predictive capability and their ability to capture information encoded by spatially correlated voxels. In addition, an important secondary outcome is a multivariate representation of the pattern underlying the prediction. Despite an impressive array of applications, most fMRI applications are framed as classification problems and predictions are limited to categorical class decisions. For many applications, quantitative predictions are desirable that more accurately represent variability within subject groups and that can be correlated with behavioural variables. We evaluate the predictive capability of Gaussian process (GP) models for two types of quantitative prediction (multivariate regression and probabilistic classification) using whole-brain fMRI volumes. As a proof of concept, we apply GP models to an fMRI experiment investigating subjective responses to thermal pain and show GP models predict subjective pain ratings without requiring anatomical hypotheses about functional localisation of relevant brain processes. Even in the case of pain perception, where strong hypotheses do exist, GP predictions were more accurate than any region previously demonstrated to encode pain intensity. We demonstrate two brain mapping methods suitable for GP models and we show that GP regression models outperform state of the art support vector- and relevance vector regression. For classification, GP models perform categorical prediction as accurately as a support vector machine classifier and furnish probabilistic class predictions.
Efficient execution of perceptual-motor tasks requires rapid voluntary reconfiguration of cognitive task sets as circumstances unfold. Such acts of cognitive control, which are thought to rely on a network of cortical regions in prefrontal and posterior parietal cortex, include voluntary shifts of attention among perceptual inputs or among memory representations, or switches between categorization or stimulus-response mapping rules. A critical unanswered question is whether task set shifts in these different domains are controlled by a common, domain-independent mechanism or by separate, domain-specific mechanisms. Recent studies have implicated a common region of medial superior parietal lobule (mSPL) as a domain-independent source of cognitive control during shifts between perceptual, mnemonic, and rule representations. Here, we use fMRI and event-related multivoxel pattern classification to show that spatial patterns of brain activity within mSPL reliably express which of several domains of cognitive control is at play on a moment-by-moment basis. Critically, these spatiotemporal brain patterns are stable over time within subjects tested several months apart and across a variety of tasks, including shifting visuospatial attention, switching categorization rules, and shifting attention in working memory.
Most people born deaf and exposed to oral language show scant evidence of sensitivity to the phonology of speech when processing written language. In this respect they differ from hearing people. However, occasionally, a prelingually deaf person can achieve good processing of written language in terms of phonological sensitivity and awareness, and in this respect appears exceptional. We report the pattern of event-related fMRI activation in such a deaf reader while performing a rhyme-judgment on written words with similar spelling endings that do not provide rhyme clues. The left inferior frontal gyrus pars opercularis and the left inferior parietal lobe showed greater activation for this task than for a letter-string identity matching task. This participant was special in this regard, showing significantly greater activation in these regions than a group of hearing participants with a similar level of phonological and reading skill. In addition, SR showed activation in the left mid-fusiform gyrus; a region which did not show task-specific activation in the other respondents. The pattern of activation in this exceptional deaf reader was also unique compared with three deaf readers who showed limited phonological processing. We discuss the possibility that this pattern of activation may be critical in relation to phonological decoding of the written word in good deaf readers whose phonological reading skills are indistinguishable from those of hearing readers.
Inferring the intentions of other people from their actions recruits an inferior fronto-parietal action observation network as well as a putative social network that includes the posterior superior temporal sulcus (STS). However, the functional dynamics within and among these networks remains unclear. Here we used functional magnetic resonance imaging (fMRI) and high-density electroencephalogram (EEG), with a repetition suppression design, to assess the spatio-temporal dynamics of decoding intentions. Suppression of fMRI activity to the repetition of the same intention was observed in inferior frontal lobe, anterior intraparietal sulcus (aIPS), and right STS. EEG global field power was reduced with repeated intentions at an early (starting at 60 ms) and a later (approximately 330 ms) period after the onset of a hand-on-object encounter. Source localization during these two intervals involved right STS and aIPS regions highly consistent with RS effects observed with fMRI. These results reveal the dynamic involvement of temporal and parietal networks at multiple stages during the intention decoding and without a strict segregation of intention decoding between these networks.
We demonstrate that multivoxel pattern analysis can be used to decode place-related information in fMRI. Subjects performed a working memory version of the Morris water maze task in a virtual environment with a single wall cue. The voxel data that corresponds to when subjects were located at the goal was extracted for seven regions implicated in spatial navigation, and then used to train a pattern classifier based on partial least squares. Using a leave-one-out (LOO) test procedure, goal locations at E, W, N positions (relative to the cue as S) were predicted significantly better than a naive classifier for voxels in medial prefrontal cortex, hippocampus, and inferior parietal cortex. Prediction with voxels from other regions involved in navigation was also better than a naive classifier, which raises the possibility that goal-location information is widely disseminated among the navigation network. It turns out that predictive capability of all regions combined significantly decreases, relative to no change, only when voxel data from the hippocampus is left out. This implies that the hippocampus contains some unique information that identifies goal locations, whereas other regions contain information that also identifies goal locations but is more redundant. Classification of goal locations is an important step toward decoding a variety of place-related information in spatial navigation with fMRI.
The ability to rise above the present environment and reflect upon the past, the future, and the minds of others is a fundamentally defining human feature. It has been proposed that these three self-referential processes involve a highly interconnected core set of brain structures known as the default mode network (DMN). The DMN appears to be active when individuals are engaged in stimulus-independent thought. This network is a likely candidate for supporting multiple processes, but this idea has not been tested directly. We used fMRI to examine brain activity during autobiographical remembering, prospection, and theory-of-mind reasoning. Using multivariate analyses, we found a common pattern of neural activation underlying all three processes in the DMN. In addition, autobiographical remembering and prospection engaged midline DMN structures to a greater degree and theory-of-mind reasoning engaged lateral DMN areas. A functional connectivity analysis revealed that activity of a critical node in the DMN, medial prefrontal cortex, was correlated with activity in other regions in the DMN during all three tasks. We conclude that the DMN supports common aspects of these cognitive behaviors involved in simulating an internalized experience.
In speech comprehension, the processing of auditory information and linguistic context are mutually dependent. This functional magnetic resonance imaging study examines how semantic expectancy ("cloze probability") in variably intelligible sentences ("noise vocoding") modulates the brain bases of comprehension. First, intelligibility-modulated activation along the superior temporal sulci (STS) was extended anteriorly and posteriorly in low-cloze sentences (e.g., "she weighs the flour") but restricted to a mid-superior temporal gyrus/STS area in more predictable high-cloze sentences (e.g., "she sifts the flour"). Second, the degree of left inferior frontal gyrus (IFG) (Brodmann's area 44) involvement in processing low-cloze constructions was proportional to increasing intelligibility. Left inferior parietal cortex (IPC; angular gyrus) activation accompanied successful speech comprehension that derived either from increased signal quality or from semantic facilitation. The results show that successful decoding of speech in auditory cortex areas regulates language-specific computation (left IFG and IPC). In return, semantic expectancy can constrain these speech-decoding processes, with fewer neural resources being allocated to highly predictable sentences. These findings offer an important contribution toward the understanding of the functional neuroanatomy in speech comprehension.
Neuroimaging can address activity across the entire brain in relation to cognition, but is typically correlative rather than causal. Brain stimulation can target a local brain area causally, but without revealing the entire network affected. Combining brain stimulation with concurrent neuroimaging allows a new causal approach to how interplay between extended networks of brain regions can support cognition. Brain stimulation does not affect only the targeted local region but also activity in remote interconnected regions. These remote effects depend on cognitive factors (e.g. task-condition), revealing dynamic changes in interplay between brain areas. We illustrate this with examples from top-down modulation of visual cortex, response-competition, inter-hemispheric rivalry and motor tasks; but the new approach should be applicable to many domains of cognition.
Positive and negative emotional states are thought to have originated from fundamentally opposing approach and avoidance behaviors. Furthermore, affective valence has been hypothesized to exert opposing biases in cognitive control. Here we examined with functional magnetic resonance imaging whether the opposing influences of positive and negative states extend to perceptual encoding in the visual cortices. Based on prior behavioral research, we hypothesized that positive states would broaden and negative states would narrow visual field of view (FOV). Positive, neutral, and negative states were induced on alternating blocks. To index FOV, observers then viewed brief presentations (300 ms) of face/place concentric center/surround stimuli on interleaved blocks. Central faces were attended, rendering the place surrounds unattended. As face and place information was presented at different visual eccentricities, our physiological metric of FOV was a valence-dependent modulation of place processing in the parahippocampal place area (PPA). Consistent with our hypotheses, positive affective states increased and negative states decreased PPA response to novel places as well as adaptation to repeated places. Individual differences in self-reported positive and negative affect correlated inversely with PPA encoding of peripheral places, as well as with activation in the mesocortical prefrontal cortex and amygdala. Psychophysiological interaction analyses further demonstrated that valence-dependent responses in the PPA arose from opponent coupling with extrafoveal regions of the primary visual cortex during positive and negative states. These findings collectively suggest that affective valence differentially biases gating of early visual inputs, fundamentally altering the scope of perceptual encoding.
The assessment of sexual orientation is of importance to the diagnosis and treatment of sex offenders and paraphilic disorders. Phallometry is considered gold standard in objectifying sexual orientation, yet this measurement has been criticized because of its intrusiveness and limited reliability.
Speech and language are considered uniquely human abilities: animals have communication systems, but they do not match human linguistic skills in terms of recursive structure and combinatorial power. Yet, in evolution, spoken language must have emerged from neural mechanisms at least partially available in animals. In this paper, we will demonstrate how our understanding of speech perception, one important facet of language, has profited from findings and theory in nonhuman primate studies. Chief among these are physiological and anatomical studies showing that primate auditory cortex, across species, shows patterns of hierarchical structure, topographic mapping and streams of functional processing. We will identify roles for different cortical areas in the perceptual processing of speech and review functional imaging work in humans that bears on our understanding of how the brain decodes and monitors speech. A new model connects structures in the temporal, frontal and parietal lobes linking speech perception and production.
The ability to group stimuli into meaningful categories is fundamental to natural behavior. Raw perceptions would be useless without an ability to classify items as, for example, threat or food. Previous work suggests that people have a tendency to group stimuli either on the basis of a single dimension or by overall similarity (e.g., Milton, F.N., Longmore, C.A., and Wills, A.J. (2008). Processes of overall similarity sorting in free classification. J. Exp. Psychol. Hum. Percept. Perform, 34, 676-692.). It has recently been suggested that overall similarity sorting can engage similar rule-based processes to single-dimension sorting and, in addition, requires greater use of working memory (Milton, F.N., and Wills, A.J. (2004). The influences of stimulus properties on category construction. J. Exp. Psychol. Learn. Mem. Cogn, 30, 407-415.). These predictions were tested in an event-related fMRI study of spontaneous categorization. Results showed a striking overlap of activation between overall similarity and single-dimension sorting indicating engagement of common neural processes. Furthermore, overall similarity sorting recruited additional activity in bilateral precuneus, right cuneus, left cerebellum, left postcentral gyrus, right thalamus and right ventrolateral frontal cortex (VLFC). Our findings suggest that overall similarity sorting can be the result of rule-based processes and highlight a potential role for right VLFC in integrating multi-dimensional sensory information to form conceptual categories.
Functional neuroimaging is widely used to unravel changes in brain functioning in psychiatric disorders. In the current study, we review single-photon emission tomography (SPECT), positron emission tomography (PET) and functional magnetic resonance imaging (fMRI) studies in anorexia nervosa (AN), a difficult-to-treat eating disorder with the highest mortality rate among psychiatric disorders. We discuss the role of the parietal cortex, anterior and subgenual cingulate cortex, frontal cortex and temporal lobe in light of the cardinal symptoms of AN. The insights of the current review may ultimately lead to the development of new treatments.
It is well established that the left inferior frontal gyrus plays a key role in the cerebral cortical network that supports reading and visual word recognition. Less clear is when in time this contribution begins. We used magnetoencephalography (MEG), which has both good spatial and excellent temporal resolution, to address this question.
Visual working memory provides an essential link between perception and higher cognitive functions, allowing for the active maintenance of information about stimuli no longer in view. Research suggests that sustained activity in higher-order prefrontal, parietal, inferotemporal and lateral occipital areas supports visual maintenance, and may account for the limited capacity of working memory to hold up to 3-4 items. Because higher-order areas lack the visual selectivity of early sensory areas, it has remained unclear how observers can remember specific visual features, such as the precise orientation of a grating, with minimal decay in performance over delays of many seconds. One proposal is that sensory areas serve to maintain fine-tuned feature information, but early visual areas show little to no sustained activity over prolonged delays. Here we show that orientations held in working memory can be decoded from activity patterns in the human visual cortex, even when overall levels of activity are low. Using functional magnetic resonance imaging and pattern classification methods, we found that activity patterns in visual areas V1-V4 could predict which of two oriented gratings was held in memory with mean accuracy levels upwards of 80%, even in participants whose activity fell to baseline levels after a prolonged delay. These orientation-selective activity patterns were sustained throughout the delay period, evident in individual visual areas, and similar to the responses evoked by unattended, task-irrelevant gratings. Our results demonstrate that early visual areas can retain specific information about visual features held in working memory, over periods of many seconds when no physical stimulus is present.
Multi-voxel pattern analysis (MVPA) was used to analyze blood-oxygen level dependent functional magnetic resonance imaging (BOLD fMRI) data, which were acquired as human subjects received brief vibrotactile stimulation of their hands and feet. Support vector machines trained and tested on the whole brain fMRI data were able to accurately decode the body site of single touches, with mean performance of 92% in a two-way discrimination task (chance performance 50%) and 70% in a four-way discrimination task (chance performance 25%). Primary and secondary somatosensory areas (S1 and S2) alone decoded the touched body site with high accuracy. S1 was more accurate at decoding touches closely spaced on the body surface (different fingers of the same hand) whereas S2 and S1 were equally accurate at decoding widely spaced touches (hand vs. foot). The hand and foot regions of S1 (S1hand and S1foot) were separately examined in a two-way classification task. S1hand was better able to decode the hand of stimulation (left vs. right), and S1foot was better able to decode the foot of stimulation. In addition to S1 and S2, vibrotactile responses were observed in a region of visual cortex, areas MST and STP (MST/STP) in lateral occipito-temporal lobe. MST/STP was able to accurately decode the hand but not the foot of stimulation, supporting the idea of a role for MST/STP in eye-hand coordination.
Information about arm movement direction in neuronal activity of the cerebral cortex can be used for movement control mediated by a brain-machine interface (BMI). Here we provide a topographic analysis of the information related to arm movement direction that can be extracted from single trials of electrocorticographic (ECoG) signals recorded from the human frontal and parietal cortex based on a precise assignment of ECoG recording channels to the subjects' individual cortical anatomy and function. To this aim, each electrode contact was identified on structural MRI scans acquired while the electrodes were implanted and was thus related to the brain anatomy of each patient. Cortical function was assessed by direct cortical electrical stimulation. We show that activity from the primary motor cortex, in particular from the region showing hand and arm motor responses upon electrical stimulation, carries most directional information. The premotor, posterior parietal and lateral prefrontal cortex contributed gradually less, but still significant information. This gradient was observed for decoding from movement-related potentials, and from spectral amplitude modulations in low frequencies and in the high gamma band. Our findings thus demonstrate a close topographic correlation between cortical functional anatomy and direction-related information in humans that might be used for brain-machine interfacing.
Despite growing interest in applying machine learning to neuroimaging analyses, few studies have gone beyond classifying sensory input to directly predicting behavioral output. With spatial resolution on the order of millimeters and temporal resolution on the order of seconds, functional magnetic resonance imaging (fMRI) is a promising technology for such applications. However, fMRI data's low signal-to-noise ratio, high dimensionality, and extensive spatiotemporal correlations present formidable analytic challenges. Here, we apply different machine-learning algorithms to previously acquired data to examine the ability of fMRI activation in three regions-the nucleus accumbens (NAcc), medial prefrontal cortex (MPFC), and insula-to predict purchasing. Our goal was to improve spatiotemporal interpretability as well as classification accuracy. To this end, sparse penalized discriminant analysis (SPDA) enabled automatic selection of correlated variables, yielding interpretable models that generalized well to new data. Relative to logistic regression, linear discriminant analysis, and linear support vector machines, SPDA not only increased interpretability but also improved classification accuracy. SPDA promises to allow more precise inferences about when specific brain regions contribute to purchasing decisions. More broadly, this approach provides a general framework for using neuroimaging data to build interpretable models, including those that predict choice.
The flow of information from sensory stimuli to motor responses in the human brain can be flexibly re-routed depending on task demands. However, it has remained unclear which sequence of processes is involved in preparing the brain for an upcoming task. Here, we used a combination of fMRI and multivariate pattern classification to decompose the information flow in a task-switching experiment. Specifically, we present a time-resolved decoding approach that allowed us to track the temporal buildup of task-related information. This approach also allowed us to distinguish encoding of the task from encoding of target stimuli and motor responses, thus separating between different components of information processing. We were able to decode from parietal and lateral prefrontal cortex which specific task-set a subject was currently holding. Importantly, this revealed that the intraparietal sulcus encoded task-set information before prefrontal cortex, and it was the only region to encode the specific task-set before the relevant target stimulus was presented. This suggests that task-related information in parietal cortex does not rely on input from prefrontal cortex as previously suggested. In contrast, our findings suggest that parietal cortex might play a role in establishing task-sets in prefrontal cortex.
Different EEG-vigilance stages from full alertness to sleep onset can be separated during rest. Also fMRI research recently focused on the resting condition and identified several resting state networks. In order to deepen the understanding of different levels of global brain function from relaxed wakefulness to sleep onset the association between EEG-vigilance stages and BOLD signals was analysed. EEG-vigilance stages were attributed to consecutive 3-sec-EEG-segments by an algorithm using topographic and spectral information. Results of the classification were validated by analysing the heart rates during the different brain states. Vigilance stages served as regressors for the analysis of the simultaneously acquired fMRI data. Additionally resting state networks were derived from the fMRI data using independent component analysis (ICA). Also vigilance associated brain activity revealed by EEG-based standardized low resolution tomography (sLORETA) was compared to the results of the fMRI analysis. Results showed increased BOLD signal in the occipital cortex, the anterior cingulate cortex, the frontal cortex, the parietal cortices and the temporal cortices and decreasing BOLD signals in the thalamus and the frontal cortex for declining vigilance stages (A2, A3, B1, B2/B3) in comparison to the high vigilance stage A1. Resting state networks revealed a spatial overlap with the vigilance stage associated BOLD maps in conjunction analyses. sLORETA showed increased neuroelectric alpha activity at the occipital cortex comparable to occipital BOLD signal decreases when comparing stage A with stage B. Different EEG-vigilance stages during rest are associated with pronounced differences of BOLD signals in several brain areas which partly correspond to the resting state networks. For cognitive fMRI-research it therefore seems important to pay attention to vigilance switches in order to separate vigilance associated BOLD signal changes from those specifically related to cognition.
Decoding and information theoretic techniques were used to analyze the predictions that can be made from functional magnetic resonance neuroimaging data on individual trials. The subjective pleasantness produced by warm and cold applied to the hand could be predicted on single trials with typically in the range 60-80% correct from the activations of groups of voxels in the orbitofrontal and medial prefrontal cortex and pregenual cingulate cortex, and the information available was typically in the range 0.1-0.2 (with a maximum of 0.6) bits. The prediction was typically a little better with multiple voxels than with one voxel, and the information increased sublinearly with the number of voxels up to typically seven voxels. Thus the information from different voxels was not independent, and there was considerable redundancy across voxels. This redundancy was present even when the voxels were from different brain areas. The pairwise stimulus-dependent correlations between voxels, reflecting higher-order interactions, did not encode significant information. For comparison, the activity of a single neuron in the orbitofrontal cortex can predict with 90% correct and encode 0.5 bits of information about whether an affectively positive or negative visual stimulus has been shown, and the information encoded by small numbers of neurons is typically independent. In contrast, the activation of a 3 x 3 x 3-mm voxel reflects the activity of approximately 0.8 million neurons or their synaptic inputs and is not part of the information encoding used by the brain, thus providing a relatively poor readout of information compared with that available from small populations of neurons.
OBJECTIVE: The purpose of this investigation was to determine whether there is an association between the putative reading disability (RD) susceptibility gene Doublecortin Domain Containing 2 (DCDC2), and gray matter (GM) distribution in the brain, in a sample of healthy control individuals. METHOD: Fifty-six control subjects were genotyped for an RD-associated deletion in intron 2 of DCDC2. Voxel based morphometry (VBM) was used to examine structural magnetic resonance imaging (MRI) scans to assess GM differences between the two groups. RESULTS: Individuals heterozygous for the deletion exhibited significantly higher GM volumes in reading/language and symbol-decoding related brain regions including superior, medial and inferior temporal, fusiform, hippocampal/para-hippocampal, inferior occipito-parietal, inferior and middle frontal gyri, especially in the left hemisphere. GM values correlated with published data on regional DCDC2 expression in a lateralized manner. CONCLUSIONS: These data suggest a role for DCDC2 in GM distribution in language-related brain regions in healthy individuals.
Do our brains implicitly track the energetic content of the foods we see? Using electrical neuroimaging of visual evoked potentials (VEPs) we show that the human brain can rapidly discern food's energetic value, vis a vis its fat content, solely from its visual presentation. Responses to images of high-energy and low-energy food differed over two distinct time periods. The first period, starting at approximately 165 ms post-stimulus onset, followed from modulations in VEP topography and by extension in the configuration of the underlying brain network. Statistical comparison of source estimations identified differences distributed across a wide network including both posterior occipital regions and temporo-parietal cortices typically associated with object processing, and also inferior frontal cortices typically associated with decision-making. During a successive processing stage (starting at approximately 300 ms), responses differed both topographically and in terms of strength, with source estimations differing predominantly within prefrontal cortical regions implicated in reward assessment and decision-making. These effects occur orthogonally to the task that is actually being performed and suggest that reward properties such as a food's energetic content are treated rapidly and in parallel by a distributed network of brain regions involved in object categorization, reward assessment, and decision-making.
NA
Past experience is hypothesized to reduce computational demands in PFC by providing bottom-up predictive information that informs subsequent stimulus-action mapping. The present fMRI study measured cortical activity reductions ("neural priming"/"repetition suppression") during repeated stimulus classification to investigate the mechanisms through which learning from the past decreases demands on the prefrontal executive system. Manipulation of learning at three levels of representation-stimulus, decision, and response-revealed dissociable neural priming effects in distinct frontotemporal regions, supporting a multiprocess model of neural priming. Critically, three distinct patterns of neural priming were identified in lateral frontal cortex, indicating that frontal computational demands are reduced by three forms of learning: (a) cortical tuning of stimulus-specific representations, (b) retrieval of learned stimulus-decision mappings, and (c) retrieval of learned stimulus-response mappings. The topographic distribution of these neural priming effects suggests a rostrocaudal organization of executive function in lateral frontal cortex.
Neuroimaging studies of reading converge to suggest that linguistically elementary stimuli are confined to the activation of bilateral posterior regions, whereas linguistically complex stimuli additionally recruit left hemispheric anterior regions, raising the hypotheses of a gradual bilateral-to-left and a posterior-to-anterior recruitment of reading related areas. Here, we tested these two hypotheses by contrasting a repertoire of eight categories of stimuli ranging from simple orthographic-like characters to words and pseudowords in a single experiment, and by measuring BOLD signal changes and connectivity while 16 fluent readers passively viewed the stimuli. Our results confirm the existence of a bilateral-to-left and posterior-to-anterior recruitment of reading related areas, straightforwardly resulting from the increase in stimuli's linguistic processing load, which reflects reading processes: visual analysis, orthographic encoding and phonological decoding. Connectivity analyses strengthened the validity of these observations and additionally revealed an enhancement of the left parieto-frontal information trafficking for higher linguistic processing. Our findings clearly establish the notion of a gradual spatio-functional recruitment of reading areas and demonstrate, to the best of our knowledge, the first evidence of a robust and staged link between the level of linguistic processing, the spatial distribution of brain activity and its information trafficking.
This study describes a functional magnetic resonance imaging study of humans engaged in long-term memory (LTM) and working memory tasks. A pattern classifier learned to identify patterns of brain activity associated with viewing and making judgments about three categories of pictures (famous people, famous locations, and common objects). The evaluation of these stimuli relied on perception and long-term semantic and/or episodic memories. We investigated whether this classifier could successfully decode brain activity from a subsequent delayed paired-associate recognition working memory task that required the short-term retention of the same stimuli. We reasoned that the LTM-trained classifier would be able to decode delay-period activity only if that activity reflected, to some extent, the temporary activation of LTM. Our results demonstrated successful decoding: delay-period activity from a distributed network of brain regions matched learned patterns of activity for task-relevant stimuli to a greater extent than for task-irrelevant stimuli. In varying degrees throughout the delay, activity reflected the target (a retrospective code) and its associate (a prospective code) with considerable variability among subjects. Although prefrontal cortex (PFC) demonstrated category-specific patterns of activity during the LTM task, these patterns were not reinstated in PFC during the working memory task. We conclude that the short-term retention of information can be supported by the temporary reactivation of LTM representations.
We investigated the functional characteristics of brain regions implicated in processing of speech melody by presenting words spoken in either neutral or angry prosody during a functional magnetic resonance imaging experiment using a factorial habituation design. Subjects judged either affective prosody or word class for these vocal stimuli, which could be heard for either the first, second, or third time. Voice-sensitive temporal cortices, as well as the amygdala, insula, and mediodorsal thalami, reacted stronger to angry than to neutral prosody. These stimulus-driven effects were not influenced by the task, suggesting that these brain structures are automatically engaged during processing of emotional information in the voice and operate relatively independent of cognitive demands. By contrast, the right middle temporal gyrus and the bilateral orbito-frontal cortices (OFC) responded stronger during emotion than word classification, but were also sensitive to anger expressed by the voices, suggesting that some perceptual aspects of prosody are also encoded within these regions subserving explicit processing of vocal emotion. The bilateral OFC showed a selective modulation by emotion and repetition, with particularly pronounced responses to angry prosody during the first presentation only, indicating a critical role of the OFC in detection of vocal information that is both novel and behaviorally relevant. These results converge with previous findings obtained for angry faces and suggest a general involvement of the OFC for recognition of anger irrespective of the sensory modality. Taken together, our study reveals that different aspects of voice stimuli and perceptual demands modulate distinct areas involved in the processing of emotional prosody.
The pattern of intonation accompanying an utterance provides a powerful cue as to a speaker's emotional state of mind. Most prior lesion studies have demonstrated that the nodal point for decoding these prosodic emotion cues is mediated by unimodal auditory cortex in the right posterior lateral temporal lobe. However, functional neuroimaging has brought with it increasing attention to the equivalent left hemisphere region in this role. This study used fMRI to quantitatively assess the hypothesis that involvement of the left posterior lateral temporal lobe depended on the linguistic load or verbal complexity of the prosodic emotion stimuli. BOLD contrast data was acquired on a 3T scanner whilst 16 healthy young adults identified the prosodic emotion in three conditions: 'sentences' comprised of words, a repeated monosyllable, and a single prolonged syllable (asyllabic). Whole-brain analyses were performed using SPM5 and supplemented by posterior lateral temporal lobe region of interest (ROI) analyses. The whole-brain analyses appeared to show bilateral temporal lobe activation across the conditions, however, the ROI analyses indicated a highly significant decrease in activity in the left ROI as verbal complexity decreased. Changes in right ROI activity were not statistically significant. Our results indicate that the likelihood of observing a notable left temporal lobe response in functional neuroimaging studies of emotional prosody comprehension depends on the verbal complexity of the prosodic emotion stimuli. Despite the right hemisphere dominance underlying this task, the left hemisphere region may be co-activated in its attempt to extract phonetic-segmental information from the acoustic stimuli whether or not the stimuli contain meaningful phonetic-segmental information.
The human capacity to implicitly acquire knowledge of structured sequences has recently been investigated in artificial grammar learning using functional magnetic resonance imaging. It was found that the left inferior frontal cortex (IFC; Brodmann's area (BA) 44/45) was related to classification performance. The objective of this study was to investigate whether the IFC (BA 44/45) is causally related to classification of artificial syntactic structures by means of an off-line repetitive transcranial magnetic stimulation (rTMS) paradigm. We manipulated the stimulus material in a 2 x 2 factorial design with grammaticality status and local substring familiarity as factors. The participants showed a reliable effect of grammaticality on classification of novel items after 5 days of exposure to grammatical exemplars without performance feedback in an implicit acquisition task. The results show that rTMS of BA 44/45 improves syntactic classification performance by increasing the rejection rate of non-grammatical items and by shortening reaction times of correct rejections specifically after left-sided stimulation. A similar pattern of results is observed in FMRI experiments on artificial syntactic classification. These results suggest that activity in the inferior frontal region is causally related to artificial syntax processing.
The article describes a case of a 15-year old boy after a head contusion with a five-month history of headaches and two seizure episodes. MR imaging revealed a partly solid and partly cystic cortical-subcortical tumour within the precentral gyrus with post-contrast enhancement. The patient underwent gross total resection of the lesion. Histologically the neoplasm was composed of pseudopapillary gliovascular structures surrounded by solid glioneuronal tumour areas. The expression of GFAP and nestin characterized the central parts of the tumour. Moreover the immunolabelling for synaptophysin, neurofilaments, Olig2 and NCAM was present in the peripheral part of the lesion. The neoplasm was consistent with a papillary glioneuronal tumour - one of the new entities in the last WHO CNS tumour classification.
Although lateral prefrontal cortex (LPFC) is clearly involved in decision-making, competing functional characterizations exist. One characterization posits that activation reflects the need to select among competing representations. In contrast, recent fMRI research suggests that activation is driven by the criterial classification of representations, even with minimal competition. To adjudicate between these hypotheses, we used event-related fMRI and contrasted tasks that required different numbers of criterial classifications prior to response in both perceptual and memory domains. Additionally, we manipulated the level of interstimulus competition by increasing the number of probes. Experiment 1 demonstrated that LPFC activation tracked the number of intermediate classifications during trials yet was insensitive to the number of competing probes and the behavioral decline accompanying competition. Furthermore, Experiment 2 demonstrated equivalent increases in LPFC activation for a task requiring two overt criterial classifications (independent classification) and one requiring two covert criterial classifications prior to the single overt response (same-different judgment). As found in Experiment 1, both tasks showed greater activation than a judgment requiring only one classification act (forced choice). These data indicate that LPFC responses reflect the number of executed criterial classifications or judgments, independent of the number of competing stimuli and the overt response demands of the decision task.
We examined the correlation between behavioural reaction time and functional imaging parameters of the blood oxygenated level dependent (BOLD) response in Broca's Area during a word identification task, and whether the correlation [Magnetic Resonance Imaging 22 (2004) 451-455] varies as a function of four stimulus types: regular words (REGs) (e.g., hint), irregular words (IRRs) (e.g., pint), nonwords (NWs) (e.g., bint), and pseudohomophones (PHs) (e.g., pynt). Participants named letter strings aloud during a functional magnetic resonance imaging study. Naming reaction times were recorded during regular gaps in image acquisition, and BOLD parameters were extracted via a Tikhonov regularized BOLD analysis technique. The results revealed that only PH reaction times were correlated with BOLD width, providing evidence that Broca's area supports phonetic decoding accompanied with phonological lexical access. In addition, we advanced the development of equation-based models of cognitive behaviour and neurophysiology, whereby we showed that the relationship of mathematical independence that exists for predicting REG accuracy, given IRR and NW or PH naming accuracy, was present for naming reaction time, BOLD width, BOLD time to peak, and BOLD intensity. Therefore, we provide converging behavioural and neuroanatomical evidence for a mathematically independent relationship between sight vocabulary and phonetic decoding systems, consistent with a dual-route model of reading.
Traditionally, the left frontal and parietal lobes have been associated with language production while regions in the temporal lobe are seen as crucial for language comprehension. However, recent evidence suggests that the classical language areas constitute an integrated network where each area plays a crucial role both in speech production and perception. We used functional MRI to examine whether observing speech motor movements (without auditory speech) relative to non-speech motor movements preferentially activates the cortical speech areas. Furthermore, we tested whether the activation in these regions was modulated by task difficulty. This dissociates between areas that are actively involved with speech perception from regions that show an obligatory activation in response to speech movements (e.g. areas that automatically activate in preparation for a motoric response). Specifically, we hypothesized that regions involved with decoding oral speech would show increasing activation with increasing difficulty. We found that speech movements preferentially activate the frontal and temporal language areas. In contrast, non-speech movements preferentially activate the parietal region. Degraded speech stimuli increased both frontal and parietal lobe activity but did not differentially excite the temporal region. These findings suggest that the frontal language area plays a role in visual speech perception and highlight the differential roles of the classical speech and language areas in processing others' motor speech movements.
Subacute sclerosing panencephalitis (SSPE), a post-measles progressive neurological disorder is still common in India because of indifferent vaccination compliance. However, the acute fulminant form of SSPE is extremely rare. An unusual case of fulminant SSPE in an 18-year-old man from south India with an ultra-short course of 19 days presenting with hemiparesis in absence of myoclonus and progressive cognitive decline, is reported. MRI showed frontal and parieto-occipital demyelination extending to nuclear areas. Antimeasles antibodies were demonstrable in CSF and serum with oligoclonal bands in CSF despite normal CSF protein and cell count. At autopsy, unlike classical SSPE, oligodendroglia containing measles viral antigen was sparse despite florid necrotizing leukoencephalitis with acute demyelination. Measles virus was isolated from the brain with hypermutation in M gene confirming the diagnosis. Phylogenetic analysis of the viral genotype indicated that it belonged to D7 genotype which is considered rare in India.
Prior exposure to a stimulus can facilitate its subsequent identification and classification, a phenomenon called priming. This behavioural facilitation is usually accompanied by a reduction in neural response within specific cortical regions (repetition suppression, RS). Recent research has suggested that both behavioural priming and RS can be largely determined by previously learned stimulus-response associations. According to this view, a direct association forms between the stimulus presented and the response made to it. On a subsequent encounter with the stimulus, this association automatically cues the response, bypassing the various processing stages that were required to select that response during its first presentation. Here we reproduce behavioural evidence for such stimulus-response associations, and show the PFC to be sensitive to such changes. In contrast, RS within ventral temporal regions (such as the fusiform cortex), which are usually associated with perceptual processing, is shown to be robust to response changes. The present study therefore suggests a dissociation between RS within the PFC, which may be sensitive to retrieval of stimulus-response associations, and RS within posterior perceptual regions, which may reflect facilitation of perceptual processing independent of stimulus-response associations.
The aim of our work was to localize cortical areas involved in the processing of incomplete figures using functional MRI (fMRI) for 8 healthy volunteers (18-30 year old) with the did of anatomical and fMRI fast imaging technique: echo planar imaging (EPI), whole brain scan (36 slices) matrix 64 x 64, 3.7 second. We used 1.5 T MR-scanner and BOLD-method (Blood Oxygenation Level Dependent), based on distinctions of magnetic properties of hemoglobin. Fast imaging technique on modern MR-scanners with > or = 1.5 T provides precise statistical maps of oxygenation increase with high spatial resolution. For test stimuli we used matrix of Gabor grating. We used two types of 10 x 10 matrices with chaotic and ordered orientation of Gabor gratings. The size, brightness and contrast of the stimuli were identical. The chaotic and ordered patterns activated different brain areas. We establish that ordered patterns activated only primary visual cortex - V1 and V2, (BA17-18), wheareas chaotic patterns activated in addition primary visual cortex, the V3,V4,V5 (BA19) of the occipital cortex and the area 7 of parietal area (BA7) classification. Decision making for that task is localized in prefrontal and frontal cortex, including (BA 6, 9, 10).
Despite the importance of visual categorization for interpreting sensory experiences, little is known about the neural representations that mediate categorical decisions in the human brain. Here, we used psychophysics and pattern classification for the analysis of functional magnetic resonance imaging data to predict the features critical for categorical decisions from brain activity when observers categorized the same stimuli using different rules. Although a large network of cortical and subcortical areas contain information about visual categories, we show that only a subset of these areas shape their selectivity to reflect the behaviorally relevant features rather than simply physical similarity between stimuli. Specifically, temporal and parietal areas show selectivity for the perceived form and motion similarity, respectively. In contrast, frontal areas and the striatum represent the conjunction of spatiotemporal features critical for complex and adaptive categorization tasks and potentially modulate selectivity in temporal and parietal areas. These findings provide novel evidence for flexible neural coding in the human brain that translates sensory experiences to categorical decisions by shaping neural representations across a network of areas with dissociable functional roles in visual categorization.
Systemizing ability exists on a spectrum, with a high systemizing style meaning proficiency in analysing the rules of a system, to predict how that system works. This study uses fMRI to investigate a spectrum of low to high systemizing, to assess whether individuals with a high systemizing style exhibit an attentional bias towards local details. This is the first study to test for the neural correlates of systemizing. Participants with a range of scores on the Systemizing Quotient (SQ) were given a version of the Navon task during fMRI, which elicits perceptual conflict between local and global levels of visual attention. SQ score was correlated with a focus on local detail in the behavioural study. During conditions eliciting perceptual conflict SQ score was associated with increased activation in the lateral prefrontal, parietal and extrastriate visual cortices. However, neural investigations did not imply a neural correlate of systemizing during local processing per se. Results are discussed in terms of a heightened ability to maintain an attentional set in those with a high systemizing cognitive style.
There have been several studies supporting the notion of a ventral-dorsal distinction in the primate cortex for visual object processing, whereby the ventral stream specializes in object identification, and the dorsal stream is engaged during object localization and interaction. There is also a growing body of evidence supporting a ventral stream that specializes in lexical (i.e., whole-word) reading, and a dorsal stream that is engaged during sub-lexical reading (i.e., phonetic decoding). Here, we consider the extent to which word-reading processes are located in regions either intersecting with, or unique from, regions that sub-serve object processing along these streams. Object identification was contrasted with lexical-based reading, and object interaction processing (i.e., deciding how to interact with an object) was contrasted with sub-lexical reading. Our results suggest that object identification and lexical-based reading are largely ventral and modular, showing mainly unique regions of activation (parahippocampal and occipital-temporal gyri function associated with object identification, and lingual, lateral occipital, and posterior inferior temporal gyri function associated with lexical-based reading) and very little shared activation (posterior inferior frontal gyrus). Object interaction processing and phonetic decoding are largely dorsal, and show both modular regions of activation (more lateralized to the dorsal-frontal right hemisphere for pseudohomophone naming, and more to the dorsal-frontal left hemisphere for the object interaction task) as well as significant shared regions of processing (precentral gyri, left inferior frontal cortex, left postcentral gyrus, left lateral occipital cortex, and superior posterior temporal gyri). Given that the perceptual experimental conditions show primarily modular and very little shared processing, whereas the analytical conditions show both substantial modular and shared processing, we discuss a reconsideration of "modularity of mind" which involves a continuum between strictly modular processing and varying degrees of shared processing, and which also depends on the nature of the tasks compared (i.e., perceptual versus analytical).
Most current models of knowledge organization in the brain are based on hierarchical or taxonomic categories (animals, tools). Another important organizational pattern is thematic categorization, i.e. categories held together by external relations, a unifying scene or event (car and garage). We used fMRI to examine neural activation patterns as subjects performed a category construction task where these two category types were contrasted. Subjects were visually presented with a target word followed by the presentation of two match words and had to choose by button press one match that goes best with the target word. In the balanced or cross-categorization condition (Car/Garage Bus) both match words fit the target; in the biased conditions only one match word fit the target either thematically (Car/Garage Brush) or taxonomically (Car/Bus Eraser). We found that in the biased conditions, thematic and taxonomic categories recruited very similar cortical regions: left inferior frontal, middle temporal and occipital regions. In the balanced condition subjects showed no behavioral preference for either thematic or taxonomic matches. However, contrasting signal changes during a subjective taxonomic choice in the presence of a thematic alternative vs. a subjective thematic choice in the presence of a taxonomic alternative required the additional recruitment of right middle frontal gyrus, left precuneus and left thalamus. Our results suggest that thematic relations between objects are processed similarly to taxonomic relations, but require less cerebral processing demand, providing validation for thematic categories as an alternative principle of conceptual organization.
Gender is an important biological determinant of vulnerability to psychosocial stress. We used perfusion based functional magnetic resonance imaging (fMRI) to measure cerebral blood flow (CBF) responses to mild to moderate stress in 32 healthy people (16 males and 16 females). Psychological stress was elicited using mental arithmetic tasks under varying pressure. Stress in men was associated with CBF increase in the right prefrontal cortex (RPFC) and CBF reduction in the left orbitofrontal cortex (LOrF), a robust response that persisted beyond the stress task period. In contrast, stress in women primarily activated the limbic system, including the ventral striatum, putamen, insula and cingulate cortex. The asymmetric prefrontal activity in males was associated with a physiological index of stress responses-salivary cortisol, whereas the female limbic activation showed a lower degree of correlations with cortisol. Conjunction analyses indicated only a small degree of overlap between the stress networks in men and women at the threshold level of P < 0.01. Increased overlap of stress networks between the two genders was revealed when the threshold for conjunction analyses was relaxed to P < 0.05. Further, machine classification was used to differentiate the central stress responses between the two genders with over 94% accuracy. Our study may represent an initial step in uncovering the neurobiological basis underlying the contrasting health consequences of psychosocial stress in men and women.
The aim of the study was to investigate the relationship between personality structure and brain activity of individuals while resting with eyes closed. In the experiment 110 individuals participated (55 males and 55 females). They were clustered into 5 personality types according to the dimensions of general and emotional intelligence, and the five-factor personality model (FFM) -- extraversion (E), neuroticism (N), openness (O), conscientiousness (C) and agreeableness (A). The resting EEG of individuals was analyzed using three methods: a Fast Fourier Transformation (FFT); Approximated entropy (ApEn), and a low resolution brain electromagnetic tomography (LORETA). The results show that most robust differences between personality types were observed in the gamma band, between types with extreme constellations of dimensions (neurotic type - low emotional intelligence and A; high N), or between types with specific combinations of dimensions (introverts with high IQ, versus extraverts with low to average IQ). These differences were also gender specific. In the gamma band females with different personality structures differed much more than did males, whereas in the lower-1 alpha band a reverse pattern was observed. It was further shown that the differences were much more pronounced in the parieto-occipital brain areas than in the frontal areas.
Probabilistic cytoarchitectonic maps in standard reference space provide a powerful tool for the analysis of structure-function relationships in the human brain. While these microstructurally defined maps have already been successfully used in the analysis of somatosensory, motor or language functions, several conceptual issues in the analysis of structure-function relationships still demand further clarification. In this paper, we demonstrate the principle approaches for anatomical localisation of functional activations based on probabilistic cytoarchitectonic maps by exemplary analysis of an anterior parietal activation evoked by visual presentation of hand gestures. After consideration of the conceptual basis and implementation of volume or local maxima labelling, we comment on some potential interpretational difficulties, limitations and caveats that could be encountered. Extending and supplementing these methods, we then propose a supplementary approach for quantification of structure-function correspondences based on distribution analysis. This approach relates the cytoarchitectonic probabilities observed at a particular functionally defined location to the areal specific null distribution of probabilities across the whole brain (i.e., the full probability map). Importantly, this method avoids the need for a unique classification of voxels to a single cortical area and may increase the comparability between results obtained for different areas. Moreover, as distribution-based labelling quantifies the "central tendency" of an activation with respect to anatomical areas, it will, in combination with the established methods, allow an advanced characterisation of the anatomical substrates of functional activations. Finally, the advantages and disadvantages of the various methods are discussed, focussing on the question of which approach is most appropriate for a particular situation.
Decoding emotional prosody is crucial for successful social interactions, and continuous monitoring of emotional intent via prosody requires working memory. It has been proposed by Ross and others that emotional prosody cognitions in the right hemisphere are organized in an analogous fashion to propositional language functions in the left hemisphere. This study aimed to test the applicability of this model in the context of prefrontal cortex working memory functions. BOLD response data were therefore collected during performance of two emotional working memory tasks by participants undergoing fMRI. In the prosody task, participants identified the emotion conveyed in pre-recorded sentences, and working memory load was manipulated in the style of an N-back task. In the matched lexico-semantic task, participants identified the emotion conveyed by sentence content. Block-design neuroimaging data were analyzed parametrically with SPM5. At first, working memory for emotional prosody appeared to be right-lateralized in the PFC, however, further analyses revealed that it shared much bilateral prefrontal functional neuroanatomy with working memory for lexico-semantic emotion. Supplementary separate analyses of males and females suggested that these language functions were less bilateral in females, but their inclusion did not alter the direction of laterality. It is concluded that Ross et al.'s model is not applicable to prefrontal cortex working memory functions, that evidence that working memory cannot be subdivided in prefrontal cortex according to material type is increased, and that incidental working memory demands may explain the frontal lobe involvement in emotional prosody comprehension as revealed by neuroimaging studies.
We describe here a classification system based on automatically identified cortical sulci. Multivariate recognition methods are required for the detection of complex brain patterns with a spatial distribution. However, such methods may face the well-known issue of the curse of dimensionality-the risk of overfitting the training dataset in high-dimensional space. We overcame this problem, using a classifier pipeline with one- or two-stage of descriptor selection based on machine-learning methods, followed by a support vector machine classifier or linear discriminant analysis. We compared alternative designs of the pipeline on two different datasets built from the same database corresponding to 151 brains. The first dataset dealt with cortex asymmetry and the second dealt with the effect of the subject's sex. Our system successfully (98%) distinguished between the left and right hemispheres on the basis of sulcal shape (size, depth, etc.). The sex of the subject could be determined with a success rate of 85%. These results highlight the attractiveness of multivariate recognition models combined with appropriate descriptor selection. The sulci selected by the pipeline are consistent with previous whole-brain studies on sex effects and hemispheric asymmetries.
Priming is a nonconscious form of memory in which an encounter with a stimulus influences the subsequent identification, production or classification of the same or a related stimulus. Neuroimaging studies have revealed that behavioral priming is typically accompanied by reduced activity in several cortical regions. We review recent studies that have concerned two key issues. First, specificity effects produced by changes between study and test in either the physical features of stimuli or the behavioral response reveal cortical sensitivity to the perceptual, conceptual and stimulus-to-decision mapping properties of primed items. Second, correlations between behavioral priming and activity reductions are robust across a range of tasks and procedures in prefrontal regions but not in posterior regions. On the basis of these recent studies, we suggest that the reduction in cortical activity during priming involves at least two different mechanisms.
Recent work has linked mentalising ability to ventromedial frontal brain regions, the temporal poles and the temporo-parietal junction. The present study set out to examine the performance of participants with focal frontal and posterior lesions and a matched healthy control group on mentalising tasks with different types of pragmatic materials. Four types of materials were used: control physical events, human actions, and direct and indirect sarcastic remarks. Ability to interpret these was tested by asking participants both to explain the events, actions or remarks, and then to choose the best solution from four alternatives presented. Those with frontal lesions were impaired in comprehension of each of the sets of mentalistic materials, but were intact in comprehension of the control non-mentalistic items. There was some evidence linking the generation of free responses for the mentalistic materials to lateral frontal regions; this may be mediated by executive skills. There was also evidence linking selection amongst alternative solutions to right frontal regions, particularly ventromedial areas. There was little evidence that posterior regions played any significant part, at least for the present mentalistic materials. Errors in sarcasm comprehension made by participants with frontal lesions revealed that these were not always literal in nature, suggesting two separable components in comprehension: appreciating that a meaning is not intended literally, and understanding the specific meaning in the social context.
When humans are engaged in goal-related processing, activity in prefrontal cortex is increased. However, it has remained unclear whether this prefrontal activity encodes a subject's current intention. Instead, increased levels of activity could reflect preparation of motor responses, holding in mind a set of potential choices, tracking the memory of previous responses, or general processes related to establishing a new task set. Here we study subjects who freely decided which of two tasks to perform and covertly held onto an intention during a variable delay. Only after this delay did they perform the chosen task and indicate which task they had prepared. We demonstrate that during the delay, it is possible to decode from activity in medial and lateral regions of prefrontal cortex which of two tasks the subjects were covertly intending to perform. This suggests that covert goals can be represented by distributed patterns of activity in the prefrontal cortex, thereby providing a potential neural substrate for prospective memory. During task execution, most information could be decoded from a more posterior region of prefrontal cortex, suggesting that different brain regions encode goals during task preparation and task execution. Decoding of intentions was most robust from the medial prefrontal cortex, which is consistent with a specific role of this region when subjects reflect on their own mental states.
We frequently encounter conflicting emotion cues. This study examined how the neural response to emotional prosody differed in the presence of congruent and incongruent lexico-semantic cues. Two hypotheses were assessed: (i) decoding emotional prosody with conflicting lexico-semantic cues would activate brain regions associated with cognitive conflict (anterior cingulate and dorsolateral prefrontal cortex) or (ii) the increased attentional load of incongruent cues would modulate the activity of regions that decode emotional prosody (right lateral temporal cortex). While the participants indicated the emotion conveyed by prosody, functional magnetic resonance imaging data were acquired on a 3T scanner using blood oxygenation level-dependent contrast. Using SPM5, the response to congruent cues was contrasted with that to emotional prosody alone, as was the response to incongruent lexico-semantic cues (for the 'cognitive conflict' hypothesis). The right lateral temporal lobe region of interest analyses examined modulation of activity in this brain region between these two contrasts (for the 'prosody cortex' hypothesis). Dorsolateral prefrontal and anterior cingulate cortex activity was not observed, and neither was attentional modulation of activity in right lateral temporal cortex activity. However, decoding emotional prosody with incongruent lexico-semantic cues was strongly associated with left inferior frontal gyrus activity. This specialist form of conflict is therefore not processed by the brain using the same neural resources as non-affective cognitive conflict and neither can it be handled by associated sensory cortex alone. The recruitment of inferior frontal cortex may indicate increased semantic processing demands but other contributory functions of this region should be explored.
Although previous studies have implicated a diverse set of brain regions in reward-related decision making, it is not yet known which of these regions contain information that directly reflects a decision. Here, we measured brain activity using functional MRI in a group of subjects while they performed a simple reward-based decision-making task: probabilistic reversal-learning. We recorded brain activity from nine distinct regions of interest previously implicated in decision making and separated out local spatially distributed signals in each region from global differences in signal. Using a multivariate analysis approach, we determined the extent to which global and local signals could be used to decode subjects' subsequent behavioral choice, based on their brain activity on the preceding trial. We found that subjects' decisions could be decoded to a high level of accuracy on the basis of both local and global signals even before they were required to make a choice, and even before they knew which physical action would be required. Furthermore, the combined signals from three specific brain areas (anterior cingulate cortex, medial prefrontal cortex, and ventral striatum) were found to provide all of the information sufficient to decode subjects' decisions out of all of the regions we studied. These findings implicate a specific network of regions in encoding information relevant to subsequent behavioral choice.
Several functional areas are proposed to reside in human lateral occipitotemporal cortex, including the motion-selective human homolog of macaque area MT (hMT), object-form-selective lateral occipital complex (LO), and body-selective extrastriate body area (EBA). Indeed, several functional magnetic resonance imaging (fMRI) studies have reported significant activation overlap among these regions. The standard interpretation of this overlap would be that the common areas of activation reflect engagement of common neural systems. Alternatively, motion, object form, and body form may be processed independently within this general region. To distinguish these possibilities, we first analyzed the lateral occipitotemporal responses to motion, objects, bodies, and body parts with whole-brain group-average analyses and within-subjects functional region of interest (ROI) analyses. The activations elicited by these stimuli, each relative to a matched control, overlapped substantially in the group analysis. When hMT, LO, and EBA were defined functionally within subjects, each ROI in each hemisphere (except right-hemisphere hMT) showed significant selectivity for motion, intact objects, bodies, and body parts, although only the peak voxel of each region was tested. In contrast, multi-voxel analyses of variations in selectivity patterns revealed that visual motion, object form, and the form of the human body elicited three relatively independent patterns of fMRI activity in lateral occipitotemporal cortex. Multi-voxel approaches, in contrast to other methods, can reveal the functional significance of overlapping fMRI activity in extrastriate cortex and, by extension, elsewhere in the brain.
Simulations provide a way of generating data where ground truth is known, enabling quantitative testing of image processing methods. In this paper, we present the construction of 20 realistic digital brain phantoms that can be used to simulate medical imaging data. The phantoms are made from 20 normal adults to take into account intersubject anatomical variabilities. Each digital brain phantom was created by registering and averaging four T1, T2, and proton density (PD)-weighted magnetic resonance imaging (MRI) scans from each subject. A fuzzy minimum distance classification was used to classify voxel intensities from T1, T2, and PD average volumes into grey-matter, white matter, cerebro-spinal fluid, and fat. Automatically generated mask volumes were required to separate brain from nonbrain structures and ten fuzzy tissue volumes were created: grey matter, white matter, cerebro-spinal fluid, skull, marrow within the bone, dura, fat, tissue around the fat, muscles, and skin/muscles. A fuzzy vessel class was also obtained from the segmentation of the magnetic resonance angiography scan of the subject. These eleven fuzzy volumes that describe the spatial distribution of anatomical tissues define the digital phantom, where voxel intensity is proportional to the fraction of tissue within the voxel. These fuzzy volumes can be used to drive simulators for different modalities including MRI, PET, or SPECT. These phantoms were used to construct 20 simulated T1-weighted MR scans. To evaluate the realism of these simulations, we propose two approaches to compare them to real data acquired with the same acquisition parameters. The first approach consists of comparing the intensities within the segmented classes in both real and simulated data. In the second approach, a whole brain voxel-wise comparison between simulations and real T1-weighted data is performed. The first comparison underlines that segmented classes appear to properly represent the anatomy on average, and that inside these classes, the simulated and real intensity values are quite similar. The second comparison enables the study of the regional variations with no a priori class. The experiments demonstrate that these variations are small when real data are corrected for intensity nonuniformity.
In the present study, we compared the effects of temporal compression (averaging across multiple scans) and space selection (i.e. selection of "regions of interest" from the whole brain) on single-subject and multi-subject classification of fMRI data using the support vector machine (SVM). Our aim was to investigate various data transformations that could be applied before training the SVM to retain task discriminatory variance while suppressing irrelevant components of variance. The data were acquired during a blocked experiment design: viewing unpleasant (Class 1), neutral (Class 2) and pleasant pictures (Class 3). In the multi-subject level analysis, we used a "leave-one-subject-out" approach, i.e. in each iteration, we trained the SVM using data from all but one subject and tested its performance in predicting the class label of the this last subject's data. In the single-subject level analysis, we used a "leave-one-block-out" approach, i.e. for each subject, we selected randomly one block per condition to be the test block and trained the SVM using data from the remaining blocks. Our results showed that in a single-subject level both temporal compression and space selection improved the SVM accuracy. However, in a multi-subject level, the temporal compression improved the performance of the SVM, but the space selection had no effect on the classification accuracy.
Most current models of the neurophysiology of basic reading processes agree on a system involving two cortical streams: a ventral stream (occipital-temporal) used when accessing familiar words encoded in lexical memory, and a dorsal stream (occipital-parietal-frontal) used when phonetically decoding words (i.e., mapping sublexical spelling onto sounds). The models diverge, however, on the issue of whether the insular cortex is involved. The present fMRI study required participants to read aloud exception words (e.g., 'one', which must be read via lexical memory) and pseudohomophones (e.g., 'wun', which must be read via sublexical spelling to sound translation) to examine the processing streams as well as the insular cortex, and their relationship to lexical and sublexical reading processes. The present study supports the notion of independent ventral-lexical and dorsal-sublexical streams, and further suggests the insular cortex to be sensitive to phonological processing (particularly sublexical spelling-sound translation). These latter findings illuminate the nature of insular activity during reading, which must be explored further in future studies, and accounted for in models of the neurophysiology of reading.
One of the least well understood regions of the human brain is rostral prefrontal cortex, approximating Brodmann's area 10. Here, we investigate the possibility that there are functional subdivisions within this region by conducting a meta-analysis of 104 functional neuroimaging studies (using positron emission tomography/functional magnetic resonance imaging). Studies involving working memory and episodic memory retrieval were disproportionately associated with lateral activations, whereas studies involving mentalizing (i.e., attending to one's own emotions and mental states or those of other agents) were disproportionately associated with medial activations. Functional variation was also observed along a rostral-caudal axis, with studies involving mentalizing yielding relatively caudal activations and studies involving multiple-task coordination yielding relatively rostral activations. A classification algorithm was trained to predict the task, given the coordinates of each activation peak. Performance was well above chance levels (74% for the three most common tasks; 45% across all eight tasks investigated) and generalized to data not included in the training set. These results point to considerable functional segregation within rostral prefrontal cortex.
The human brain supports acquisition mechanisms that extract structural regularities implicitly from experience without the induction of an explicit model. It has been argued that the capacity to generalize to new input is based on the acquisition of abstract representations, which reflect underlying structural regularities in the input ensemble. In this study, we explored the outcome of this acquisition mechanism, and to this end, we investigated the neural correlates of artificial syntactic classification using event-related functional magnetic resonance imaging. The participants engaged once a day during an 8-day period in a short-term memory acquisition task in which consonant-strings generated from an artificial grammar were presented in a sequential fashion without performance feedback. They performed reliably above chance on the grammaticality classification tasks on days 1 and 8 which correlated with a corticostriatal processing network, including frontal, cingulate, inferior parietal, and middle occipital/occipitotemporal regions as well as the caudate nucleus. Part of the left inferior frontal region (BA 45) was specifically related to syntactic violations and showed no sensitivity to local substring familiarity. In addition, the head of the caudate nucleus correlated positively with syntactic correctness on day 8 but not day 1, suggesting that this region contributes to an increase in cognitive processing fluency.
This work presents a framework driven by parcellation of brain gray matter in standard normalized space to classify the neuronal fibers obtained from diffusion tensor imaging (DTI) in entire human brain. Classification of fiber bundles into groups is an important step for the interpretation of DTI data in terms of functional correlates of white matter structures. Connections between anatomically delineated brain regions that are considered to form functional units, such as a short-term memory network, are identified by first clustering fibers based on their terminations in anatomically defined zones of gray matter according to Talairach Atlas, and then refining these groups based on geometric similarity criteria. Fiber groups identified this way can then be interpreted in terms of their functional properties using knowledge of functional neuroanatomy of individual brain regions specified in standard anatomical space, as provided by functional neuroimaging and brain lesion studies.
Little is known about the neural correlates of affective prosody in the context of affective semantic discourse. We used functional magnetic resonance imaging to investigate this issue while subjects performed 1) affective classification of sentences having an affective semantic content and 2) grammatical classification of sentences with neutral semantic content. Sentences of each type were produced half by actors and half by a text-to-speech software lacking affective prosody. Compared with neutral sentences processing, sentences with affective semantic content--with or without affective prosody--led to an increase in activation of a left inferior frontal area involved in the retrieval of semantic knowledge. In addition, the posterior part of the left superior temporal sulcus (STS) together with the medial prefrontal cortex were recruited, although not activated by neutral sentences classification. Interestingly, these areas have been described as implicated during self-reflection or other's mental state inference that possibly occurred during the affective classification task. When affective prosody was present, additional rightward activations of the human-selective voice area and the posterior part of STS were observed, corresponding to the processing of speaker's voice emotional content. Accurate affective communication, central to social interactions, requires the cooperation of semantics, affective prosody, and mind-reading neural networks.
We used event-related fMRI to test whether recognition memory depends on visual similarity between familiar prototypes and novel exemplars. Subjects memorized portraits, landscapes, and abstract compositions by six painters with a unique style, and later performed a memory recognition task. The prototypes were presented with new exemplars that were either visually similar or dissimilar. Behaviorally, novel, dissimilar items were detected faster and more accurately. We found activation in a distributed cortical network that included face- and object-selective regions in the visual cortex, where familiar prototypes evoked stronger responses than new exemplars; attention-related regions in parietal cortex, where responses elicited by new exemplars were reduced with decreased similarity to the prototypes; and the hippocampus and memory-related regions in parietal and prefrontal cortices, where stronger responses were evoked by the dissimilar exemplars. Our findings suggest that recognition memory is mediated by classification of novel exemplars as a match or a mismatch, based on their visual similarity to familiar prototypes.
Eighteen healthy young adults underwent event-related (ER) functional magnetic resonance imaging (fMRI) of the brain while performing a visual category learning task. The specific category learning task required subjects to extract the rules that guide classification of quasi-random patterns of dots into categories. Following each classification choice, visual feedback was presented. The average hemodynamic response was calculated across the eighteen subjects to identify the separate networks associated with both classification and feedback. Random-effects analyses identified the different networks implicated during the classification and feedback phases of each trial. The regions included prefrontal cortex, frontal eye fields, supplementary motor and eye fields, thalamus, caudate, superior and inferior parietal lobules, and areas within visual cortex. The differences between classification and feedback were identified as (i) overall higher volumes and signal intensities during classification as compared to feedback, (ii) involvement of the thalamus and superior parietal regions during the classification phase of each trial, and (iii) differential involvement of the caudate head during feedback. The effects of learning were then evaluated for both classification and feedback. Early in learning, subjects showed increased activation in the hippocampal regions during classification and activation in the heads of the caudate nuclei during the corresponding feedback phases. The findings suggest that early stages of prototype-distortion learning are characterized by networks previously associated with strategies of explicit memory and hypothesis testing. However as learning progresses the networks change. This finding suggests that the cognitive strategies also change during prototype-distortion learning.
This paper reviews a body of work conducted in our laboratory that applies functional magnetic resonance imaging (fMRI) to better understand the biological response and change that occurs during prototype-distortion learning. We review results from two experiments (Little, Klein, Shobat, McClure, & Thulborn, 2004; Little & Thulborn, 2005) that provide support for increasing neuronal efficiency by way of a two-stage model that includes an initial period of recruitment of tissue across a distributed network that is followed by a period of increasing specialization with decreasing volume across the same network. Across the two studies, participants learned to classify patterns of random-dot distortions (Posner & Keele, 1968) into categories. At four points across this learning process subjects underwent examination by fMRI using a category-matching task. A large-scale network, altered across the protocol, was identified to include the frontal eye fields, both inferior and superior parietal lobules, and visual cortex. As behavioral performance increased, the volume of activation within these regions first increased and later in the protocol decreased. Based on our review of this work we propose that: (i) category learning is reflected as specialization of the same network initially implicated to complete the novel task, and (ii) this network encompasses regions not previously reported to be affected by prototype-distortion learning.
With recent investigation beginning to reveal the cortical and subcortical neuroanatomical correlates of humor appreciation, the present event-related functional MRI (fMRI) study was designed to elucidate sex-specific recruitment of these humor related networks. Twenty healthy subjects (10 females) underwent fMRI scanning while subjectively rating 70 verbal and nonverbal achromatic cartoons as funny or unfunny. Data were analyzed by comparing blood oxygenation-level-dependent signal activation during funny and unfunny stimuli. Males and females share an extensive humor-response strategy as indicated by recruitment of similar brain regions: both activate the temporal-occipital junction and temporal pole, structures implicated in semantic knowledge and juxtaposition, and the inferior frontal gyrus, likely to be involved in language processing. Females, however, activate the left prefrontal cortex more than males, suggesting a greater degree of executive processing and language-based decoding. Females also exhibit greater activation of mesolimbic regions, including the nucleus accumbens, implying greater reward network response and possibly less reward expectation. These results indicate sex-specific differences in neural response to humor with implications for sex-based disparities in the integration of cognition and emotion.
Although task switching is often considered one of the fundamental abilities underlying executive functioning and general intelligence, there is little evidence that switching is a unitary construct and little evidence regarding the relationship between brain activity and switching performance. We examined individual differences in multiple types of attention shifting in order to determine whether behavioral performance and fMRI activity are correlated across different types of shifting. The participants (n = 39) switched between objects and attributes both when stimuli were perceptually available (external) and when stimuli were stored in memory (internal). We found that there were more switch-related activations in many regions associated with executive control--including the dorsolateral and medial prefrontal and parietal cortices--when behavioral switch costs were higher (poor performance). Conversely, activation in the ventromedial prefrontal cortex (VMPFC) and the rostral anterior cingulate was consistently correlated with good performance, suggesting a general role for these areas in efficient attention shifting. We discuss these findings in terms of a model of cognitive-emotional interaction in attention shifting, in which reward-related signals in the VMPFC guide efficient selection of tasks in the lateral prefrontal and parietal cortices.
Deception is a clinically important behavior with poorly understood neurobiological correlates. Published functional MRI (fMRI) data on the brain activity during deception indicates that, on a multisubject group level, lie is distinguished from truth by increased prefrontal and parietal activity. These findings are theoretically important; however, their applied value will be determined by the accuracy of the discrimination between single deceptive and truthful responses in individual subjects. This study presents the first quantitative estimate of the accuracy of fMRI in conjunction with a formal forced-choice paradigm in detecting deception in individual subjects. We used a paradigm balancing the salience of the target cues to elicit deceptive and truthful responses and determined the accuracy of this model in the classification of single lie and truth events. The relative salience of the task cues affected the net activation associated with lie in the superior medial and inferolateral prefrontal cortices. Lie was discriminated from truth on a single-event level with an accuracy of 78%, while the predictive ability expressed as the area under the curve (AUC) of the receiver operator characteristic curve (ROC) was 85%. Our findings confirm that fMRI, in conjunction with a carefully controlled query procedure, could be used to detect deception in individual subjects. Salience of the task cues is a potential confounding factor in the fMRI pattern attributed to deception in forced choice deception paradigms.
Repetition priming is a nonconscious form of memory that is accompanied by reductions in neural activity when an experience is repeated. To date, however, there is no direct evidence that these neural reductions underlie the behavioral advantage afforded to repeated material. Here we demonstrate a causal linkage between neural and behavioral priming in humans. fMRI (functional magnetic resonance imaging) was used in combination with transcranial magnetic stimulation (TMS) to target and disrupt activity in the left frontal cortex during repeated classification of objects. Left-frontal TMS disrupted both the neural and behavioral markers of priming. Neural priming in early sensory regions was unaffected by left-frontal TMS--a finding that provides evidence for separable conceptual and perceptual components of priming.
Inferences drawn from functional magnetic resonance imaging (fMRI) studies are dependent on the statistical criteria used to define different brain regions as "active" or "inactive" under the experimental manipulation. In fMRI studies of multisensory integration, additional criteria are used to classify a subset of the active brain regions as "multisensory." Because there is no general agreement in the literature on the optimal criteria for performing this classification, we investigated the effects of seven different multisensory statistical criteria on a single test dataset collected as human subjects performed auditory, visual, and auditory- visual object recognition. Activation maps created using the different criteria differed dramatically. The classification of the superior temporal sulcus (STS) was used as a performance measure, because a large body of converging evidence demonstrates that the STS is important for auditory-visual integration. A commonly proposed criterion, "supra-additivity" or "super-additivity", which requires the multisensory response to be larger than the summed unisensory responses, did not classify STS as multisensory. Alternative criteria, such as requiring the multisensory response to be larger than the maximum or the mean of the unisensory responses, successfully classified STS as multisensory. This practical demonstration strengthens theoretical arguments that the super-additivity is not an appropriate criterion for all studies of multisensory integration. Moreover, the importance of examining evoked fMRI responses, whole brain activation maps, maps from multiple individual subjects, and mixed-effect group maps are discussed in the context of selecting statistical criteria.
In neuroimaging studies of word reading in natural scripts, the effect of alphabeticality is often confounded with the effect of practice. We used an artificial script to separately manipulate the effects of practice and alphabeticality following training with and without explicit letter instructions. Participants received multi-session training in reading nonsense words, written in an artificial script, wherein each phoneme was represented by 2 discrete symbols . Three training conditions were compared: alphabetical whole words with letter decoding instruction (explicit); alphabetical whole-words (implicit) and non-alphabetical whole-words (arbitrary). Each participant was trained on the arbitrary condition and on one of the alphabetical conditions (explicit or implicit). fMRI scans were acquired after training during reading of trained words and relatively novel words in the alphabetical and arbitrary conditions. Our results showed greater activation in the explicit compared to the arbitrary conditions, but only for relatively-novel words, in the left posterior inferior frontal gyrus (IFG). In the implicit condition, the left posterior IFG was active in both trained and relatively novel words. These results indicate the involvement of the left posterior IFG in letter decoding, and suggest that reading of explicitly well-trained words did not rely on letter decoding, while in implicitly trained words letter decoding persisted into later stages. The superior parietal lobules showed reduced activation for items that received more practice, across all training conditions. Altogether, our results suggest that the alphabeticality of the word, the amount of practice and type of instructions have independent and interacting effects on brain activation during reading.
Aloud reading of novel words is achieved by phonological decoding, a process in which grapheme-to-phoneme conversion rules are applied to "sound out" a word's spoken representation. Numerous brain imaging studies have examined the neural bases of phonological decoding by contrasting pseudoword (pronounceable nonwords) to real word reading. However, only a few investigations have examined pseudoword reading under both aloud and silent conditions, task parameters that are likely to significantly alter the functional anatomy of phonological decoding. Subjects participated in an fMRI study of aloud pseudoword, aloud real word, silent pseudoword, and silent real word reading. Using this two-by-two design, we examined effects of word-type (real words vs. pseudowords) and response-modality (silent vs. aloud) and their interactions. We found 1) four regions to be invariantly active across the four reading conditions: the anterior aspect of the left precentral gyrus (Brodmann's Area (BA) 6), and three areas within the left ventral occipitotemporal cortex; 2) a main effect of word-type (pseudowords > words) in left inferior frontal gyrus and left intraparietal sulcus; 3) a main effect of response-modality (aloud > silent) that included bilateral motor, auditory, and extrastriate cortex; and 4) a single left hemisphere extrastriate region showing a word-type by response-modality interaction effect. This region, within the posterior fusiform cortex at BA 19, was uniquely modulated by varying phonological processing demands. This result suggests that when reading, word forms are subject to phonological analysis at the point they are first recognized as alphabetic stimuli and BA 19 is involved in processing the phonological properties of words.
This paper treats support vector machine (SVM) classification applied to block design fMRI, extending our previous work with linear discriminant analysis [LaConte, S., Anderson, J., Muley, S., Ashe, J., Frutiger, S., Rehm, K., Hansen, L.K., Yacoub, E., Hu, X., Rottenberg, D., Strother S., 2003a. The evaluation of preprocessing choices in single-subject BOLD fMRI using NPAIRS performance metrics. NeuroImage 18, 10-27; Strother, S.C., Anderson, J., Hansen, L.K., Kjems, U., Kustra, R., Siditis, J., Frutiger, S., Muley, S., LaConte, S., Rottenberg, D. 2002. The quantitative evaluation of functional neuroimaging experiments: the NPAIRS data analysis framework. NeuroImage 15, 747-771]. We compare SVM to canonical variates analysis (CVA) by examining the relative sensitivity of each method to ten combinations of preprocessing choices consisting of spatial smoothing, temporal detrending, and motion correction. Important to the discussion are the issues of classification performance, model interpretation, and validation in the context of fMRI. As the SVM has many unique properties, we examine the interpretation of support vector models with respect to neuroimaging data. We propose four methods for extracting activation maps from SVM models, and we examine one of these in detail. For both CVA and SVM, we have classified individual time samples of whole brain data, with TRs of roughly 4 s, thirty slices, and nearly 30,000 brain voxels, with no averaging of scans or prior feature selection.
In macaque monkeys performing a memory-guided saccade task for a reward of variable size, neuronal activity in several areas of frontal cortex is stronger when the monkey anticipates a larger reward. This effect might depend on either the size or the value of the reward. To distinguish between these possibilities, we recorded from neurons in frontal cortex while controlling value through a manipulation of time rather than amount. A cue presented at the beginning of each trial, predicted the length of the delay during which the monkey would have to maintain fixation before performing a saccade and receiving a reward of fixed size. Predicting a short delay had effects closely similar to those of predicting a large reward: 1) monkeys were more motivated when working for a reward at short delay, 2) neurons tended to fire more strongly before a short delay, 3) individual neurons firing more strongly before a short delay tended also to fire more strongly before a large reward, and 4) the tendency to fire more strongly before a short delay was far more pronounced in premotor areas caudal to the arcuate sulcus than in association areas rostral to it. The association areas, in contrast, were marked by a tendency for neurons to fire more strongly at the end of the long delay. We conclude that predicting a short delay, like predicting a large reward, induces an enhancement of neuronal activity related to motivational modulation of the monkey's preparatory state.
Human functional imaging and neurocytology have produced important revisions to the organization of the cingulate gyrus and demonstrate four structure/function regions: anterior, midcingulate (MCC), posterior (PCC), and retrosplenial. This study evaluates the brain of a rhesus and 11 cynomolgus monkeys with Nissl staining and immunohistochemistry for neuron-specific nuclear binding protein, intermediate neurofilament proteins, and parvalbumin. The MCC region was identified along with its two subdivisions (a24' and p24'). The transition between areas 24 and 23 does not involve a simple increase in the number of neurons in layer IV but includes an increase in neuron density in layer Va of p24', a dysgranular layer IV in area 23d, granular area 23, with a neuron-dense layer Va and area 31. Each area on the dorsal bank of the cingulate gyrus has an extension around the fundus of the cingulate sulcus (f 24c, f 24c', f 24d, f 23c), whereas most cortex on the dorsal bank is composed of frontal motor areas. The PCC is composed of a dysgranular area 23d, area 23c in the caudal cingulate sulcus, a dorsal cingulate gyral area 23a/b, and a ventral area 23a/b. Finally, a dysgranular transition zone includes both area 23d and retrosplenial area 30. The distribution of areas was plotted onto flat maps to show the extent of each and their relationships to the vertical plane at the anterior commissure, corpus callosum, and cingulate sulcus. This major revision of the architectural organization of monkey cingulate cortex provides a new context for connection studies and for devising models of neuron diseases.
The caudate nucleus is commonly active when learning relationships between stimuli and responses or categories. Previous research has not differentiated between the contributions to learning in the caudate and its contributions to executive functions such as feedback processing. We used event-related functional magnetic resonance imaging while participants learned to categorize visual stimuli as predicting "rain" or "sun." In each trial, participants viewed a stimulus, indicated their prediction via a button press, and then received feedback. Conditions were defined on the bases of stimulus-outcome contingency (deterministic, probabilistic, and random) and feedback (negative and positive). A region of interest analysis was used to examine activity in the head of the caudate, body/tail of the caudate, and putamen. Activity associated with successful learning was localized in the body and tail of the caudate and putamen; this activity increased as the stimulus-outcome contingencies were learned. In contrast, activity in the head of the caudate and ventral striatum was associated most strongly with processing feedback and decreased across trials. The left superior frontal gyrus was more active for deterministic than probabilistic stimuli; conversely, extrastriate visual areas were more active for probabilistic than deterministic stimuli. Overall, hippocampal activity was associated with receiving positive feedback but not with correct classification. Successful learning correlated positively with activity in the body and tail of the caudate nucleus and negatively with activity in the hippocampus.
Using event-related functional magnetic resonance imaging, we identified brain regions involved in successful relational memory (RM) during encoding and retrieval for semantic and perceptual associations or in general, independent of phase and content. Participants were scanned while encoding and later retrieving associations between pairs of words (semantic RM) or associations between words and fonts (perceptual RM). Encoding success activity (ESA) was identified by comparing study-phase activity for items subsequently remembered (hits) versus forgotten (misses) and retrieval success activity (RSA) by comparing test-phase activity for hits versus misses. The study yielded three main sets of findings. First, ESA-RSA differences were found within the medial temporal lobes (MTLs) and within the prefrontal cortex (PFC). Within the left MTL, ESA was greater in the anterior hippocampus, and RSA was greater in the posterior parahippocampal cortex/hippocampus. This finding is consistent with the notion of an encoding-retrieval gradient along the longitudinal MTL axis. Within the left PFC, ESA was greater in ventrolateral PFC, and RSA was greater in dorsolateral and anterior PFC. This is the first evidence of a dissociation in successful encoding and retrieval activity within left PFC. Second, consistent with the transfer-appropriate processing principle, some ESA regions were reactivated during RSA in a content-specific manner. For semantic RM, these regions included the left ventrolateral PFC, whereas for perceptual RM, they included occipitoparietal and right parahippocampal regions. Finally, only one region in the entire brain was associated with RM in general (i.e., for both semantic and perceptual ESA and RSA): the left hippocampus. This finding highlights the fundamental role of the hippocampus in RM.
Studies of human classification learning using functional neuroimaging have suggested that basal ganglia and medial temporal lobe memory systems may interact during learning. We review these results and outline a set of possible mechanisms for such interactions. Effective connectivity analyses suggest that interaction between basal ganglia and medial temporal lobe are mediated by prefrontal cortex rather than by direct connectivity between regions. A review of possible neurobiological mechanisms suggests that interactions may be driven by neuromodulatory systems in addition to mediation by interaction of inputs to prefrontal cortical neurons. These results suggest that memory system interactions may reflect multiple mechanisms that combine to optimize behavior based on experience.
The orbitofrontal cortex contains the secondary taste cortex, in which the reward value of taste is represented. It also contains the secondary and tertiary olfactory cortical areas, in which information about the identity and also about the reward value of odours is represented. The orbitofrontal cortex also receives information about the sight of objects from the temporal lobe cortical visual areas, and neurons in it learn and reverse the visual stimulus to which they respond when the association of the visual stimulus with a primary reinforcing stimulus (such as taste) is reversed. This is an example of stimulus-reinforcement association learning, and is a type of stimulus-stimulus association learning. More generally, the stimulus might be a visual or olfactory stimulus, and the primary (unlearned) positive or negative reinforcer a taste or touch. A somatosensory input is revealed by neurons that respond to the texture of food in the mouth, including a population that responds to the mouth feel of fat. In complementary neuroimaging studies in humans, it is being found that areas of the orbitofrontal cortex are activated by pleasant touch, by painful touch, by taste, by smell, and by more abstract reinforcers such as winning or losing money. Damage to the orbitofrontal cortex can impair the learning and reversal of stimulus-reinforcement associations, and thus the correction of behavioural responses when there are no longer appropriate because previous reinforcement contingencies change. The information which reaches the orbitofrontal cortex for these functions includes information about faces, and damage to the orbitofrontal cortex can impair face (and voice) expression identification. This evidence thus shows that the orbitofrontal cortex is involved in decoding and representing some primary reinforcers such as taste and touch; in learning and reversing associations of visual and other stimuli to these primary reinforcers; and in controlling and correcting reward-related and punishment-related behavior, and thus in emotion. The approach described here is aimed at providing a fundamental understanding of how the orbitofrontal cortex actually functions, and thus in how it is involved in motivational behavior such as feeding and drinking, in emotional behavior, and in social behavior.
Mesencephalic dopaminergic system (MDS) neurons may participate in learning by providing a prediction error signal to their targets, which include ventral striatal, orbital, and medial frontal regions, as well as by showing sensitivity to the degree of uncertainty associated with individual stimuli. We investigated the mechanisms of probabilistic classification learning in humans using functional magnetic resonance imaging to examine the effects of feedback and uncertainty. The design was optimized for separating neural responses to stimulus, delay, and negative and positive feedback components. Compared with fixation, stimulus and feedback activated brain regions consistent with the MDS, whereas the delay period did not. Midbrain activity was significantly different for negative versus positive feedback (consistent with coding of the "prediction error") and was reliably correlated with the degree of uncertainty as well as with activity in MDS target regions. Purely cognitive feedback apparently engages the same regions as rewarding stimuli, consistent with a broader characterization of this network.
Recent observation of objects speeds up their subsequent identification and classification. This common form of learning, known as repetition priming, can operate in the absence of explicit memory for earlier experiences, and functional neuroimaging has shown that object classification improved in this way is accompanied by 'neural priming' (reduced neural activity) in prefrontal, fusiform and other cortical regions. These observations have led to suggestions that cortical representations of items undergo 'tuning', whereby neurons encoding irrelevant information respond less as a given object is observed repeatedly, thereby facilitating future availability of pertinent object knowledge. Here we provide experimental support for an alternative hypothesis, in which reduced cortical activity occurs because subjects rapidly learn their previous responses. After a primed object classification (such as 'bigger than a shoebox'), cue reversal ('smaller than a shoebox') greatly slowed performance and completely eliminated neural priming in fusiform cortex, which suggests that these cortical item representations were no more available for primed objects than they were for new objects. In contrast, prefrontal cortex activity tracked behavioural priming and predicted the degree to which cue reversal would slow down object classification--highlighting the role of the prefrontal cortex in executive control.
The control of food intake and the mechanisms of energy homeostasis are now known to depend on a series of peripheral signals that act directly on the central nervous system leading to appropriate adaptive responses. However, in humans, the increasing occurrence of associated pathologies due to abnormal food-intake preferences such as obesity and anorexia implies that food intake control depend also on cortical processing. Recent functional neuroimaging studies on human volunteers reveal that the central processing of gustatory information in humans is performed in similar areas to those of other primates, with primary gustatory cortical areas in the frontal operculum/anterior insula complex responding efficiently to stimulus decoding by isolating peripheral signals on internal physiological states whereas regions of the ventromedial prefrontal cortex seem to integrate information on the sensory aspects of taste stimuli with the abovementioned peripheral signals on the current homeostatic state of the organism.
Structural and functional hippocampal abnormalities have been previously reported in institutionalized psychopathic and aggressive populations. This study assessed whether prior findings of a right greater than left (R > L) functional asymmetry in caught violent offenders generalize to the structural domain in unsuccessful, caught psychopaths.
In the present study, an implicit strategy manipulation was used to explore the contribution of memory strategy to brain activation and behavioral performance. Participants were biased to use either a short-term (maintenance-focused) or long-term (retrieval-focused) memory strategy within a single memory task through manipulation of task context. In comparing directly matched trials across the different task contexts, we observed clear changes in both behavioral performance and brain activity across a network of regions located primarily within lateral and medial frontal cortex. These effects of the memory strategy manipulation suggest that when a retrieval-focused strategy is induced, mnemonic processes are preferentially engaged during the encoding period. In contrast, when a maintenance-focused strategy is induced, mnemonic processes are preferentially engaged during the delay and response periods. Taken together, the results imply that covert cognitive strategies play an important role in modulating brain activation and behavior during memory tasks.
The authors used functional magnetic resonance imaging (fMRI) to define the neural regions mediating self-referential processing of emotional stimuli and to explore how these regions are influenced by the emotional valence of the stimulus.
Memory retrieval is to bring the remembered information on-line or to reactivate the information. The critical determinant of memory retrieval mechanisms is whether the information has been maintained on-line or off-line, regardless of whether it is long-term memory or short-term, working memory. Similar reactivation processes occur during retrieval from long-term memory and from working memory when online maintenance has been interrupted. The reactivation is achieved by interactions between the posterior association areas, medial temporal lobe and prefrontal cortex. Posterior association areas maintain the representations of remembered information and are reactivated at retrieval. The medial temporal lobe is primarily involved in retrieval from off-line memory and triggers the reactivation by associating a whole set of features and episodes during encoding of the information. The prefrontal cortex is involved in retrieval from both on-line and off-line memory. It controls reactivation by setting up retrieval mode, starting retrieval attempt, and monitoring the contents of reactivated information. The prefrontal cortex also controls the selection of task-relevant information from information maintained on-line.
Recently, patent foramen ovale(PFO) has been highlighted as an important risk factor of cerebral infarctions in young adults. We report a patient of multiple cerebral embolism associated with PFO and deep venous thrombosis caused by a uterine myoma. A 40-year-old woman suddenly suffered from right hemiparesis with motor aphasia. Brain angiography showed an occlusion of M2 portion of the left middle cerebral artery, but atherosclerotic changes were not seen. She developed left facial paresis 23 days later and admitted to our hospital. Brain MRI revealed multiple cerebral infarcts in the left insular cortex, the deep white matter of the right frontal lobe, and bilateral thalamus. Hypoxia with the perfusion defects of S1 and S2 sections of the right lung demonstrated by scintigraphy suggested pulmonary embolism. Transesophageal echocardiography showed a PFO with spontaneous left-to-right shunt and right-to-left shunt evoked by the Valsalva maneuver. Although venography could not detect thrombi, it revealed severe compression of the right external iliac vein by a uterine myoma. These findings suggested thrombi in the right external iliac vein were the embolic source when combined with elevated coagulation markers. An uterine myoma should be considered as an important risk factor for an embolic source in case of cerebral embolism with PFO.
We report on an adult patient with a right frontal astrocytoma, classification WHO II, who suffered from radionecrosis 3.5 years after surgery and combined radio- and chemotherapy. Beginning 8 years after initial diagnosis, repeated episodes of bilateral cerebral hemorrhage and cavitation occurred. This case description emphasizes the possibility of repeated hemorrhage as a delayed reaction to brain irradiation and chemotherapy.
Deception is a complex cognitive activity, and different types of lies could arise from different neural systems. We investigated this possibility by first classifying lies according to two dimensions, whether they fit into a coherent story and whether they were previously memorized. fMRI revealed that well-rehearsed lies that fit into a coherent story elicit more activation in right anterior frontal cortices than spontaneous lies that do not fit into a story, whereas the opposite pattern occurs in the anterior cingulate and in posterior visual cortex. Furthermore, both types of lies elicited more activation than telling the truth in anterior prefrontal cortices (bilaterally), the parahippocampal gyrus (bilaterally), the right precuneus, and the left cerebellum. At least in part, distinct neural networks support different types of deception.
Theories of perception have proposed a basic distinction between parallel pre-attentive and serial attentive modes of processing. However, chronometric measures are often ambiguous in separating parallel and serial processes. We have used the activity of attention-related regions of the human brain, measured with functional magnetic resonance imaging, to separate parallel from serial processes at the single-trial level in a visual quantification task. In this task, some have suggested the deployment of two qualitatively different processes, a fast parallel 'subitizing' for sets of one, two or three objects and a slow serial counting for larger sets. Our results indicate that attention-related regions of the posterior parietal and frontal cortices show a sudden increase in activity only from numerosity four onwards, confirming the parallel-serial dichotomy of subitizing and counting. Moreover, using the presence or absence of attentional shifts, as inferred from the activation of posterior parietal regions, we successfully predict whether, on a given trial, subjects deployed a serial exploration of the display or a parallel apprehension. Beyond the subitizing/counting debate, this approach may prove useful to probe the attentional demands of other cognitive tasks.
Subsystems of category learning have been identified on the basis of general domains of content (e.g., tools, faces). The present study examined categories from the standpoint of internal structure and determined brain topography associated with expressing two fundamentally different category rule structures (criterion attribute, CA, and family resemblance, FR). CA category learning involves processing stimuli by isolated features and classifying by properties held by all members. FR learning involves processing stimuli by integral wholes and classifying on overall similarity among members without sharing identical features. fMRI BOLD response to CA and FR categorization was measured with pseudowords as stimuli. Category knowledge for both tasks was mastered prior to brain imaging. Areas of activation emerged unique to the structure of each category and followed from the nature of the rule abstraction procedure. CA categorization was implemented by strong target monitoring and expectation (medial parietal), rule maintenance in working memory, feature selection processes (inferior frontal), and a sensitivity to high frequency components of the stimulus such as isolated features (anterior temporal). FR categorization, consistent with its multi-featural nature, involved word-level processing (left extrastriate) that evoked articulatory rehearsal (medial cerebellar). The data suggest category structure is an important determinant of brain response during categorization. For instance, anterior temporal structures may help attune visual processing systems to high frequency components to support the learning of criterial, highly predictive rules.
The movements of the faces and bodies of other conspecifics provide stimuli of considerable interest to the social primate. Studies of single cells, field potential recordings and functional neuroimaging data indicate that specialized visual mechanisms exist in the superior temporal sulcus (STS) of both human and non-human primates that produce selective neural responses to moving natural images of faces and bodies. STS mechanisms also process simplified displays of biological motion involving point lights marking the limb articulations of animate bodies and geometrical shapes whose motion simulates purposeful behaviour. Facial movements such as deviations in eye gaze, important for gauging an individual's social attention, and mouth movements, indicative of potential utterances, generate particularly robust neural responses that differentiate between movement types. Collectively such visual processing can enable the decoding of complex social signals and through its outputs to limbic, frontal and parietal systems the STS may play a part in enabling appropriate affective responses and social behaviour.
Changes in regional blood oxygen level dependent (BOLD) signals in response to brief visual stimuli can exhibit a variety of time-courses. To demonstrate the anatomical distribution of BOLD response shapes during a match to sample task, a formal analysis of their time-courses is presented. An event-related design was used to estimate regional BOLD responses evoked by a cue word, which instructed the subject to attend to the motion or color of an upcoming target, and those evoked by a briefly presented moving target consisting of colored dots. Regional BOLD time-courses were adequately represented by the linear combination of three orthogonal waveforms. BOLD response shapes were then classified using a fuzzy clustering scheme. Three classes (sustained, phasic, and negative) best characterized cue responses. Four classes (sustained, sustained-phasic, phasic, and bi-phasic) best characterized target responses. In certain regions, the shape of the BOLD responses was modulated by the instruction to attend to the target's motion or color. A left frontal and a posterior parietal region showed sustained activity when motion was cued and transient activity when color was cued. A right thalamic and a left lateral occipital region showed sustained activity when color was cued and transient activity when motion was cued. Following the target several regions showed more sustained activity during motion than color trials. In summary, the effect of the task variable was focal following the cue and widespread following the target. We conclude that the temporal patterns of neural activity affected the shape of the BOLD signal.
The purpose of this study was to estimate the development of the gyrus and sulcus formation in normal fetuses on the basis of the neuroanatomical findings using MR images in relation to gestational age.
This article represents a symposium of the 2002 joint meeting of RSA and ISBRA held in San Francisco. Presentations were Neuropathology of alcohol-related cerebellar damage in humans, by Antony J. Harding; Neuropathological evidence of cerebellar damage in an animal model of alcoholism, by Roberta Pentney and Cynthia Dlugos; Understanding cortical-cerebellar circuits through neuroimaging study of chronic alcoholics, by Peter R. Martin and Mitchell H. Parks; and Functional reorganization of the brain in alcoholism: neuroimaging evidence, by John E. Desmond, S.H. Annabel Chen, Michelle R. Pryor, Eve De Rosa, Adolf Pfefferbaum, and Edith V. Sullivan.
Striatal learning systems have been implicated in learning relationships between visual stimuli and outcomes. In the present study, the activity of the striatum during visual concept learning in humans was examined by using functional magnetic resonance imaging (fMRI). Participants performed three concept-learning tasks and a baseline task. The participants were trained to criterion before fMRI scanning on two tasks, verbal and implicit. In the verbal task, classification could be performed on the basis of a simple verbal rule, but in the implicit task, there was no simple verbal rule. The novel-implicit learning task, in which an implicit structure was used, was not encountered by the participants before scanning. Across all three concept-learning tasks, there was significant activation in the striatum, in comparison with the baseline task. The striatum was recruited similarly in classification when the participants had different levels of expertise (novel-implicit vs. verbal and implicit) and were able to verbalize their learning to different degrees (verbal vs. implicit and novel-implicit). There was left lateral occipital activation when learning was implicit (implicit and novel-implicit), but not when learning was easily verbalized (verbal).
Functional magnetic resonance imaging (fMRI) was used to examine the functional anatomy of word comprehension in the auditory and visual modalities of presentation. We asked our subjects to determine if word pairs were semantically associated (e.g., table, chair) and compared this to a reference task where they were asked to judge whether word pairs rhymed (e.g., bank, tank). This comparison showed task-specific and modality-independent activation for semantic processing in the heteromodal cortices of the left inferior frontal gyrus (BA 46, 47) and left middle temporal gyrus (BA 21). There were also modality-specific activations in the fusiform gyrus (BA 37) for written words and in the superior temporal gyrus (BA 22) for spoken words. Our findings are consistent with the hypothesis that word form recognition (lexical encoding) occurs in unimodal cortices and that heteromodal brain regions in the anterior as well as posterior components of the language network subserve word comprehension (semantic decoding).
Categorization of dot patterns is a frequently used paradigm in the behavioral study of natural categorization. To determine the human brain regions involved in categorization, we used Positron Emission Tomography to compare regional Cerebral Blood Flow patterns in two tasks employing patterns that consisted of nine dots. In the categorization task, subjects categorized novel exemplars of two categories, generated by distorting two prototypes, and other random dot patterns. In the control task, subjects judged the position of similarly distorted patterns. Each task was presented at two matched levels of difficulty. Fixation of the fixation target served as baseline condition. The categorization task differentially activated the orbitofrontal cortex and two dorsolateral prefrontal regions. These three prefrontal regions were equally weakly active in the position discrimination task and the baseline condition. The intraparietal sulcus was activated in both tasks, albeit significantly less in the position discrimination than in the categorization task. A similar activation pattern was present in the neostriatum. Task difficulty had no effect. These functional imaging results show that the dot-pattern categorization task strongly engages prefrontal and parietal cortical areas. The activation of prefrontal cortex during visual categorization in humans agrees with the recent finding of category-related responses in macaque prefrontal neurons.
Knowledge about environmental objects derives from representations of multiple object features both within and across sensory modalities. While our understanding of the neural basis for visual object representation in the human and nonhuman primate brain is well advanced, a similar understanding of auditory objects is in its infancy. We used a name verification task and functional magnetic resonance imaging (fMRI) to characterize the neural circuits that are activated as human subjects match visually presented words with either simultaneously presented pictures or environmental sounds. The difficulty of the matching judgment was manipulated by varying the level of semantic detail at which the words and objects were compared. We found that blood oxygen level dependent (BOLD) signal was modulated in ventral and dorsal regions of the inferior frontal gyrus of both hemispheres during auditory and visual object categorization, potentially implicating these areas as sites for integrating polymodal object representations with concepts in semantic memory. As expected, BOLD signal increases in the fusiform gyrus varied with the semantic level of object categorization, though this effect was weak and restricted to the left hemisphere in the case of auditory objects.
This review presents neuroimaging studies which have explored the cerebral substrates of the central executive component of the working memory model proposed by Baddeley and Hitch [Working memory (1986); Recent advances in learning and motivation (1974)]. These studies have demonstrated that different executive functions (manipulating and updating of information, dual-task coordination, inhibition and shifting processes) not only recruit various frontal areas, but also depend upon posterior (mainly parietal) regions. Such results are in agreement with the hypothesis that executive functions rely on a distributed cerebral network not restricted to anterior cerebral areas. Moreover, the intervention of similar prefrontal regions in a large number of executive tasks suggests that the central executive functioning must be understood in terms of different interactions between a network of regions rather than in terms of a specific association between one region and one higher-level cognitive process.
Neuroimaging studies have implicated the anterior-most or frontopolar regions of prefrontal cortex (FP-PFC, e.g., Brodmann's Area 10) as playing a central role in higher cognitive functions such as planning, problem solving, reasoning, and episodic memory retrieval. The current functional magnetic resonance imaging (fMRI) study tested the hypothesis that FP-PFC subserves processes related to the monitoring and management of subgoals, while maintaining information in working memory (WM). Subjects were scanned while performing two variants of a simple delayed response WM task. In the control WM condition, subjects monitored for the presence of a specific concrete probe word (LIME) occurring following a specific abstract cue word (FATE). In the subgoal WM condition, subjects monitored for the presence of any concrete probe word immediately following any abstract cue word. Thus, the task required semantic classification of the probe word (the subgoal task), while the cue was simultaneously maintained in WM, so that both pieces of information could be integrated into a target determination. In a second control condition, subjects performed abstract/concrete semantic classification without WM demands. A region within right FP-PFC was identified which showed significant activation during the subgoal WM condition, but no activity in either of the two control conditions. However, this FP-PFC region was not modulated by direct manipulation of active maintenance demands. In contrast, left dorsolateral PFC was affected by active maintenance demands, but the effect did not interact with the presence of a subgoal task. Finally, left ventral PFC regions showed activation in response to semantic classification, but were not affected by WM demands. These results suggest a triple dissociation of function within PFC regions, and further indicate that FP-PFC is selectively engaged by the requirement to monitor and integrate subgoals during WM tasks.
We investigated the influence of different task demands, task designs, and presentation modalities on the functional MRI activation patterns during a language lateralization task in a group of 14 right-handed control subjects. A word classification task was presented as target task appropriate to evoke language-related activation in the inferior frontal gyrus (IFG). The choice of the contrasting baseline task was demonstrated to have a major impact on the functional outcome: While a fixation baseline elicited activations in the inferior frontal gyrus of both hemispheres, a nonsemantic perceptual control task helped to isolate the relevant target task of word classification. The modality of stimulus presentation did not influence the functional data: Auditory and visual presentation modes broadly evoked activations in similar brain regions during word classification. Minor differences in task performance and the side of the responding hand did not interfere with the functional activation patterns of the target task. On the basis of our results, a protocol of functional lateralization in the inferior frontal gyrus is suggested. J. Magn. Reson. Imaging 2001;13:668-675.
Seeing an object on one occasion may facilitate or prime processing of the same object if it is later again encountered. Such priming may also be found -- but at a reduced level -- for different but perceptually similar objects that are alternative exemplars or 'tokens' of the initially presented object. We explored the neural correlates of this perceptual specificity using event-related functional magnetic resonance imaging (fMRI) procedures, contrasting neural activity when participants made object classification decisions (size judgments) regarding previously presented objects (repeated same), alternative exemplars of previously presented objects (repeated different), or entirely new objects (novel). Many frontal regions (including bilateral frontal operculum, bilateral posterior inferior frontal/precentral, left anterior inferior frontal, and superior frontal cortices) and multiple late visual and posterior regions (including middle occipital, fusiform, fusiform-parahippocampal, precuneus, and posterior cingulate, all bilaterally), demonstrated reduced neural activity for repeated compared to novel objects. Greater repetition-induced reductions were observed for same than for different exemplars in several of these regions (bilateral posterior inferior frontal, right precuneus, bilateral middle occipital, bilateral fusiform, bilateral parahippocampal and bilateral superior parietal). Additionally, right fusiform (occipitotemporal) cortex showed significantly less priming for different versus same exemplars than did left fusiform. These findings converge with behavioral evidence from divided visual field studies and with neuropsychological evidence underscoring the key role of right occipitotemporal cortex in processing specific visual form information; possible differences in the representational-functional role of left fusiform are discussed.
